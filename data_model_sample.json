[
    {
        "Id": 38279,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "29/09/2012",
        "OwnerUserId": "",
        "Score": 6,
        "ViewCount": 2142.0,
        "Title": "run multiple time no-limit hold em poker",
        "Body": "suppose flop limit hold em two player go all-in hole card community card board present card drawn player make best card hand community card hole card sometimes people like run deal final card time either split pot one person win twice win whole pot also variation run time benefit one player figure percentage chance player win easy feat run idea approach problem run",
        "Tags": [
            "probability",
            "random-variable",
            "conditional-probability",
            "expected-value"
        ],
        "AnswerCount": 2.0,
        "CommentCount": 1,
        "Topics": {
            "13": 0.29151228,
            "22": 0.101917,
            "43": 0.41865295
        }
    },
    {
        "Id": 260013,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 260014.0,
        "CreationDate": "04/02/2017",
        "OwnerUserId": 147858.0,
        "Score": 3,
        "ViewCount": 836.0,
        "Title": "kurtosis skewness nonparametric bootstrap",
        "Body": "run nonparametric bootstrap estimate confidence interval mean sample question loop correct acceptable compute skewness kurtosis randomly sample data iteration way distribution kurtosis skewness would plot histogram useful information however still want single estimate kurtosis skewness would compute distribution bootstrapped mean",
        "Tags": [
            "bootstrap",
            "skewness",
            "kurtosis"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "43": 0.28374895,
            "45": 0.3631677
        }
    },
    {
        "Id": 3411,
        "PostTypeId": 2,
        "ParentId": 3400.0,
        "AcceptedAnswerId": "",
        "CreationDate": "08/10/2010",
        "OwnerUserId": 930.0,
        "Score": 16,
        "ViewCount": "",
        "Title": "",
        "Body": "begin article aim promote use ps epidemiology oakes church cite hern\u00e1n robin claim confound effect epidemiology guarantee result observational study unaffected unmeasured confound answer epidemiologist provide \u2018 \u2019 say ensure result observational study unbiased useless propofol say result useful design rcts also ps certainly offer complete solution problem least necessarily yield well result match multivariate method see e g propensity score p construction probabilistic causal indicator choice covariates enter propensity score function key element ensure reliability weakness say mainly stand control unobserved confounders quite likely retrospective case-control study others factor consider model misspecification impact direct effect estimate really ols case though b may miss data level covariates c ps overcome synergistic effect know affect causal interpretation reference found roger newson slide causality confounders propensity score relatively well-balanced pro con use propensity score illustration real study also several good paper discuss use propensity score observational study environmental epidemiology two year ago statistic medicine enclose couple end like pearl review offer large perspective causality issue ps discuss p obviously find many illustration look apply research would like add two recent article william r shadish come across andrew gelman website use propensity score discuss two paper largely focus causal inference observational study compare randomize setting reference oakes j church r invite commentary advance propensity score method epidemiology american journal epidemiology hernan robin j instrument causal inference epidemiologist dream epidemiology rubin design versus analysis observational study causal effect parallel design randomize trial statistic medicine \u2013 shrier letter editor statistic medicine \u2013 pearl j remark method propensity score statistic medicine \u2013 stuart e develop practical recommendation use propensity score discussion \u2018 critical appraisal propensity score match medical literature \u2019 peter austin statistic medicine \u2013 pearl j causal inference statistic overview statistic survey oakes j johnson p j propensity score match social epidemiology method social epidemiology j oakes kaufman ed pp jossez-bass h\u00f6fler causal inference base counterfactuals bmc medical research methodology winkelmayer w c kurth propensity score help hype nephrology dialysis transplantation shadish w r clark h steiner p nonrandomized experiment yield accurate answer randomize experiment compare random nonrandom assignment jasa cook shadish w r wong v c three condition experiment observational study produce comparable causal estimate new finding within-study comparison journal policy analysis management \u2013",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "29": 0.15688583,
            "30": 0.19007696,
            "43": 0.30631793
        }
    },
    {
        "Id": 133653,
        "PostTypeId": 2,
        "ParentId": 48434.0,
        "AcceptedAnswerId": "",
        "CreationDate": "16/01/2015",
        "OwnerUserId": 56042.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "also depend signal noise data dependent variable pretty well explain combination variable model think get away low n p ratio suspect absolute minimum number n also require get decent model apart ratio one way look tree build use sqrt p variable number large number point small tree fit without really real model hence lot over-fitted tree give false variable importance usually variable importance chart see lot top variable almost level importance conclude give noise",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "9": 0.14269026,
            "43": 0.32553253
        }
    },
    {
        "Id": 205554,
        "PostTypeId": 2,
        "ParentId": 205471.0,
        "AcceptedAnswerId": "",
        "CreationDate": "05/04/2016",
        "OwnerUserId": 7828.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "literature recommends standardization pca data never see recommend essentially pca multivariate standardization redundancy first standardize every attribute pca form normalization may help may harm correlation data harmful helpful variable corlated originate good signal great correlate root cause helpful well reduce correlation",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "14": 0.12477387,
            "25": 0.10418756,
            "35": 0.10434619,
            "38": 0.10936104,
            "43": 0.35433927
        }
    },
    {
        "Id": 225482,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 225485.0,
        "CreationDate": "25/07/2016",
        "OwnerUserId": 44075.0,
        "Score": 5,
        "ViewCount": 477.0,
        "Title": "discrete probability distribution two tail",
        "Body": "interested know whether discrete probability distribution similar poisson also extend negative value part e take negative value low limit value take family discrete distribution meeting requirement edit requirement shape necessarily symmetrical one condition met value close likely observe value improbable besides probability observe value great low monotonically decrease less like poisson distribution also negative side edit answer comment yes data follow sort bimodal distribution mode",
        "Tags": [
            "distributions",
            "poisson-distribution",
            "fitting"
        ],
        "AnswerCount": 2.0,
        "CommentCount": 3,
        "Topics": {
            "43": 0.37514338,
            "45": 0.18922725
        }
    },
    {
        "Id": 25751,
        "PostTypeId": 2,
        "ParentId": 25750.0,
        "AcceptedAnswerId": "",
        "CreationDate": "03/04/2012",
        "OwnerUserId": 5836.0,
        "Score": 7,
        "ViewCount": "",
        "Title": "",
        "Body": "propensity score way match group way use propensity score heart way characterize probability expose give covariates adjust one number way include match theoretically break one condition necessary confound problem case-control study hard calculate true probability exposure reason hard calculate true probability disease whole cohort work unbalanced sample say article discuss use propensity-score method case-control study one might good place start main thrust much less straightforward use unless credible reason adjust use propensity score instead outcome-oriented approach like include covariates model might worthwhile edit comment matter match exposure outcome match match covariates propensity score way roll covariates one composite covariate propensity score match try find case control equal probability expose covariates save exposure interest observe sugi paper link actual code generate propensity score use match follow code model predict probability exposure revasc see page paper",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 4,
        "Topics": {
            "12": 0.11653487,
            "30": 0.12127768,
            "43": 0.30348825
        }
    },
    {
        "Id": 176264,
        "PostTypeId": 2,
        "ParentId": 176262.0,
        "AcceptedAnswerId": "",
        "CreationDate": "09/10/2015",
        "OwnerUserId": 76981.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "think refer meant something like assume iid thus uncorrelated model several instance e say iid mean independent word independent etc",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 4,
        "Topics": {
            "43": 0.30154625
        }
    },
    {
        "Id": 415354,
        "PostTypeId": 2,
        "ParentId": 415352.0,
        "AcceptedAnswerId": "",
        "CreationDate": "29/06/2019",
        "OwnerUserId": 250623.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "think basically almost answer question already function exactly linear approximately linear close enough set cut-off value different allow know function approximately linear value bring close enough depend define cut-off",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "5": 0.10043538,
            "20": 0.19021235,
            "43": 0.29109254,
            "45": 0.10723813
        }
    },
    {
        "Id": 173746,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "22/09/2015",
        "OwnerUserId": 82893.0,
        "Score": 3,
        "ViewCount": 727.0,
        "Title": "get posterior bayesian linear regression flat prior",
        "Body": "one homework problem intro bayesian class derive posterior simple linear regression problem give response vector nx n many case regressor matrix nxp n many case p many regressors coefficient vector nx error error term n prior p flat reference prior use pdf multivar normal distribution start focus matrix algebra exponent sure go instead complete square professor usually u get exponent form x normally distribute variable b c contain x told b order figure kernel problem unable get form would appreciate help even point relevant matrix algebra rule undergrad deathly afraid matrix work overcome fear see related question give answer chose ask follow reason none see mention flat prior none go matrix algebra sufficient detail go hang around night happy address question concern thank time attention",
        "Tags": [
            "regression",
            "self-study",
            "bayesian"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 6,
        "Topics": {
            "20": 0.118517086,
            "43": 0.41831023
        }
    },
    {
        "Id": 260885,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 260911.0,
        "CreationDate": "09/02/2017",
        "OwnerUserId": 148562.0,
        "Score": 3,
        "ViewCount": 156.0,
        "Title": "binomial glm non-significant difference opposite group observation",
        "Body": "follow basic question concern binomial glm suppose set observation binary response measure three different treatment c incidentally response true case case c case accord binomial glm however latter contrast difference significant result make sense appropriate test case outcome give treatment completely fall one response type appreciate feedback",
        "Tags": [
            "r",
            "regression",
            "logistic",
            "generalized-linear-model",
            "separation"
        ],
        "AnswerCount": 2.0,
        "CommentCount": 1,
        "Topics": {
            "41": 0.11988168,
            "43": 0.30938157
        }
    },
    {
        "Id": 151172,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 151175.0,
        "CreationDate": "07/05/2015",
        "OwnerUserId": 76479.0,
        "Score": 3,
        "ViewCount": 52.0,
        "Title": "time series analysis good idea analysis use factor describe intra-seasonal description",
        "Body": "hope question vague suppose look hourly sale say walmart dillons data give month clear data time series frequency wise add different feature data describe hour day treat problem multivariate time series also well explain sale morning hour one model sale even hour rush hour another model situation two instance change quite drastically feel like follow different rule relatively new time series analysis knowledge arima model model include description season model",
        "Tags": [
            "time-series"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "28": 0.12518016,
            "43": 0.46265754
        }
    },
    {
        "Id": 153192,
        "PostTypeId": 2,
        "ParentId": 82526.0,
        "AcceptedAnswerId": "",
        "CreationDate": "20/05/2015",
        "OwnerUserId": 66477.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "fisher information estimator fisher information random sample wikipedia say mathematical statistic fisher information sometimes simply call information way measure amount information observable random variable x carry unknown parameter upon probability x depends true fisher information kind connection two random variable instead estimator function x",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "18": 0.10053057,
            "20": 0.14744115,
            "41": 0.19698396,
            "43": 0.15548599,
            "47": 0.16068731
        }
    },
    {
        "Id": 142043,
        "PostTypeId": 2,
        "ParentId": 142037.0,
        "AcceptedAnswerId": "",
        "CreationDate": "17/03/2015",
        "OwnerUserId": 49398.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "think xeon mean shrink towards diagonal matrix see instance another common approach assume covariance matrix multiple identity matrix enough information estimate covariance even exist third approach resort low rank reconstruction matrix use svd retain component explain large percentage variance data",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 3,
        "Topics": {
            "17": 0.2073856,
            "43": 0.3790673
        }
    },
    {
        "Id": 357274,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "15/07/2018",
        "OwnerUserId": 213958.0,
        "Score": 0,
        "ViewCount": 638.0,
        "Title": "report r output lme use lmertest exactly use anova",
        "Body": "fit linear mixed-effects model r would like report finding \u2019 use lmertest get p value term give model work great continuous binary predictor much categorical one category \u2019 one term associate variable \u2019 use anova function contrast model contains give categorical variable model first question okay report lmertest p value continuous binary predictor anova p value rest e okay mix \u201c strategy \u201d second question put lme model anova function single argument get nice table variable associate p value however unclear exactly row mean word \u2019 sure interpret p value whether \u2019 safe report case could easily report variable way variable seem table anova model variable model without variable would especially interaction thrown \u2019 example code comment question find simulated data use",
        "Tags": [
            "anova",
            "lme4-nlme",
            "reporting"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 11,
        "Topics": {
            "11": 0.15477662,
            "43": 0.2559589
        }
    },
    {
        "Id": 161773,
        "PostTypeId": 2,
        "ParentId": 161758.0,
        "AcceptedAnswerId": "",
        "CreationDate": "16/07/2015",
        "OwnerUserId": 80745.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "element statistical learn great book read understand material able discern algorithm might well suit type problem like simpler version book try introduction statistical learn author simpler book focus practical application provide comparison different method book less comprehensive esl book though diagram scikit-learn pretty good one illustrate machine learn possibility user already point usually narrow possible procedure use sort cross validation method test out-of-sample error rate",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "10": 0.117231295,
            "15": 0.112571776,
            "29": 0.27262837,
            "43": 0.33573017
        }
    },
    {
        "Id": 117096,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "28/09/2014",
        "OwnerUserId": 46082.0,
        "Score": 1,
        "ViewCount": 3087.0,
        "Title": "post-hoc non homogeneous data",
        "Body": "post-hoc test use non homogeneous data checked data set normally distributes use anova levene test show data homogeneous p value want check whether group significance need post-hoc test post-hoc use",
        "Tags": [
            "statistical-significance",
            "anova",
            "spss"
        ],
        "AnswerCount": 2.0,
        "CommentCount": 2,
        "Topics": {
            "21": 0.13703002,
            "43": 0.3451799,
            "44": 0.2167756
        }
    },
    {
        "Id": 110175,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "31/07/2014",
        "OwnerUserId": 53192.0,
        "Score": 2,
        "ViewCount": 1832.0,
        "Title": "compare sample mean experimental-control group study",
        "Body": "hop compare sample mean experimental group sample mean three control group context study experimental group consists person particular criminal record afford various rehabilitative service person count number rearrests begin treatment control group similarly size people share similar characteristic experimental group person control group count number arrest control group begin treatment e control group monitor time period want test statistically significant difference number rearrests experimental group control group distribution arrest experimental group control group non-normal skewed right people either zero rearrests rearrest few few people rearrest however test homogeneity variance reflect variance equal test use case also test use follow assumption normal distribution true homogeneity variance true normal distribution true homogeneity variance false normal distribution false homogeneity variance true normal distribution false homogeneity variance false",
        "Tags": [
            "t-test",
            "nonparametric"
        ],
        "AnswerCount": 3.0,
        "CommentCount": 1,
        "Topics": {
            "39": 0.15125592,
            "43": 0.1947128
        }
    },
    {
        "Id": 281586,
        "PostTypeId": 2,
        "ParentId": 235120.0,
        "AcceptedAnswerId": "",
        "CreationDate": "25/05/2017",
        "OwnerUserId": 162543.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "consider explanatory model could look factor holiday promotion etc affect seasonal part time-series data could help prediction seasonal portion check time series additive multiplicative visual examination use preliminary analysis increase trend size fluctuation data increase increase level series high probability multiplicative model size fluctuation remains constant irrespective level series high probability additive model statistically confirm refer follow post check time series well fit additive multiplicative model link help understand whether multiplicative additive well fit time-series brief compare auto correlation acf remainder component time series additive multiplicative decomposition give low acf well fit",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 3,
        "Topics": {
            "1": 0.15444118,
            "24": 0.30598655,
            "28": 0.13138738,
            "43": 0.24504748
        }
    },
    {
        "Id": 54493,
        "PostTypeId": 2,
        "ParentId": 54460.0,
        "AcceptedAnswerId": "",
        "CreationDate": "27/03/2013",
        "OwnerUserId": 22287.0,
        "Score": 6,
        "ViewCount": "",
        "Title": "",
        "Body": "mean probability element paragraph take follow book chapter order statistic probability briefly say random variable cdf continuous derivative call probability element probability density function pdf think would something measure theory well go probability theory course might help",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 3,
        "Topics": {
            "19": 0.102135,
            "29": 0.169408,
            "43": 0.22574627,
            "45": 0.10574887
        }
    },
    {
        "Id": 398920,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "22/03/2019",
        "OwnerUserId": 223288.0,
        "Score": 1,
        "ViewCount": 117.0,
        "Title": "use multilevel model situation pre post control group",
        "Body": "question whether multilevel model appropriate situation \u2019 work analysis look effect treatment patient disease one pre measurement one post measurement control group several covariates e g demographic outcome continuous research question \u2022 effect treatment \u2022 covariates demographic influence outcome think linear mixed model \u2022 random intercept subject \u2022 dummy variable post period pre period interpret treatment effect \u2022 covariates model e interact variable none covariates time-varying seem alternative approach would run normal linear regression outcome post period measurement pre period measurement independent variable variable would include able easily find example multilevel model pre post data \u2019 wonder multilevel model approach really require measurement e g time point subject withdrew study \u2019 get data sample size great one set people another set group people need analyze separately note two group analyze separately one represent patient disease second case patient \u2019 disease thought thank",
        "Tags": [
            "multilevel-analysis",
            "treatment-effect"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 1,
        "Topics": {
            "21": 0.17258462,
            "30": 0.10583798,
            "33": 0.100509375,
            "43": 0.23021747
        }
    },
    {
        "Id": 235348,
        "PostTypeId": 2,
        "ParentId": 235292.0,
        "AcceptedAnswerId": "",
        "CreationDate": "16/09/2016",
        "OwnerUserId": 127257.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "many circumstance economics difficult mislead try solve equation isolation without consider relevant endogenous exogenous effect economic analysis normally dynamic sense economic system like spiderweb sensitive say say need go little deeper use additional info isolate solution one hand consider price elasticity demand need remember good product service ultimately classify belonging certain product group certain consumer business example normal product middle scale inferior giffen product one end scale luxury product end scale luxury item normal item others etc elasticity curve look different different product group need idea approximately product belongs hand core price elasticity demand question extent substitute item consideration question substitute automatically draw attention question degree similar product artificially differentiate example abc company sell k gas generator specific quality direct certain re-sellers sell gas generator one specific re-seller differentiate different model different color superficial difference product require artificial price change test elasticity graph show price go near cent near cent maybe price need artificially change high low number certain time period observe broader range behavior get well idea price elasticity demand first sentence mention endogenous exogenous effect need make grid top important endogenous exogenous effect respect product must create multiple price elasticity demand graph curve base different level endogenous exogenous effect start create multiple price volume graph constrain different level endogenous exogenous effect even though mention product consideration question really matter strategy use construct identify curve graph price elasticity demand still follow certain technique example must consider income product could break analysis multiple income group reexamine price volume plot consider price change close substitute week time period complete grid generate many different price volume plot give combination endogenous exogenous input constraint even region weather season culture might something price elasticity demand complete grid begin graphically see price elasticity demand curve approximates without exact curve price one important factor economic analysis relevant price reflect impact endogenous exogenous effect shortcut analysis mislead final outcome remember economist frequently use structural equation system resolve many economic question thus price analysis simple might even need survey data complete grid",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "28": 0.17077042,
            "43": 0.36188835
        }
    },
    {
        "Id": 377656,
        "PostTypeId": 2,
        "ParentId": 314865.0,
        "AcceptedAnswerId": "",
        "CreationDate": "18/11/2018",
        "OwnerUserId": 76981.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "linear regression covariance matrix estimate effect use extract standard error simple formula scalar estimate residual variance covariance predictor multiply number sample compute standard error coefficient simply take square root diagonal relate question well standard error regression parameter similar implies entry diagonal close diagonal similar value might happen one scenario might happen one column similarly distribute term variance column covariance column point merely speculate data might give variable take value seem quite likely similarly distribute predictor hypothesis true",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "5": 0.10336124,
            "26": 0.11397804,
            "43": 0.28800756
        }
    },
    {
        "Id": 5678,
        "PostTypeId": 2,
        "ParentId": 5054.0,
        "AcceptedAnswerId": "",
        "CreationDate": "21/12/2010",
        "OwnerUserId": 1764.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "far tell reading common method determine time series trend smooth series perhaps iterate fashion pakistan sbp paper seasonal adjustment methodology section describes x- arima though also use seasonal factor perhaps could also use perhaps could simply ignore link might include bank england web page u census bureau paper page",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "24": 0.12886351,
            "28": 0.1343441,
            "43": 0.31816143
        }
    },
    {
        "Id": 92849,
        "PostTypeId": 2,
        "ParentId": 92848.0,
        "AcceptedAnswerId": "",
        "CreationDate": "07/04/2014",
        "OwnerUserId": 18152.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "t-test anova normality assumption require within unique cell marginals variable latter plot show important one reason assumption first place since two test standard glm family respective residual must normally distribute include group mean predictor equivalent look observe data distribution see normal",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "39": 0.156891,
            "43": 0.2411363,
            "45": 0.24528484
        }
    },
    {
        "Id": 114937,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 115094.0,
        "CreationDate": "10/09/2014",
        "OwnerUserId": 53561.0,
        "Score": 0,
        "ViewCount": 93.0,
        "Title": "compare goodness two estimation",
        "Body": "perform two estimation follow result blue line result simple linear regression displayed data point truth red line result estimation different data ass goodness red estimation comparison blue",
        "Tags": [
            "regression",
            "method-comparison"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 7,
        "Topics": {
            "25": 0.13633825,
            "26": 0.14803208,
            "36": 0.18144856,
            "39": 0.16461165
        }
    },
    {
        "Id": 339114,
        "PostTypeId": 2,
        "ParentId": 338802.0,
        "AcceptedAnswerId": "",
        "CreationDate": "07/04/2018",
        "OwnerUserId": 99274.0,
        "Score": -2,
        "ViewCount": "",
        "Title": "",
        "Body": "first see op edit answer really good answer question simplest answer question look calculate directly mean temperature location subtract find expect difference temperature particular time particular day mean temperature require approximately year data establish mean time series like arima three month data single year would useful next useful would regression next useful would correlation agreement would useful measure information rich enough explain temperature covary detail follow square correlation coefficient call coefficient determination coefficient determination symbolically would give fraction variance temperature mutually explain occurs time found daily average temperature compile augusta maine miami florida chart correlation coefficient determination km distance city similar km athens oslo distance like athens oslo miami augusta time zone meaning daily temperature high low correlate positively hourly data readily available would correlate unique measurement rather average include may low coefficient determination nonetheless correlation high even suspect note chart double hysteresis loop presumably largely due difference timing temperature modification proximity labrador current maine gulf stream current miami one convert miami augusta temperature use bivariate regression e use passing-bablok invertible unlike ols regression moreover chart show quite smooth progression temperature along curvilinear close path one year loop particular day year particular time day difference temperature two site average fairly stable want know calculate look difference example gung average annual temperature difference two site degree f compile day want measure agreement lin concordance correlation coefficient would one alternative useful one see similar intraclass correlation measure distant agreement identity line agreement right question ask time series model like see arimaprocess average monthly temperature oslo sweden melbourne australia degree centigrade correlation coefficient determination op coefficient determination excel correl cell cell cell cell note high negative correlation two point earth could apart ols regression show let u compare result lin concordance augusta miami value oslo melbourne concordance first case concordance weak second case slightly strong disagreement go far explain difference temperature",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 13,
        "Topics": {
            "43": 0.4843344
        }
    },
    {
        "Id": 20800,
        "PostTypeId": 2,
        "ParentId": 20791.0,
        "AcceptedAnswerId": "",
        "CreationDate": "09/01/2012",
        "OwnerUserId": 3277.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "ordinary linear pca likert-scale variable already treat variable scale continuous variable variable evenly space measure benchmark put word regard ordinal far recognize ordinal e potentially evenly space nature might consider perform categorical pca catpca quantifies measure level nonlinearly achieve best principal component yes principal component continuous usual regression apt",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 5,
        "Topics": {
            "11": 0.28822592,
            "35": 0.14348786,
            "43": 0.2600258
        }
    },
    {
        "Id": 410844,
        "PostTypeId": 2,
        "ParentId": 410841.0,
        "AcceptedAnswerId": "",
        "CreationDate": "30/05/2019",
        "OwnerUserId": 249205.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "use cauchy-schwarz inegality way",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "20": 0.40655658,
            "43": 0.15617609
        }
    },
    {
        "Id": 199392,
        "PostTypeId": 2,
        "ParentId": 144172.0,
        "AcceptedAnswerId": "",
        "CreationDate": "01/03/2016",
        "OwnerUserId": 69416.0,
        "Score": 4,
        "ViewCount": "",
        "Title": "",
        "Body": "actually multiple way remember auc normalize form mann-whitney-u statistic sum rank either class mean find optimal auc problem order score score high one class frame example highly infeasible linear-programming-problem solve heuristically appropriate relaxation one method interest find approximate gradient auc optimize stochastic-gradient-descent plenty read naive approach use iverson-bracket another way look sought order score could response score function input parameter want maximize consider relaxation could sample class get contribution full gradient",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "0": 0.11964669,
            "23": 0.20551638,
            "43": 0.2751589,
            "48": 0.121440604
        }
    },
    {
        "Id": 276478,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "28/04/2017",
        "OwnerUserId": 159299.0,
        "Score": 1,
        "ViewCount": 182.0,
        "Title": "cdt multiple correspondence analysis mca multiple modality select one variable",
        "Body": "follow set qualitative data analyze thus want run multiple correspondence analysis mca however variable modality non-exclusive e use question ok mca data yes construct complete disjunctive table cdt issue yes sure best cdt use follow two difference first table weight binary second weight cell boolean equal conisidered modality select otherwise e g since select two modality variable weight modality variable observation equal modality select",
        "Tags": [
            "pca",
            "multivariate-analysis",
            "correspondence-analysis",
            "qualitative",
            "multiple-membership"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "11": 0.16999787,
            "32": 0.15135787,
            "43": 0.2365345
        }
    },
    {
        "Id": 197966,
        "PostTypeId": 2,
        "ParentId": 197952.0,
        "AcceptedAnswerId": "",
        "CreationDate": "22/02/2016",
        "OwnerUserId": 1390.0,
        "Score": 9,
        "ViewCount": "",
        "Title": "",
        "Body": "suspect difference term fit value get look would call model fit coefficient estimate variance term model identical compare see estimate model note also estimate subject group variance standard deviation give output information compute model use give near enough match u worry hence method must return different fit value generate fit value get value produce intent purpose document return contribution population level average subject-specific component represent random effect spline fix effect case model return fit value fix effect part model would explain difference write fix spline term fix random effect get little blur book simon wood mention issue example fit sole data use talk use component model exclude include random effect respectively say appropriate model diagnostics specific example use need depend context specific usage research question need simple random effect like familiar gam mgcv might simpler round use random effect spline basis rather deal weird output hybrid gamm model fit via show two model effectively equivalent difference report whether fit value include exclude subject id specific effect",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 13,
        "Topics": {
            "1": 0.1682375,
            "33": 0.17421733,
            "43": 0.31376714
        }
    },
    {
        "Id": 81762,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 113233.0,
        "CreationDate": "09/01/2014",
        "OwnerUserId": 21471.0,
        "Score": 5,
        "ViewCount": 403.0,
        "Title": "convergence distribution central limit theorem",
        "Body": "iid exponential mean let show definition exponential distribution look mgf mgf b show converges distribution identify asymptotic distribution central limit theorem know converges distribution see implies c show converges distribution identify asymptotic distribution theorem textbook introduction mathematical statistic hogg state let sequence random variable suppose function differentiable part b see apply theorem theorem think answer correct",
        "Tags": [
            "self-study",
            "mathematical-statistics",
            "convergence",
            "central-limit-theorem"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 5,
        "Topics": {
            "20": 0.39122173,
            "45": 0.26821208
        }
    },
    {
        "Id": 125010,
        "PostTypeId": 2,
        "ParentId": 124998.0,
        "AcceptedAnswerId": "",
        "CreationDate": "21/11/2014",
        "OwnerUserId": 21972.0,
        "Score": 4,
        "ViewCount": "",
        "Title": "",
        "Body": "add dummy variable great methodology accounting causal impact attribution specific event account quarter holiday add dummy variable data wary add many dummy variable cause over-generalization aka overfit sound like sample large enough make ok add example would probably make separate dummy var holiday rather single holiday dummy variable assume holiday similar effect add dummy variable one create contrast matrix categorical variable explain example caution dummy variable make model output difficult interpret explain though may score high prediction model training set related question cv great answer regression base example day week obvious next step add time model easy measurement occur equal time interval need convert time interval since first measurement first measurement add variable take care trend portion time series much like dummy variable take care seasonality causal event model want dig complicate model like autoregression arima etc recommend open free book enough forecasting principle practice rob j hyndman george athana\u00adsopou\u00adlos suspect would find extremely useful- basic need know forecasting great r code go along author well respect field",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "11": 0.1607029,
            "43": 0.36203945
        }
    },
    {
        "Id": 355363,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "10/07/2018",
        "OwnerUserId": 213456.0,
        "Score": 0,
        "ViewCount": 178.0,
        "Title": "excessive bias produce bootstrapped confidence interval include observe mean bootstrapped mean",
        "Body": "one statistic estimate model follow result problem get follow confidence interval question introduce boostrap variance need studentized interval percentile interval include observe value statistic strange mean bootstrap value small observe value ci high observe value bootstrapped mean",
        "Tags": [
            "confidence-interval",
            "bootstrap"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 2,
        "Topics": {
            "16": 0.37017396,
            "43": 0.20193209,
            "49": 0.16884418
        }
    },
    {
        "Id": 232077,
        "PostTypeId": 2,
        "ParentId": 232076.0,
        "AcceptedAnswerId": "",
        "CreationDate": "27/08/2016",
        "OwnerUserId": 24669.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "feel essentially definition sample joint marginal distribution result would unnecessary answer however try formalize question consider bivariate real-valued case distribution distribution consider bivariate case sample joint distribution begin equation p x mathcal x mathcal p x mathcal x mathcal end equation borel may take begin equation p x mathcal x p x mathcal x mathbb r p x mathcal x mathbb r p x mathcal x end equation distribution similar reason go show pair independent also independent drop independent pair distribution produce independent distribution",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "19": 0.22109823,
            "41": 0.104655795,
            "43": 0.22768,
            "45": 0.17859288
        }
    },
    {
        "Id": 184393,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 184413.0,
        "CreationDate": "01/12/2015",
        "OwnerUserId": 96576.0,
        "Score": 5,
        "ViewCount": 963.0,
        "Title": "gibbs sample complex full conditional",
        "Body": "sample question relate gibbs sample complicate full conditional suppose complicate full conditional want single sample sample directly three fold question point would one use metropolis-hastings step accept first mh step yield new typically well use burn phase accept new typically many step require fine tune good jumping distribution case mh step reject sample keep new keep go thank",
        "Tags": [
            "sampling",
            "simulation",
            "gibbs",
            "metropolis-hastings"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "22": 0.115370795,
            "41": 0.20378406,
            "43": 0.36549547
        }
    },
    {
        "Id": 93762,
        "PostTypeId": 2,
        "ParentId": 93740.0,
        "AcceptedAnswerId": "",
        "CreationDate": "14/04/2014",
        "OwnerUserId": 43480.0,
        "Score": 3,
        "ViewCount": "",
        "Title": "",
        "Body": "use binomial glm provide freedom model different sample size use glm function follow presence absence show number present absent case",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "3": 0.109223865,
            "11": 0.16211359,
            "18": 0.10617893,
            "43": 0.14128777
        }
    },
    {
        "Id": 113692,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 113708.0,
        "CreationDate": "29/08/2014",
        "OwnerUserId": 54915.0,
        "Score": 4,
        "ViewCount": 2937.0,
        "Title": "test identicality discrete distribution",
        "Body": "standard way test whether two vector drawn discrete distribution r something like kolmogorov-smirnov test discrete distribution think two-sample chi-squared test would appropriate package provide get work",
        "Tags": [
            "r",
            "hypothesis-testing",
            "chi-squared",
            "kolmogorov-smirnov",
            "discrete-data"
        ],
        "AnswerCount": 2.0,
        "CommentCount": 3,
        "Topics": {
            "43": 0.27445817,
            "44": 0.24413551
        }
    },
    {
        "Id": 6611,
        "PostTypeId": 2,
        "ParentId": 6540.0,
        "AcceptedAnswerId": "",
        "CreationDate": "27/01/2011",
        "OwnerUserId": 2914.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "question state precise interval yield hypothesis must conservative infinite interval yield pessimist approximation suggestion qualify confidence interval calculate rate p time sum data different sign must calculate positive number one sum negative number one sum way get proper upper low bound exponent -function return day timestamps rate term sum -formula take reciprocal example work progress try find one-liner operate data",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 4,
        "Topics": {
            "3": 0.11288299,
            "20": 0.12333275,
            "43": 0.38649586
        }
    },
    {
        "Id": 393309,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 393364.0,
        "CreationDate": "19/02/2019",
        "OwnerUserId": 110901.0,
        "Score": 1,
        "ViewCount": 313.0,
        "Title": "convex optimization gradient descent faster regularizer add",
        "Body": "sure true statement appear intuition around among expert field quite understand idea give convex optimization problem solve use gradient descent sometimes advantageous add extra term objective function regularizer wikipedia suggests regularizers good avoid overfitting case problem simply find optimal minimizes convex function seem apply sense add regularizer well convex optimization one prove instance convergence faster add term realize perhaps basic broad question reference fine answer big fit scope site",
        "Tags": [
            "optimization",
            "regularization"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "23": 0.3923133,
            "43": 0.35481387
        }
    },
    {
        "Id": 56666,
        "PostTypeId": 2,
        "ParentId": 56660.0,
        "AcceptedAnswerId": "",
        "CreationDate": "20/04/2013",
        "OwnerUserId": 805.0,
        "Score": 3,
        "ViewCount": "",
        "Title": "",
        "Body": "continuous proportion sometimes model use beta regression logit transformation proportion sometimes use multiple proportion sum compositional data sometimes do via dirichlet model term help find many relevant question answer cv good pointer generally google search highly productive reference e g smithson verkuilen j well lemon squeezer maximum-likelihood regression beta-distributed dependent variable psychological method also see",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "11": 0.3379912,
            "43": 0.32785225
        }
    },
    {
        "Id": 35249,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "28/08/2012",
        "OwnerUserId": 7102.0,
        "Score": 30,
        "ViewCount": 16350.0,
        "Title": "machine learn technique parse string",
        "Body": "lot address string want parse component course data dirty come many country many language write different way contains misspelling miss piece extra junk etc right approach use rule combine fuzzy gazetteer match like explore machine learn technique label training data supervise learn question sort machine learn problem really seem cluster classification regression closest come would classify token really want classify simultaneously satisfy constraint like one country really many way tokenize string want try one pick best know exists thing call statistical parse know anything machine learn technique could explore parse address",
        "Tags": [
            "machine-learning",
            "text-mining"
        ],
        "AnswerCount": 4.0,
        "CommentCount": 7,
        "Topics": {
            "15": 0.11653586,
            "43": 0.4564781
        }
    },
    {
        "Id": 373526,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "24/10/2018",
        "OwnerUserId": 208827.0,
        "Score": 7,
        "ViewCount": 784.0,
        "Title": "consistent unbiased estimator location parameter",
        "Body": "give cauchy distribution pdf find consistent unbiased estimator reason far try mle seem closed-form expression see maximum likelihood estimator location parameter cauchy distribution mom work cauchy distribution moment",
        "Tags": [
            "mathematical-statistics",
            "maximum-likelihood",
            "unbiased-estimator",
            "estimators",
            "cauchy"
        ],
        "AnswerCount": 3.0,
        "CommentCount": 1,
        "Topics": {
            "20": 0.19496244,
            "26": 0.1851325,
            "43": 0.1929125,
            "49": 0.11729814
        }
    },
    {
        "Id": 48748,
        "PostTypeId": 2,
        "ParentId": 33562.0,
        "AcceptedAnswerId": "",
        "CreationDate": "29/01/2013",
        "OwnerUserId": 8013.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "question model use objective analysis objective develop classifier predict binary outcome see three model approximately give approximately classifier make moot point since care model develops classifier might use cross validation split sample validation determine model performs best similar data inference model estimate different model parameter three regression model special case glms use link function variance structure determine relationship binary outcome case continuous predictor nls logistic regression model use link function logit nls minimizes square error fitting curve logistic regression maximum likelihood estimate model data assumption linear model model probability binary distribution observe outcome think reason consider nls useful inference probit regression us different link function cumulative normal distribution function taper faster logit often use make inference binary data observe binary threshold unobserved continuous normally distribute outcome empirically logistic regression model use far often analysis binary data since model coefficient odds-ratio easy interpret maximum likelihood technique good convergence property",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "1": 0.21412173,
            "11": 0.15671267,
            "43": 0.1965235
        }
    },
    {
        "Id": 215272,
        "PostTypeId": 2,
        "ParentId": 126705.0,
        "AcceptedAnswerId": "",
        "CreationDate": "29/05/2016",
        "OwnerUserId": 114340.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "handwritten answer typo pdf trick use -p p converge e p small gamma function k k r x choose r converages r x r x- r x- r r easily derive formula three plug",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "3": 0.26474696,
            "19": 0.23185608,
            "20": 0.18220355,
            "43": 0.10516811
        }
    },
    {
        "Id": 393670,
        "PostTypeId": 2,
        "ParentId": 393660.0,
        "AcceptedAnswerId": "",
        "CreationDate": "21/02/2019",
        "OwnerUserId": 238499.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "answer pere right want get result hand make n independent experiment success probability p probability get exactly k successful ie read-only result calculate follow p k p k -p n-k n k n-k first factor p k probability get k consecutive success second factor -p n-k probability get n-k consecutive faillures final term n k n-k stand possible permutation result really care success faillures consecutive conclusion guess get right",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "3": 0.11581928,
            "13": 0.2870048,
            "19": 0.1308629,
            "43": 0.1449839
        }
    },
    {
        "Id": 411236,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 411241.0,
        "CreationDate": "02/06/2019",
        "OwnerUserId": 249683.0,
        "Score": 4,
        "ViewCount": 111.0,
        "Title": "binary logistic regression two dependent variable",
        "Body": "continuous independent variable x two binary dependent variable z try run binary logistic regression model correlation z x age z two pas fail task want determine probability individual passing give task give age also want test whether individual pas one task tend also pas go try follow r give coefficient x however reverse order z formula x coefficient becomes positive anyone know run binary logistic regression r correlate dependent variable",
        "Tags": [
            "r",
            "regression",
            "logistic",
            "binary-data",
            "multivariate-regression"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "3": 0.18180338,
            "11": 0.275968,
            "43": 0.11850979
        }
    },
    {
        "Id": 286889,
        "PostTypeId": 2,
        "ParentId": 286880.0,
        "AcceptedAnswerId": "",
        "CreationDate": "23/06/2017",
        "OwnerUserId": 166382.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "work similar problem multivariate input image try predict single value output output regression problem way make output layer size linear activation right success image recognition realm apply technique problem example use cnn layer recognize temporal pattern input data cnn useful spatial pattern like image possible transform data image use technique stft problem input feature two relationship possible benefit apply cnn would best develop multi-layer neural network relu activation regularizer dropout l l help model generalize sure data normalize properly feature equal impact training",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "6": 0.27652928,
            "15": 0.174542,
            "43": 0.27117538
        }
    },
    {
        "Id": 331068,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 331354.0,
        "CreationDate": "28/02/2018",
        "OwnerUserId": 66461.0,
        "Score": 3,
        "ViewCount": 133.0,
        "Title": "algorithm performance validation set good criterion model selection",
        "Body": "think old question cross validation benefit reading post thread use test data set cross-validation difference test set validation set three partition training validation test still follow question get data set feature x label supervise learn split training validation test data set train model model different value hyper-parameter e g degree polynomial training data set tune hyper-parameters model validation set pick winner apply tune model winner step test data set get estimation performance tune model real world stop tune hyper-parameter let look step clearly model well performance training set necessarily well performance validation set model could overfit training set validation set prevent overfitting training set come back point minute let look step say step chosen model e specific value hyper-parameter best performance validation set believe performance score model validation set roughly equal actual performance score model new real world data set usually optimistic need apply winner model step test set performance score obtain test set give u well estimation model performance new real world data set question is\uff1a step winner best performance validation set also best performance test set care model selection e rank different model step care whether performance score validation set good estimation actual performance model whole procedure sound like winner step also overfitting validation set nothing different logic overfitting training data set hope miss critical logic hope someone could correct base logic winner step necessarily best somewhat well performance test set model try question base logic e step overfitting validation set point step validation step furthermore criterion follow select model use e g apply test set would argue multiple validation set pick model best average performance validation set model winner almost surely winner give another test set idea n-fold cross-validation us data train model think well splitting data set part think term model selection e horse race n-fold cv well approach additional want fair estimate true error rate may divide training data part apply n-fold cv one part use part estimate true error rate training validation test set distribution large suppose see winner step also winner step theoretical result give procedure least faith",
        "Tags": [
            "hypothesis-testing",
            "cross-validation",
            "validation"
        ],
        "AnswerCount": 2.0,
        "CommentCount": 0,
        "Topics": {
            "10": 0.4559646,
            "43": 0.29974824
        }
    },
    {
        "Id": 242966,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "28/10/2016",
        "OwnerUserId": 124737.0,
        "Score": 4,
        "ViewCount": 56.0,
        "Title": "sample covariance",
        "Body": "sample covariance define intuition use correction term instead estimate intuition lose two degree freedom",
        "Tags": [
            "variance",
            "estimation",
            "covariance",
            "sample"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 1,
        "Topics": {
            "17": 0.13900247,
            "18": 0.108569875,
            "26": 0.17578557,
            "40": 0.11962529,
            "43": 0.18003203,
            "47": 0.10552785
        }
    },
    {
        "Id": 278480,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "09/05/2017",
        "OwnerUserId": 160692.0,
        "Score": 1,
        "ViewCount": 54.0,
        "Title": "interpret precision matrix generalise least square regression",
        "Body": "suppose estimate parameter regression b x w e set spatial coordinate e uncorrelated error term w n c c covariance matrix assume depends distance two observation cij decrease dij gls estimate b b hat x c x x c x transpose x c inverse c precision matrix question interpret weight c estimation correct say weight give observation distant",
        "Tags": [
            "regression",
            "spatial",
            "generalized-least-squares"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 0,
        "Topics": {
            "5": 0.11350452,
            "17": 0.10256845,
            "19": 0.15864287,
            "26": 0.15705712,
            "37": 0.14522393
        }
    },
    {
        "Id": 308095,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 308101.0,
        "CreationDate": "15/10/2017",
        "OwnerUserId": 181005.0,
        "Score": 0,
        "ViewCount": 593.0,
        "Title": "extrapolate mean data range assume normal distribution",
        "Body": "follow summary data point value assume normal distribution value calculate mean note precise rough estimation near would sufficient many thanks",
        "Tags": [
            "normal-distribution",
            "extrapolation"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 3,
        "Topics": {
            "25": 0.20230031,
            "43": 0.3259345,
            "45": 0.13784686,
            "49": 0.15759167
        }
    },
    {
        "Id": 12961,
        "PostTypeId": 2,
        "ParentId": 3294.0,
        "AcceptedAnswerId": "",
        "CreationDate": "13/07/2011",
        "OwnerUserId": 529.0,
        "Score": 5,
        "ViewCount": "",
        "Title": "",
        "Body": "already nice suggestion would like add follow article describe hmms perspective application biology sean eddy hidden markov model profile hidden markov model hidden markov model",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "29": 0.31436843,
            "34": 0.12438002,
            "43": 0.21995188
        }
    },
    {
        "Id": 323917,
        "PostTypeId": 2,
        "ParentId": 323873.0,
        "AcceptedAnswerId": "",
        "CreationDate": "19/01/2018",
        "OwnerUserId": 7224.0,
        "Score": 4,
        "ViewCount": "",
        "Title": "",
        "Body": "boo stefanski essential statistical inference pp four condition sample density give sufficient consistency identifiability parameter e set measure one boundedness expect likelihood e neighbourhood true parameter value differentiability log-likelihood e continuously differentiable neighbourhood true parameter value almost every support uniform integrability e bound uniformly integrable function integrable",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "20": 0.116475716,
            "41": 0.1995166,
            "47": 0.13494113
        }
    },
    {
        "Id": 15796,
        "PostTypeId": 2,
        "ParentId": 15790.0,
        "AcceptedAnswerId": "",
        "CreationDate": "20/09/2011",
        "OwnerUserId": 2116.0,
        "Score": 5,
        "ViewCount": "",
        "Title": "",
        "Body": "regression coefficient jointly normal single coefficient normal since want test hypothesis single coefficient use distribution care happens coefficient however want take account coefficient modify hypothesis example test two coefficient zero answer conditional z score possible statistic similar z-score involve one coefficient",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "5": 0.19609371,
            "20": 0.10010343,
            "43": 0.31228456
        }
    },
    {
        "Id": 257512,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "22/01/2017",
        "OwnerUserId": 105853.0,
        "Score": 0,
        "ViewCount": 1065.0,
        "Title": "data normalization lme mixed model r",
        "Body": "set data go apply lme mixed model however data normally distribute see graph try log sqrt zscore box-cox transformation even graph look well kolmogorov-smirnov test give max p-value residual plot lme model use data transform box-cox attach lilliefors kolmogorov-smirnov normality testp-value e- knowledge residual plot pattern see plot normality test kolmogorov-smirnov shapiro give value question also normalize data use mixed model lme",
        "Tags": [
            "r",
            "mixed-model",
            "data-transformation",
            "residuals"
        ],
        "AnswerCount": 2.0,
        "CommentCount": 1,
        "Topics": {
            "14": 0.10343065,
            "36": 0.11873593,
            "43": 0.16286793,
            "44": 0.13697013,
            "46": 0.14329925
        }
    },
    {
        "Id": 123957,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "14/11/2014",
        "OwnerUserId": 60665.0,
        "Score": 2,
        "ViewCount": 613.0,
        "Title": "appropriate combine two treatment group one study composite whole meta-analysis",
        "Body": "treatment group would like combine appear virtually least generic label group e g group treatment implement different site stratify subgroup recombine sensible whole lipsey wilson recommend case combine treatment group composite whole appropriate statistic available statistic refer go appropriately combine group",
        "Tags": [
            "statistical-significance",
            "meta-analysis",
            "computational-statistics",
            "descriptive-statistics",
            "meta-regression"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 2,
        "Topics": {
            "30": 0.18759927,
            "39": 0.1648385,
            "43": 0.36254483
        }
    },
    {
        "Id": 26655,
        "PostTypeId": 2,
        "ParentId": 26650.0,
        "AcceptedAnswerId": "",
        "CreationDate": "18/04/2012",
        "OwnerUserId": 5429.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "understand want r give",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "3": 0.24045108,
            "43": 0.31100887
        }
    },
    {
        "Id": 201129,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 206238.0,
        "CreationDate": "11/03/2016",
        "OwnerUserId": 74076.0,
        "Score": 28,
        "ViewCount": 46670.0,
        "Title": "training loss go happen",
        "Body": "training loss go weird cross-validation loss track training loss go two stack lstms follow kera train epoch train sample validate sample loss look like",
        "Tags": [
            "machine-learning",
            "neural-networks",
            "loss-functions",
            "lstm"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 4,
        "Topics": {
            "6": 0.15255634,
            "23": 0.12944154,
            "43": 0.30870682
        }
    },
    {
        "Id": 327988,
        "PostTypeId": 2,
        "ParentId": 327986.0,
        "AcceptedAnswerId": "",
        "CreationDate": "11/02/2018",
        "OwnerUserId": 6633.0,
        "Score": 3,
        "ViewCount": "",
        "Title": "",
        "Body": "conditional pdf give give write integral unconditional pdf substitute usual formula expectation continuous random variable",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "20": 0.5334147,
            "45": 0.15513656
        }
    },
    {
        "Id": 169500,
        "PostTypeId": 2,
        "ParentId": 169485.0,
        "AcceptedAnswerId": "",
        "CreationDate": "31/08/2015",
        "OwnerUserId": 61705.0,
        "Score": 4,
        "ViewCount": "",
        "Title": "",
        "Body": "register email list table content reputable journal register new england journal medicine lancet jama bmj clinical infectious disease etc",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "15": 0.11581704,
            "29": 0.2682741,
            "43": 0.44161293
        }
    },
    {
        "Id": 157337,
        "PostTypeId": 2,
        "ParentId": 120065.0,
        "AcceptedAnswerId": "",
        "CreationDate": "17/06/2015",
        "OwnerUserId": 17908.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "framework regularization theory see regularization theory neural network architecture girosi et al allows tackle problem look good kernel systematic way idea kernel determine smoothness stabilizer analogous control complexity mdl sense bias-variance error decomposition idea attempt solve problem differential operator like example prove result follow solution green function associate regularizer mean cross-validation search good value order differential operator",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "15": 0.1713338,
            "43": 0.38182822
        }
    },
    {
        "Id": 441093,
        "PostTypeId": 2,
        "ParentId": 441067.0,
        "AcceptedAnswerId": "",
        "CreationDate": "16/12/2019",
        "OwnerUserId": 686.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "least two advantage cox model survival data deal censor logistic regression allow survival time rather survive usually provide power often relevant question say variable let suppose surgery v death usually important distinguish die one day die year",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 11,
        "Topics": {
            "4": 0.16953993,
            "43": 0.31697822
        }
    },
    {
        "Id": 255319,
        "PostTypeId": 2,
        "ParentId": 254922.0,
        "AcceptedAnswerId": "",
        "CreationDate": "09/01/2017",
        "OwnerUserId": 144529.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "record apparently original question refer methodology first described article begg gray calculation polychotomous logistic regression parameter use individualize regression hosmer mention page apply logistic regression",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "11": 0.19948679,
            "43": 0.21835616
        }
    },
    {
        "Id": 301320,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 301322.0,
        "CreationDate": "04/09/2017",
        "OwnerUserId": 34581.0,
        "Score": 2,
        "ViewCount": 1267.0,
        "Title": "urn two color ball probability select specific color",
        "Body": "ball urn black white three remove color note probability white ball chosen next answer reason must faulty initial three ball remove possible configuration answer p w p p w b p b p w c p c p w p p remove st black ball p remove second black ball p remove third black ball p p b remove black one white also p c p math give",
        "Tags": [
            "conditional-probability"
        ],
        "AnswerCount": 3.0,
        "CommentCount": 0,
        "Topics": {
            "13": 0.20864742,
            "19": 0.1449865,
            "27": 0.11425519,
            "36": 0.115189746,
            "43": 0.32502127
        }
    },
    {
        "Id": 377412,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 377424.0,
        "CreationDate": "16/11/2018",
        "OwnerUserId": 101192.0,
        "Score": 0,
        "ViewCount": 62.0,
        "Title": "decide test use look difference kaplan-meier curve",
        "Body": "conduct survival analysis dissertation run bit road block test use look difference kaplan-meier curve treatment previous study similar design mine use wilcox-gehan test want make sure choose test power study design briefly two group relatively small sample size n n control group randomly right-censored individual pair clinical finding experimental group mortality anyone strong opinion test would appropriate test know mantel-haenzel logrank test peto peto logrank test gehan generalize wilcoxon ranksum test peto peto prentice generalize wilcoxon test tarone ware modify wilcoxon test miss please let know thanks advance",
        "Tags": [
            "survival",
            "kaplan-meier",
            "logrank"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "4": 0.17063254,
            "39": 0.17417513,
            "43": 0.27622676
        }
    },
    {
        "Id": 353981,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "01/07/2018",
        "OwnerUserId": "",
        "Score": 1,
        "ViewCount": 26.0,
        "Title": "epidemiology perform direct standardization demographic exist population",
        "Body": "suppose data cancer incidence different demographic u county direct standardization way accounting demographic difference county cancer rate demographic give county population count demographic entire u directly standardize rate calculate begin align sum r n end align option deal county population certain demographic two alternative think adjoin demographic every county observes every demographic-specific rate would throw lot information treat rate miss data seem like would require many unverifiable assumption direct standardization break point approach",
        "Tags": [
            "missing-data",
            "epidemiology"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 2,
        "Topics": {
            "2": 0.13857935,
            "4": 0.122230686,
            "43": 0.32953283
        }
    },
    {
        "Id": 245765,
        "PostTypeId": 2,
        "ParentId": 245763.0,
        "AcceptedAnswerId": "",
        "CreationDate": "14/11/2016",
        "OwnerUserId": 56405.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "calculate first change expression- integration change term limit calculate easily",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "20": 0.23399636,
            "43": 0.37280267,
            "45": 0.12774928
        }
    },
    {
        "Id": 409706,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "23/05/2019",
        "OwnerUserId": 63623.0,
        "Score": 0,
        "ViewCount": 1078.0,
        "Title": "exclude variable mean use spss run regression",
        "Body": "use spss run linear regression several predictor case threw variable spss show regression model variable bottom also show table name exclude variable sure mean suspect may detection multicollinearity involve variable understand vifs table show multicollinearity issue furthermore variable still include previous model exclude variable mean",
        "Tags": [
            "regression",
            "spss"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 3,
        "Topics": {
            "11": 0.29470274,
            "43": 0.34256765
        }
    },
    {
        "Id": 294387,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "25/07/2017",
        "OwnerUserId": 166043.0,
        "Score": 0,
        "ViewCount": 51.0,
        "Title": "test statistic suspiciously low explain extremely large dataset",
        "Body": "data track purchase non purchase online website dataset contains bunch demographic feature state age etc also whether individual purchase product percentage visitor purchase product want see specific feature statistically explain whether customer make purchase example visitor china purchase product would take mean asian demographic group focus sale effort market etc do far create function randomly sample array length function sample array time time calculate ab proportion sample make kind skeptical absolute difference never get high lead observe value even p-value intuitively feel right think might sample take array length length still feel skeptical thought approach",
        "Tags": [
            "hypothesis-testing",
            "t-test",
            "p-value"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 12,
        "Topics": {
            "37": 0.116541594,
            "43": 0.32071248
        }
    },
    {
        "Id": 243965,
        "PostTypeId": 2,
        "ParentId": 243961.0,
        "AcceptedAnswerId": "",
        "CreationDate": "03/11/2016",
        "OwnerUserId": 131956.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "recommend book deep learn ian goodfellow yoshua bengio aaron courville available cover aspect anns also fairly new technique table content part apply math machine learn basic linear algebra probability information theory numerical computation machine learn basic part ii modern practical deep network deep feedforward network regularization deep learn optimization training deep model convolutional network sequence model recurrent recursive net practical methodology application part iii deep learn research linear factor model autoencoders representation learn structure probabilistic model deep learn monte carlo method confront partition function approximate inference deep generative model",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "6": 0.17602405,
            "15": 0.17495343,
            "29": 0.22242135,
            "43": 0.2151939
        }
    },
    {
        "Id": 403505,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "17/04/2019",
        "OwnerUserId": 245040.0,
        "Score": 0,
        "ViewCount": 5.0,
        "Title": "interaction term continuous moderator independent dummy variable ols",
        "Body": "would like check interaction independent contiunous variable leverage choice auditor dummy variable audit big dependent variable contiunous number day till disclosure annual report read difficult interpret interaction term continuous variable continuous iv continuous moderator one transform moderator dummy make difference regard validity interpretation interaction term coefficient moderator continuous iv dummy thanks advance johanna",
        "Tags": [
            "least-squares",
            "interaction"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 0,
        "Topics": {
            "11": 0.34355488,
            "30": 0.15975942,
            "43": 0.19050696
        }
    },
    {
        "Id": 94472,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "20/04/2014",
        "OwnerUserId": 43653.0,
        "Score": 3,
        "ViewCount": 734.0,
        "Title": "machine learn method use geographic system prediction",
        "Body": "try experiment geographic system prediction work classify location sell product need analyze hestorical data predict success product specific area specific time lot experience statistic model data beyond high school statistic course kinda confuse event represent number item sell location lat long date anyone give example classify geographic system predict value base data far know use linear model two variable lat long someone recommend use density estimation instead use cluster use one-class svm give point one class expect learn separation member class anything else read another post use poisson counting model meanwhile lot negative point problem find good result suggest approach tackle problem",
        "Tags": [
            "svm",
            "predictive-models",
            "poisson-distribution",
            "panel-data",
            "libsvm"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 2,
        "Topics": {
            "43": 0.41432413
        }
    },
    {
        "Id": 21711,
        "PostTypeId": 2,
        "ParentId": 21631.0,
        "AcceptedAnswerId": "",
        "CreationDate": "25/01/2012",
        "OwnerUserId": 8242.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "use mean integrate square error mise cross-validation method past success oversmoothing problem could due poor method least square tends general agree previous response suggest consider reasonable respect nature data deal eyeball also fairly helpful mise cross-validation provide much objective method bandwidth selection",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "43": 0.48281133
        }
    },
    {
        "Id": 288037,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 288041.0,
        "CreationDate": "29/06/2017",
        "OwnerUserId": 167231.0,
        "Score": 2,
        "ViewCount": 744.0,
        "Title": "shallow network outperform deeper one accuracy",
        "Body": "try train different architecture convolutional neural network cnn binary classification task part undergraduate research statistical method give accuracy give cnns try strange thing relatively shallow network outperform deeper one shallow network talk follow denotes convolutional layer max-pooling layer fully-connected layer one-neuron output one class another class one result accuracy task deeper one like vgg- vgg- actually reduce number filter neuron layer sorry go deeper wider gpu core-i laptop seem get stuck accuracy already epoch equal accuracy achieve use statistical method train epoch time course make sure sample two class use training balance network bias overfitted towards one class use relu activation function rmsprop optimizer also use dropout regularization guess learn rate deeper network small get stuck local minimum weight space deeper architecture smoother vast valley local minimas dropout intend solve problem epoch small deep network need train longer converge object classify cell nucleus image think need super deep network super long time train use classify different object like cat dog donkey etc maybe need take well look data corruption checked obvious bias normalize nucleus image filter background use computer vision method assign background pixel range anyone expertise field give hint really go guess nessesary sure answer appreciate thank advance",
        "Tags": [
            "machine-learning",
            "neural-networks",
            "deep-learning",
            "conv-neural-network"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "6": 0.32365683,
            "15": 0.13867863,
            "43": 0.3314774
        }
    },
    {
        "Id": 18959,
        "PostTypeId": 2,
        "ParentId": 18945.0,
        "AcceptedAnswerId": "",
        "CreationDate": "25/11/2011",
        "OwnerUserId": 2981.0,
        "Score": 4,
        "ViewCount": "",
        "Title": "",
        "Body": "terminology may vary field field however use term define comment difference among follow term three equivalent systematic error error reduce one increase sample size increase sample size reduces random error systematic error comment term take field epidemiology specifically rothman colleague discussion error chapter modern epidemiology summarize goal investigator provide accurate estimate measure e g mean relative risk hazard ratio et cetera within population accurate estimate one valid precise valid estimate point estimate eg mean relative risk hazard ratio et cetera close true value population precise estimate narrow confidence level around point estimate addition estimate internally-valid relative study population externally-valid relative generalize population departure accuracy cause error two main type error systemic error random error systemic error often refer bias result estimate valid systemic error include error due confound selection bias information bias confound generally correct technique stratification regression selection information bias traditionally either ignore qualitatively assess analysis probably due unfamiliarity appropriate bias analysis however methodology qunatitative bias analysis exist e g lash tl ak fink random error result estimate precise random error include sample error random measurement error among others method increase precision include increase study size increase study efficiency precision-optimizing statistical analysis pool regression update illustrate increase sample size decrease systematic error dartboard analogy copy cv post matter many dart thrown board point estimate go shift towards true bulls-eye high bias bias equivalent systematic error variance equivalent random error",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 5,
        "Topics": {
            "26": 0.20952728,
            "43": 0.29394683,
            "47": 0.12536176
        }
    },
    {
        "Id": 328231,
        "PostTypeId": 2,
        "ParentId": 328225.0,
        "AcceptedAnswerId": "",
        "CreationDate": "12/02/2018",
        "OwnerUserId": 73527.0,
        "Score": 18,
        "ViewCount": "",
        "Title": "",
        "Body": "general harmonic mean prefer one try average rate instead whole number case f -measure harmonic mean penalize small precision recall whereas unweighted arithmetic mean imagine average arithmetic mean harmonic mean harmonic mean require precision recall high addition precision recall close together harmonic mean close arithmetic mean example harmonic mean compare arithmetic mean whether desirable property probably dependent use case typically consider good finally note whuber state comment harmonic mean indeed weight arithmetic mean",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 3,
        "Topics": {
            "25": 0.20693052,
            "43": 0.2645095,
            "48": 0.1011637
        }
    },
    {
        "Id": 337922,
        "PostTypeId": 2,
        "ParentId": 337917.0,
        "AcceptedAnswerId": "",
        "CreationDate": "01/04/2018",
        "OwnerUserId": 11852.0,
        "Score": 3,
        "ViewCount": "",
        "Title": "",
        "Body": "roc graph relate sample size directly small large sample roc graph method classifier use create roc different performance performance characteristic relate sample size result proportion overall sample examine author try convey performance method preferable aim detect subsample sample true positive want strongly limit number false positive instance e g patient suggest invasive medical procedure similarly want ensure vast majority true positive instance detect even expense substantial number false positive method b would preferable e g patient suggest take relatively inexpensive screen test different application may require exchange high recall high precision frank harrell correctly note well-defined utility function use maximise expect utility one case arbitrary roc graph pick methodbased overall goal e g medical diagnostics method v medical screen method b",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 4,
        "Topics": {
            "29": 0.11808693,
            "43": 0.3143087,
            "48": 0.24814846
        }
    },
    {
        "Id": 55955,
        "PostTypeId": 2,
        "ParentId": 55954.0,
        "AcceptedAnswerId": "",
        "CreationDate": "12/04/2013",
        "OwnerUserId": 686.0,
        "Score": 3,
        "ViewCount": "",
        "Title": "",
        "Body": "large condition number say independent data independent colinear dangerous degree look variance proportion large condition index see variable delete look partial least square ridge regression search colinearity option",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "11": 0.18684317,
            "23": 0.10932947,
            "43": 0.26511332
        }
    },
    {
        "Id": 327144,
        "PostTypeId": 2,
        "ParentId": 327126.0,
        "AcceptedAnswerId": "",
        "CreationDate": "06/02/2018",
        "OwnerUserId": 7290.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "glm gaussian response distribution identity link function ols regression model anova also special case standard regression model glm however design repeat measure require take non-independence data account repeat measure anova convention people say anova e rmanova mean repeat measure version likewise people say glm typically refer could ambiguous note different name thing thing assumption think different model least one wrong rate use rmanova data like describe assumption sphericity hold addition residual normally distribute",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "1": 0.1232247,
            "21": 0.23976691,
            "43": 0.21007073
        }
    },
    {
        "Id": 265759,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "06/03/2017",
        "OwnerUserId": 103501.0,
        "Score": 1,
        "ViewCount": 72.0,
        "Title": "variance point estimate least square",
        "Body": "try practice problem quite stuck question follow coefficient regressors figure asymptotic normality property l estimate hence think confidence interval however unsure express term variance l estimator would greatly appreciate help",
        "Tags": [
            "regression",
            "confidence-interval",
            "least-squares"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 1,
        "Topics": {
            "5": 0.13209973,
            "26": 0.17302902,
            "43": 0.3487161
        }
    },
    {
        "Id": 200500,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 201591.0,
        "CreationDate": "08/03/2016",
        "OwnerUserId": 35989.0,
        "Score": 107,
        "ViewCount": 5786.0,
        "Title": "asa discus limitation -values alternative",
        "Body": "already multiple thread tag p-values reveal lot misunderstanding ten month ago thread psychological journal ban -values american statistical association say analysis end calculation -value american statistical association asa believe scientific community could benefit formal statement clarify several widely agree upon principle underlie proper use interpretation -value committee list approach possible alternative supplement -values view prevalent misuse misconception concern -values statistician prefer supplement even replace -values approach include method emphasize estimation test confidence credibility prediction interval bayesian method alternative measure evidence likelihood ratio bayes factor approach decision-theoretic model false discovery rate measure approach rely assumption may directly address size effect associate uncertainty whether hypothesis correct let imagine post- -values reality asa list method use place -values well real-life replacement researcher use -values life imagine kind question appear post- -values reality maybe let try one step ahead reasonable alternative apply out-of-the-box approach convince lead researcher editor reader follow-up blog entry suggests -values unbeatable simplicity p-value require statistical model behavior statistic null hypothesis hold even model alternative hypothesis use choose \u201c good \u201d statistic would use construct p-value alternative model correct order p-value valid useful e control type error desire level offering power detect real effect contrast wonderful useful statistical method likelihood ratio effect size estimation confidence interval bayesian method need assume model hold wider range situation merely test null maybe true easily replace know broad main question simple best real-life alternative -values use replacement asa asa statement statistical significance -values american statistician press",
        "Tags": [
            "hypothesis-testing",
            "bayesian",
            "p-value",
            "frequentist"
        ],
        "AnswerCount": 10.0,
        "CommentCount": 9,
        "Topics": {
            "29": 0.10475167,
            "43": 0.41520786,
            "47": 0.11463481
        }
    },
    {
        "Id": 312507,
        "PostTypeId": 2,
        "ParentId": 312487.0,
        "AcceptedAnswerId": "",
        "CreationDate": "07/11/2017",
        "OwnerUserId": 86794.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "real clear want analysis approach seem accomplish essentially determine whether extent particular pred differs -parameter v -parameter model note difficult assign meaning difference particular point residual small parameter mean something special relationship rd predictor point function overall fit thus rest data might consider turn influence leverage analysis interested much particular point sway fit",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "11": 0.13826011,
            "43": 0.37143213
        }
    },
    {
        "Id": 55610,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 55638.0,
        "CreationDate": "09/04/2013",
        "OwnerUserId": 24109.0,
        "Score": 3,
        "ViewCount": 5728.0,
        "Title": "interaction term poisson regression",
        "Body": "confuse interpretation interaction term poisson regression hypothetical dataset imagine individual duplicate number time random noise make two group analyse data use poisson regression count data group condition interaction group error rate increase condition e incidence rate ratio analyse data use anova transform data likely yield significant group condition interaction simulated poisson regression look proportional difference across condition anova sensitive absolute difference seem poisson regression us multiplicative scale via log transformation additive effect poisson regression become multiplicative data analyse use ols anova interaction term truthful matter interpretation priori reason choose one example could argue count data exhibit ratio property analyse ratio interval accord stevens thanks hope make sense",
        "Tags": [
            "interaction",
            "poisson-regression"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 2,
        "Topics": {
            "5": 0.10244069,
            "19": 0.19560823,
            "43": 0.13259299
        }
    },
    {
        "Id": 342393,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "24/04/2018",
        "OwnerUserId": 205664.0,
        "Score": -1,
        "ViewCount": 106.0,
        "Title": "fisher exact test question",
        "Body": "group use method group b use e method total possible problem issue group use method find issue group b use e method find issue input data fisher exact test find p-value give p-value",
        "Tags": [
            "self-study",
            "fishers-exact"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 2,
        "Topics": {
            "3": 0.14454083,
            "18": 0.14880553,
            "39": 0.113785185,
            "43": 0.27312696
        }
    },
    {
        "Id": 429385,
        "PostTypeId": 2,
        "ParentId": 350443.0,
        "AcceptedAnswerId": "",
        "CreationDate": "30/09/2019",
        "OwnerUserId": 261297.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "look explanation actual statistic python code calculate p value",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "3": 0.34238374,
            "44": 0.29032287,
            "49": 0.10763306
        }
    },
    {
        "Id": 57723,
        "PostTypeId": 2,
        "ParentId": 57710.0,
        "AcceptedAnswerId": "",
        "CreationDate": "30/04/2013",
        "OwnerUserId": 1320.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "easy solution would use linear svm case support vector combine together single weight vector kernel svms might want look budget svms focus strategy reduce number support vector kept optimization one interest reference source code z wang k crammer vucetic \u201c break curse kernelization budget stochastic gradient descent large-scale svm training \u201d journal machine learn research vol pp \u2013 pdf source code",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "0": 0.12769009,
            "43": 0.26419777
        }
    },
    {
        "Id": 213863,
        "PostTypeId": 2,
        "ParentId": 212351.0,
        "AcceptedAnswerId": "",
        "CreationDate": "22/05/2016",
        "OwnerUserId": 53690.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "subject-matter explanation seasonal cycle rather hour yes could try use seasonal frequency model perhaps mechanical process depend time day seasonal period hour furthermore subject-matter explanation cycle hour would stick -hour cycle also signal -hour frequency much strong -hour frequency could include plot post show u periodogram side note quite usual remove mean subtract sample mean observation rather divide play role anyway",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "24": 0.29697403,
            "43": 0.31361106
        }
    },
    {
        "Id": 160610,
        "PostTypeId": 2,
        "ParentId": 160439.0,
        "AcceptedAnswerId": "",
        "CreationDate": "09/07/2015",
        "OwnerUserId": 78706.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "firstly think might answer take say grain salt relatively new answer question compute test statistic normally compare emerge statistic t-distribution n- predict value degree freedom normal distribution hold long null hypothesis normally distribute usage standard error instead standard diviation could use know use calcualtions test statistic know usable",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "43": 0.30266413,
            "44": 0.14393497,
            "49": 0.17915714
        }
    },
    {
        "Id": 111986,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "15/08/2014",
        "OwnerUserId": 54129.0,
        "Score": 6,
        "ViewCount": 4978.0,
        "Title": "post hoc test r",
        "Body": "quite simple question find good clear precise answer look way perform post hoc test chi test variable var good fair poor var b c contingency table r perform obtain p relation var var one handle contribution analysis pairwise prop test",
        "Tags": [
            "r",
            "chi-squared",
            "post-hoc"
        ],
        "AnswerCount": 2.0,
        "CommentCount": 4,
        "Topics": {
            "43": 0.2843297,
            "44": 0.110850975
        }
    },
    {
        "Id": 380193,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "04/12/2018",
        "OwnerUserId": 214683.0,
        "Score": 1,
        "ViewCount": 38.0,
        "Title": "report check linear regression assumption",
        "Body": "step analysis know past research imply linear relationship set hypothesis linear function get dataset researcher give data use multiple linear regression model calculate coefficient -values model fit give significant result conclude model valid say bos insists test assumption linear regression met checked linearity normality residual equivariance independence variance say result check fail support assumption question report result check result linear regression result linear regression reason assumption met sufficient evidence dataset valid linear regression prerequisite model building proper conclusion check actual analysis reason reason decide build model past research imply linear relationship moreover hypothesis like assumption met actual analysis first additional information model validity report result assumption check later",
        "Tags": [
            "regression",
            "assumptions"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 1,
        "Topics": {
            "5": 0.1675882,
            "43": 0.18998213,
            "44": 0.119803585,
            "46": 0.18304175
        }
    },
    {
        "Id": 105442,
        "PostTypeId": 2,
        "ParentId": 105437.0,
        "AcceptedAnswerId": "",
        "CreationDate": "01/07/2014",
        "OwnerUserId": 14154.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "first thing need consider type miss data mcar miss completely random mar miss random nmar miss random data mcar individual miss value exclude analysis valid inference obtain nevertheless still result lose statistical power data mar use approach like complete case analysis lead bias estimate suggestion case use multiple imputation check paper guideline best practice regard miss data multiple imputation",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "14": 0.27712193,
            "33": 0.12491356,
            "43": 0.33383572
        }
    },
    {
        "Id": 82501,
        "PostTypeId": 2,
        "ParentId": 82482.0,
        "AcceptedAnswerId": "",
        "CreationDate": "16/01/2014",
        "OwnerUserId": 887.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "training pattern incorrectly classify indeed support vector pattern support vector must right side margin decision boundary wrong side margin",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "0": 0.42574775,
            "36": 0.10263436,
            "43": 0.26482674
        }
    },
    {
        "Id": 415775,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "02/07/2019",
        "OwnerUserId": 252599.0,
        "Score": 1,
        "ViewCount": 66.0,
        "Title": "trend seasonality determination time series without look graph",
        "Body": "article read describe determine trend seasonal t effect roll eye graph graph nice visual representation look way either python r way discover t short want feed time series data program backend output value determine trend yes seasonal effect exists time series yes really appreciate idea thanks",
        "Tags": [
            "time-series",
            "forecasting",
            "seasonality",
            "trend",
            "test-for-trend"
        ],
        "AnswerCount": 2.0,
        "CommentCount": 0,
        "Topics": {
            "24": 0.35965917,
            "43": 0.31765833
        }
    },
    {
        "Id": 243614,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "01/11/2016",
        "OwnerUserId": 110394.0,
        "Score": 1,
        "ViewCount": 80.0,
        "Title": "make decline amplitude time series trend",
        "Body": "forecasting demand use holt-winters model particular product class examine performance far quarter ever forecast q surprised note average value daily estimator pretty much average observe value forecast level pretty much spot far additionally intra-year seasonality seem capture broadly correctly sense value go correct day cycle day within week week within year fairly predictable however notice model wildly exaggerate swing historical weekday historical weekday first thought issue parameterisation choice start value investigate discover actual culprit although seasonal cycle remain constant term day part degree swing direction lot less day quarter q compare series ti-ti- year word seasonal cycle term direction day magnitude far pattern consistently different simply built really simple algorithm excel cap increase decrease peak trough day percentage-terms increase decrease observe year q sample period actually work perfectly reduces mean absolute percentage error max max however pretty sure absolutely zero statistical validity sure meaningful dramatically improves performance past day particular know whether ratio seasonal-cycle amplitude year last year constant whole quarter whether temporary phenomenon sure would probably possible modify seasonal-cycle amplitude manipulate parameter model comply reality think would equally arbitrary course ideally data exogenous variable determine variation build model incorporates parameter unitary time series data exogenous data available one final complicate factor take series absolute value day-to-day percentage swing year analyse alongside equivalent value correspond day last year notice gap two decline course year sample period two appear gradually converge know whether meaningful useful",
        "Tags": [
            "time-series",
            "forecasting",
            "seasonality",
            "exponential-smoothing"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 0,
        "Topics": {
            "24": 0.16088101,
            "28": 0.22147799,
            "43": 0.38060933
        }
    },
    {
        "Id": 244571,
        "PostTypeId": 2,
        "ParentId": 244468.0,
        "AcceptedAnswerId": "",
        "CreationDate": "06/11/2016",
        "OwnerUserId": 136730.0,
        "Score": 9,
        "ViewCount": "",
        "Title": "",
        "Body": "assume variable standardize correlation transformation like mention unit length scale version standardize model change correlation variable calculate standardize transformation original linear model make let denote design matrix standardize transformation begin align mathbf x begin bmatrix x ldots x p- x ldots x p- vdots vdots vdots vdots x n ldots x n p- end bmatrix end align begin align mathbf x x begin bmatrix n mathbf mathbf mathbf r xx end bmatrix end align correlation matrix variable also know begin align sigma hat beta sigma mathbf x x sigma begin bmatrix frac n mathbf mathbf mathbf r xx end bmatrix end align -th diagonal term need prove permute row column get result let define begin align mathbf x begin bmatrix x ldots x p- x ldots x p- vdots vdots vdots x n ldots x n p- end bmatrix mathbf x begin bmatrix x x vdots x n end bmatrix end align note matrix different design matrix since care coefficient variable -vector design matrix ignore calculation hence use schur complement begin align r xx r r mathbf x r mathbf x mathbf x r mathbf x r r mathbf x r mathbf x mathbf x r mathbf x mathbf x r mathbf x mathbf x r mathbf x beta mathbf x mathbf x mathbf x beta mathbf x end align regression coefficient except intercept fact intercept origin since variable standardize mean zero hand would straightforward write everything explicit matrix form begin align r frac ssr ssto frac beta mathbf x mathbf x mathbf x beta mathbf x beta mathbf x mathbf x mathbf x beta mathbf x end align therefore begin align vif r xx frac -r end align",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "19": 0.51037943
        }
    },
    {
        "Id": 277531,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 277726.0,
        "CreationDate": "04/05/2017",
        "OwnerUserId": 147671.0,
        "Score": 1,
        "ViewCount": 49.0,
        "Title": "design experiment parameter measurement continuous",
        "Body": "like design experiment involves factor decide design yet prolly taguchi fraction factorial one use work measurable effect case measure directly work formulation paste measure effect compression r able measure must work qualitative measure like low medium high compression think anova test like know check factor level affect compression forget anova plan experiment accord design",
        "Tags": [
            "anova",
            "experiment-design",
            "qualitative"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 3,
        "Topics": {
            "11": 0.10539606,
            "21": 0.26994872,
            "43": 0.22563481
        }
    },
    {
        "Id": 189644,
        "PostTypeId": 2,
        "ParentId": 189614.0,
        "AcceptedAnswerId": "",
        "CreationDate": "07/01/2016",
        "OwnerUserId": 53955.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "base text need compute p-value thus identify time series stationary",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "24": 0.42725238,
            "43": 0.3195791
        }
    },
    {
        "Id": 330472,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "25/02/2018",
        "OwnerUserId": 122960.0,
        "Score": 5,
        "ViewCount": 770.0,
        "Title": "statistical control work logistic regression",
        "Body": "want make sure generally interpret model finding accurately fair say log-odds associate predictor assumes others held constant make similar standard linear regression would change base continuous v binary predictor",
        "Tags": [
            "logistic",
            "multiple-regression",
            "controlling-for-a-variable"
        ],
        "AnswerCount": 2.0,
        "CommentCount": 1,
        "Topics": {
            "5": 0.18463281,
            "11": 0.2770632,
            "43": 0.2573296
        }
    },
    {
        "Id": 161804,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 162734.0,
        "CreationDate": "16/07/2015",
        "OwnerUserId": 21919.0,
        "Score": 7,
        "ViewCount": 512.0,
        "Title": "lindeberg clt exponential independent variable",
        "Body": "crossposted math stackexchange clt independent non-identically distribute exponential variable problem self-study qualify exam problem suppose independent exponentially distribute random variable attempt solution use liapunov condition somehow get stuck last step justification link another user attempt answer use lindeberg condition somehow condition give problem conform solution assumption anyone hint proceed thank",
        "Tags": [
            "distributions",
            "central-limit-theorem"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 8,
        "Topics": {
            "20": 0.38861302,
            "43": 0.20425025,
            "45": 0.1812968
        }
    },
    {
        "Id": 338514,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "04/04/2018",
        "OwnerUserId": 202132.0,
        "Score": 1,
        "ViewCount": 39.0,
        "Title": "connection logit model power law distribution",
        "Body": "wonder whether appropriate make connection use logit model power law distribution dependent variable categorical ordinal therefore fitting logit model moreover found dependent variable follow power law distribution could therefore create link distribution dependent variable choice model compare alternative use probit model thought argue compare probit model logit model may appropriate due assume error distribution extreme value type however afraid confuse several different concept would appreciate insight thought previous discussion platform found logit well case extreme independent variable also always argue probit logit prediction similar thank much already advance",
        "Tags": [
            "ordered-logit",
            "power-law"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 0,
        "Topics": {
            "1": 0.12650277,
            "11": 0.26552448,
            "43": 0.3158781
        }
    },
    {
        "Id": 171622,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "08/09/2015",
        "OwnerUserId": 44902.0,
        "Score": 0,
        "ViewCount": 64.0,
        "Title": "sequential pattern mining mutiple sequence",
        "Body": "someone give hint good approach find frequent pattern multiple sequence sequence combination character e n u r g length sequence different example one sequence mmsenemsnmmemmemmssmmmemmmsmmemmssmemmsmmmemsmmmemsmemsmmmmmmmmmemsmemsmmmemsmmmmmmmmmmemsmemsmennmsmemsmemmsmmmesmemmmsmmememssmemmmmmmsmgsmgsmgsmmgsmgsmmemmsgsmmmnsmgsmmmemmmmmsmmmgsmnrmgsmnsgsmgsmmemmnsmgsmmsmgsmemmmsgsmegsmerrgsmgsmmmmmnemmnsmmnsmmmsmgsmgsmmemsgsmmmmmmmmmmrgsmmrgsmrgsmrgsmmrgsmnrresnmmmmrgsmmsgsmrrrrugsmmnrrnemssgsmrgsmemsrrmrrugsmmmmmnmmmmmmmsmgsmmrrmrrummngsmrrmmumrrmnsnrmuuumumuummmuummsnuumummummmmggrrrrrrugsrgsmusmmurrusmmmmmmngsrrrummmumgsmmnrrrusmmmmmnrrrugsmmgrrrrrrugsmrrmuugsmmmnsrrgsmmmmmmrrmmrrrrugsmrrmugsmemgsmmsrgsmrrummmrrugsmmmusmnuusrumumusmmmmmesrrmumrgsmmmnnmssrgsmgsmnrrrrrrrugsmmnnsrgsmmrrmmrrmmmmmmmmrgsmmrrmrrmmmmmrrmmmmmrrmmmnsnmsmmuurrummmmrrmmmmmmmmmmmmmmmmmmmmmmuummnuuuumummmummmssuuruuusnuuuruuumuuummmmuummmmmmmmummmmmmummmmmmmmummmmmmmmsnrmmummmmmummmenrummummummeummmmrrmnmmsmmmmmmmmmmmmmmmmmmmmummnrummuummuummummmmmmmummmummummumummmmmmmmmummuuuuummmmmmmmmmmm look method detect frequent pattern order sequence thanks help",
        "Tags": [
            "sequence-analysis",
            "sequential-pattern-mining"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 3,
        "Topics": {
            "15": 0.13198319,
            "27": 0.18115151,
            "34": 0.17263791,
            "43": 0.34834838
        }
    },
    {
        "Id": 365229,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 365953.0,
        "CreationDate": "03/09/2018",
        "OwnerUserId": 175687.0,
        "Score": 3,
        "ViewCount": 182.0,
        "Title": "feature distribution cross-validation",
        "Body": "case binary classification stratify cross-validation ensures fold contains roughly proportion two type class label make sense also ensure feature distribution maintain would expect algorithm bias class distribution",
        "Tags": [
            "machine-learning",
            "cross-validation",
            "sampling",
            "feature-selection"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 1,
        "Topics": {
            "0": 0.16906576,
            "10": 0.15762347,
            "29": 0.113751076,
            "43": 0.23572151
        }
    },
    {
        "Id": 219288,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "16/06/2016",
        "OwnerUserId": 54267.0,
        "Score": 1,
        "ViewCount": 409.0,
        "Title": "pair t-test simple latent change score model",
        "Body": "update problem embarrassingly solve specify intercept regression equation show question distinct enough change score model lavaan migrate include link reference purpose aforementioned post attempt calculate latent change score two wave observation indicate believe model specify over-identified thought day reading little think may over-complicating problem recent paper colman colleague show certain condition latent change score model equivalent simple t-test author specify path model paper follow note path diagram author state model equivalent pair sample t-test author additionally make follow constraint model residual error intercept set zero auto-regressive path set path set note within body paper author also specify baseline-centering value author supply data paper convenience provide output data follow code chunk assign object name try implement model follow probably obvious overly-restricted specification result unestimated model respecify measurement model get follow output however still appear get intercept estimate path coefficient value correct value base paper thought model specify order produce equivalent result t-test",
        "Tags": [
            "r",
            "sem",
            "growth-model",
            "change-scores"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "3": 0.16017349,
            "29": 0.12043471,
            "43": 0.24133824
        }
    },
    {
        "Id": 239080,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "08/10/2016",
        "OwnerUserId": "",
        "Score": 0,
        "ViewCount": 41.0,
        "Title": "derivative log respect parameter equates zero find estimate",
        "Body": "try figure solve follow expression parameter use maximum likelihood method solve problem huge omit get would like find estimate help please",
        "Tags": [
            "maximum-likelihood"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 8,
        "Topics": {
            "20": 0.1698752,
            "23": 0.10261261,
            "41": 0.23168659,
            "43": 0.21584623
        }
    },
    {
        "Id": 355931,
        "PostTypeId": 2,
        "ParentId": 355929.0,
        "AcceptedAnswerId": "",
        "CreationDate": "13/07/2018",
        "OwnerUserId": 66281.0,
        "Score": 4,
        "ViewCount": "",
        "Title": "",
        "Body": "general optimization method care computational cost compute objective function however strategy deal case usually use optimization method reference read paper efficient optimization computationally expensive objective function survey model optimization strategy solve high-dimensional design problem computationally-expensive black-box function note many way realize speedup strategy example strategy approximate objective function first cite paper us recursive surrogate objective function another common way bayesian approximation gaussian process combine exploration-exploitation strategy guess optimum position use gradient information result bayesian optimization approach suggest fabianwerner comment specific problem choose speedup strategy optimization method want use realize deep learn general strategy massive parallelize example generate self-play game alphazero do multiple gpus tpus parameter update usually partially synchronize eventually approximately converge",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "15": 0.14942114,
            "23": 0.32354036,
            "43": 0.34343353
        }
    },
    {
        "Id": 36325,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 37363.0,
        "CreationDate": "14/09/2012",
        "OwnerUserId": 4222.0,
        "Score": 1,
        "ViewCount": 522.0,
        "Title": "derive pdf autoregressive model",
        "Body": "hope right place way ask question try understand derive probability density function x ar model order k give t-k past observation primarily refer paper section cite japanese book derivation could find similar derivation time series book shumway stoffer pourahmadi someone please provide appropriate reference explain thanks time iinception",
        "Tags": [
            "time-series",
            "autoregressive"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 3,
        "Topics": {
            "19": 0.21090525,
            "20": 0.19642372,
            "24": 0.17553727,
            "43": 0.14921236
        }
    },
    {
        "Id": 319878,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 320048.0,
        "CreationDate": "21/12/2017",
        "OwnerUserId": 45374.0,
        "Score": 1,
        "ViewCount": 949.0,
        "Title": "mean neural network training validation loss validation accuracy drop epoch",
        "Body": "simple question find straight answer training neural network classify medical image initially focus validation accuracy epoch determine network generalise test accuracy unseen dataset see validation loss also important sometimes validation loss drop even thought epoch validation accuracy also go slightly ultimately test accuracy go gold standard guess thats use guide training adjust parameter obviously validation loss start go validation accuracy start drop indicates overfitting validation loss validation accuracy drop epoch",
        "Tags": [
            "neural-networks",
            "conv-neural-network",
            "accuracy",
            "loss-functions"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 3,
        "Topics": {
            "6": 0.13097443,
            "10": 0.31722945,
            "23": 0.17247275,
            "43": 0.2974527
        }
    },
    {
        "Id": 197970,
        "PostTypeId": 2,
        "ParentId": 197612.0,
        "AcceptedAnswerId": "",
        "CreationDate": "22/02/2016",
        "OwnerUserId": 11342.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "gbm gradient descent function space rather parameter space gbm us gradient descent calculate iteration residual tree construction residual thought step direction gamma compute gradient descent step terminal node prediction iteration thought step size correct gamma compute every terminal region base-learner gbm decision tree true although gbm support exponential loss function friedman prove equivalent adaboost",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 6,
        "Topics": {
            "9": 0.21854533,
            "23": 0.29965198,
            "43": 0.19063881
        }
    },
    {
        "Id": 295173,
        "PostTypeId": 2,
        "ParentId": 176834.0,
        "AcceptedAnswerId": "",
        "CreationDate": "29/07/2017",
        "OwnerUserId": 77222.0,
        "Score": 3,
        "ViewCount": "",
        "Title": "",
        "Body": "since know event occur observation right censor data event instead go unrecorded data would right truncate fitting lognormal intercept survival regression model follow give mles location scale parameter lognormal",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "14": 0.15346621,
            "41": 0.11848297,
            "43": 0.21679302,
            "45": 0.17977299
        }
    },
    {
        "Id": 398611,
        "PostTypeId": 2,
        "ParentId": 398581.0,
        "AcceptedAnswerId": "",
        "CreationDate": "20/03/2019",
        "OwnerUserId": 7486.0,
        "Score": 3,
        "ViewCount": "",
        "Title": "",
        "Body": "quite right usual formulation linear mixed effect model matrix form response vector random effect fix effect coefficient vector design matrix fix effect random effect respectively another way write make common distributional assumption clear matrix variance-covariance matrix residual random effect respectively",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "17": 0.11271876,
            "21": 0.107487366,
            "33": 0.29321,
            "43": 0.17817603
        }
    },
    {
        "Id": 432586,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "22/10/2019",
        "OwnerUserId": 235586.0,
        "Score": 0,
        "ViewCount": 16.0,
        "Title": "feature selection hyper-parameter tune via cross validation",
        "Body": "recently read many article book deal cross-validation little bit confuse generally build machine decide hypothesis set train model hypothesis set evaluate performance model via validation set finally evaluate best model performance test set hold-out hypothesis set include various combination hyper-parameters feature choose best feature hyper-parameters model decide combination feature hyper-parameters hypothesis set evalute via validation set yield expensive computation cost best solution get best feature hyper-parameters time",
        "Tags": [
            "cross-validation",
            "feature-selection",
            "hyperparameter"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 0,
        "Topics": {
            "0": 0.10257613,
            "10": 0.5086673
        }
    },
    {
        "Id": 36003,
        "PostTypeId": 2,
        "ParentId": 35952.0,
        "AcceptedAnswerId": "",
        "CreationDate": "10/09/2012",
        "OwnerUserId": 13955.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "silmiarly let since bound continuous dominate convergence theorem hence e x independent",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "20": 0.33083114,
            "43": 0.2717056
        }
    },
    {
        "Id": 240208,
        "PostTypeId": 2,
        "ParentId": 240106.0,
        "AcceptedAnswerId": "",
        "CreationDate": "14/10/2016",
        "OwnerUserId": 76528.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "overall put paragraph one try address question assumption basic idea behind model dictate interpret output thus use model linear regression simply projection random variable unto subspace span independent variable matter actually distribute intuitively add dimension problem provide describe newly add dimension already include model always minimize error model data much minimize error related actual descriptive power add variable add someone astrological sign regression life-time earnings might explain lot variation people life-time accumulation money whereas level education might well variable say linear regression relies bit expert knowledge assumption data knowledge lead model lot nice property like existence confidence interval dispute may p-values etc k-nearest neighbor regression local method size local neighborhood inversely proportional us potentially weight average supply data point describe behavior local neighborhood knn assumption make anything free describe anything collection local behavior flipside miss ci p-values possibility extrapolate model physical interpretation model see would anyone use knn regression interpret datasets relation knn linear regression mindful assumption underlie two different approach meaningful description independent component ie expert knowledge input output related linear regression make sense physical interpretation talk much certain amount input change output make justifiable simplify assumption data knn reasonable choice remember knn tell something small local ensemble data behaves data behaves way outset seem accurate term mse relates back assumption make data also care mse physical interpretation model first thing plot data found say plot would honestly sit try choose knn regression straight line slope always plot data residual analysis data exist vacuum math come rescue provide summary statistic say one best data data collect determines best approach model provide small mse relate analysis plot believe weight regression would much appropriate knn reason believe straight line behaviour cloud data take heteroscedacity consideration sensible armchair point view knn tendency low bias would cause knn regression wiggle around middle graph stabilize end meaningful representation data honest question stab plot plot obvious cluster analysis data set give information data look plot tempt linear regression spline polynomial trig function depend kind system data measure fit right linear model framework data set knn would capture qualitative behavior meaningful sense long read either think ask wrong kind question insist compare two different possibly ill-fitting model toy data completely misunderstood want know downvote sometimes sometimes need extrapolate bit beyond domain model initially train sorely tempt patch together data look like actually try",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "5": 0.12657076,
            "43": 0.3554014
        }
    },
    {
        "Id": 14459,
        "PostTypeId": 2,
        "ParentId": 14444.0,
        "AcceptedAnswerId": "",
        "CreationDate": "18/08/2011",
        "OwnerUserId": 3805.0,
        "Score": 4,
        "ViewCount": "",
        "Title": "",
        "Body": "one famous statistician short powerpoint presentation explain difference bayesian frequentist approach need application understand go bayesiantechniques excellent article entitle basketball beta bayes writtenby matthew richey paul zorn mathematics magazine vol dec pp clear confusion",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "29": 0.15104796,
            "41": 0.2018084,
            "43": 0.28353673,
            "47": 0.19729157
        }
    },
    {
        "Id": 406516,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "04/05/2019",
        "OwnerUserId": 104061.0,
        "Score": 1,
        "ViewCount": 51.0,
        "Title": "linear model pair data",
        "Body": "subject vary know age sex two strongly related dependent variable x z measure two method b method b know expect show reduce value variable x z use group subject several category recommend practice case create hierarchical model lme linear model difference would provide easily understandable result interpret difference somewhat loosely related classic bland-altmann method comparison still fallacious include original value equation seem easy must wrong lm age sex lm age sex lm age sex lm age sex",
        "Tags": [
            "regression",
            "linear-model",
            "paired-data",
            "lm"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 0,
        "Topics": {
            "2": 0.11900206,
            "5": 0.12662356,
            "39": 0.108413704,
            "43": 0.2817431
        }
    },
    {
        "Id": 9964,
        "PostTypeId": 2,
        "ParentId": 9911.0,
        "AcceptedAnswerId": "",
        "CreationDate": "25/04/2011",
        "OwnerUserId": 4110.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "think may want reshape data reduce let change structure data set use observation mention statistical package use r stata matlab nice out-of-the-box reshape command use side thought may need adjust cluster error reshaped data since sound like observation completely independent",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "35": 0.15385875,
            "43": 0.41099912
        }
    },
    {
        "Id": 271358,
        "PostTypeId": 2,
        "ParentId": 271356.0,
        "AcceptedAnswerId": "",
        "CreationDate": "02/04/2017",
        "OwnerUserId": 10278.0,
        "Score": 3,
        "ViewCount": "",
        "Title": "",
        "Body": "continuous task never end mean give reward end since end every often task example reading internet learn math could consider continuous task episodic task last finite amount time example play single game go episodic task win lose episodic task might single reward end task one option distribute reward evenly across action take episode continuous task reward might assign discounting recent reaction receive great reward action long time past receive vanishingly small reward example reward could geometric distance past discount factor",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "13": 0.123676516,
            "15": 0.23991428,
            "43": 0.3030134
        }
    },
    {
        "Id": 409360,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "21/05/2019",
        "OwnerUserId": 219008.0,
        "Score": 0,
        "ViewCount": 18.0,
        "Title": "property copula function simple word",
        "Body": "read lot copula function lately think understand basic concept quite well still trouble understand summary copula property someone help understand property mean simple word",
        "Tags": [
            "copula"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 0,
        "Topics": {
            "3": 0.12100077,
            "43": 0.3530435,
            "45": 0.15452386,
            "47": 0.19870472
        }
    },
    {
        "Id": 231227,
        "PostTypeId": 2,
        "ParentId": 231116.0,
        "AcceptedAnswerId": "",
        "CreationDate": "23/08/2016",
        "OwnerUserId": 7828.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "set match concept dbscan may much noise make easy find suitable parameter try rather large minpts first knn cluster labrls green obtain",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "36": 0.11615698,
            "38": 0.28162426,
            "43": 0.3278788
        }
    },
    {
        "Id": 327804,
        "PostTypeId": 2,
        "ParentId": 327800.0,
        "AcceptedAnswerId": "",
        "CreationDate": "09/02/2018",
        "OwnerUserId": 98646.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "know book follow informative tutorial two lead scholar bayesian nonparametrics ghahramani z july nonparametric bayesian method tutorial presentation uai conference pdf yee whye teh also great tutorial yee whye teh july bayesian nonparametrics document language model edit ghahramani give talk nip",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "15": 0.10746561,
            "29": 0.24495079,
            "41": 0.19137396,
            "43": 0.26637188
        }
    },
    {
        "Id": 264247,
        "PostTypeId": 2,
        "ParentId": 99667.0,
        "AcceptedAnswerId": "",
        "CreationDate": "27/02/2017",
        "OwnerUserId": 102408.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "know answer already accepted thought follow could help future reader try implement handle unbalanced class work pretty good naive bayes text classification unbalanced class",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "0": 0.2608653,
            "43": 0.37689582
        }
    },
    {
        "Id": 192080,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "23/01/2016",
        "OwnerUserId": 93377.0,
        "Score": 0,
        "ViewCount": 55.0,
        "Title": "model well conduct linear regression",
        "Body": "regression two variable x two approach assume mean form use least square solution find parameter conduct linear regression log x directly find least square solution intercept slope anyone tell approach general well ymmv case thanks",
        "Tags": [
            "regression",
            "linear-model",
            "nonlinear-regression"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 12,
        "Topics": {
            "5": 0.27932242,
            "43": 0.2941019
        }
    },
    {
        "Id": 8668,
        "PostTypeId": 2,
        "ParentId": 8662.0,
        "AcceptedAnswerId": "",
        "CreationDate": "23/03/2011",
        "OwnerUserId": 3805.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "sure would want classify sample specific distribution large sample size parsimony compare another sample look physical interpretation paramters statistical package r sa minitab allow one plot data graph yield straight line data come particular distribution see graph yield straight line data normal log normal-after log transformation weibull chi-squared come mine immediately technique allow see outlier give possiblity assign reason data point outlier r normal probability plot call qqnorm",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 3,
        "Topics": {
            "3": 0.110333346,
            "36": 0.1834688,
            "43": 0.29969835,
            "45": 0.15481815
        }
    },
    {
        "Id": 60262,
        "PostTypeId": 2,
        "ParentId": 10017.0,
        "AcceptedAnswerId": "",
        "CreationDate": "28/05/2013",
        "OwnerUserId": 12053.0,
        "Score": 30,
        "ViewCount": "",
        "Title": "",
        "Body": "white standard error cluster group framework try plm model see also link plm package documentation edit two-way cluster e g group time see follow link another helpful guide plm package specifically explains different option cluster standard error cluster information especially stata found edit example compare r stata also may helpful post provide helpful overview documentation",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "3": 0.27804253,
            "33": 0.10658861,
            "43": 0.20567416
        }
    },
    {
        "Id": 229244,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 229267.0,
        "CreationDate": "10/08/2016",
        "OwnerUserId": 127278.0,
        "Score": 4,
        "ViewCount": 2811.0,
        "Title": "high kmo low communality factor analysis",
        "Body": "perform factor analysis variable kaiser-meyer-olkin kmo measurement communality recall recommend delete variable low kmo statistic low communality case sure deal particular variable",
        "Tags": [
            "factor-analysis"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 2,
        "Topics": {
            "11": 0.10625971,
            "35": 0.1732341,
            "43": 0.34749138,
            "46": 0.102969
        }
    },
    {
        "Id": 305334,
        "PostTypeId": 2,
        "ParentId": 305253.0,
        "AcceptedAnswerId": "",
        "CreationDate": "28/09/2017",
        "OwnerUserId": 26948.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "yes suggest stack output segmentation network input image resize necessary another cnn top performs final classification way classification make use segmentation perform fcn help predict",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "0": 0.116470814,
            "6": 0.16998433,
            "15": 0.19790389,
            "43": 0.38464588
        }
    },
    {
        "Id": 45390,
        "PostTypeId": 2,
        "ParentId": 45381.0,
        "AcceptedAnswerId": "",
        "CreationDate": "07/12/2012",
        "OwnerUserId": 11867.0,
        "Score": 4,
        "ViewCount": "",
        "Title": "",
        "Body": "refer kernel kernel machine-learning literature yes kernel generally consider family gaussian kernel parametrized use power series expansion function exponential rewrite expression recall kernel close summation even infinite sum sum polynomial kernel thus still kernel polynomial kernel show map monomials degree thus gaussian kernel map polynomial kernel example two dimension secord order polynomial kernel map data look like new inner product polynomial kernel actually compute look like mapped monomials degree gaussian kernel thing degree degree degree side note often ml literature gaussian kernel define actually normalize gaussian kernel normalize kernel define use get",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "15": 0.23887031,
            "20": 0.16187099,
            "43": 0.2278606
        }
    },
    {
        "Id": 92947,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 92957.0,
        "CreationDate": "08/04/2014",
        "OwnerUserId": 34619.0,
        "Score": 6,
        "ViewCount": 1844.0,
        "Title": "question error term simple linear regression",
        "Body": "suppose linear regression model many time literature assume assumption make sense large data set due central limit theorem question certain situation feel error term normally distribute wrong assumption suppose bound variable age person exam score student situation bound possible error term force bound example suppose represent person age error term normally distribute random event could occur possible person live say year hence fix issue error term dependent variable left side linear equation bound could choose another bound distribution error term uniform distribution bound however would realistic since would imply event error term equally likely occur interested people thought problem edit reading great answer comment say would practical impose bound domain distribution example triangle density particular domain would impose type distribution bound domain resemble normal distribution disadvantage",
        "Tags": [
            "regression",
            "linear-model",
            "econometrics"
        ],
        "AnswerCount": 3.0,
        "CommentCount": 5,
        "Topics": {
            "43": 0.31509492,
            "45": 0.12652387
        }
    },
    {
        "Id": 40539,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "16/10/2012",
        "OwnerUserId": 14305.0,
        "Score": 0,
        "ViewCount": 100.0,
        "Title": "decide covarates log-transformed model do",
        "Body": "understand good way see covariate transform linear model plot residual pattern log-transformation may need right try find best model relationship several possible predictor variable know test case able slr mlr sure add test test log-scale go actually go backward method way know easy plot residual decide understand would do figure rest guess interpret anyway",
        "Tags": [
            "regression",
            "linear-model"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 6,
        "Topics": {
            "1": 0.10001974,
            "43": 0.49609485,
            "46": 0.13074885
        }
    },
    {
        "Id": 424299,
        "PostTypeId": 2,
        "ParentId": 424035.0,
        "AcceptedAnswerId": "",
        "CreationDate": "30/08/2019",
        "OwnerUserId": 53084.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "imagine test already build model one independent test set outlier outlier way influence result data point thus test estimate correlate imagine situation run cv instead outlier affect result data point say data-point affect result twice first make result bad test set second make result bad training set use fit model course need outlier data observe illustrate data point cv completely independent would one hold-out set scenario",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 3,
        "Topics": {
            "10": 0.26170164,
            "43": 0.34028536
        }
    },
    {
        "Id": 33227,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 33239.0,
        "CreationDate": "28/07/2012",
        "OwnerUserId": 12923.0,
        "Score": 6,
        "ViewCount": 405.0,
        "Title": "sample mean quantiles sort data unbiased estimator true mean",
        "Body": "study income inequality common look sample mean decile quintiles sample assume sample mean good estimator true mean set decile quintiles normally refer break point set observation divide break point suppose income value observe error error probably percentage error independent true value mean sample quantile e g top decile unbiased estimator population mean know leptokurtic distribution e g pareto sample mean understates population mean question refers bias might induced sort process one sort observe value include error rather true value intuition sample mean high decile quintile would bias upwards positive error would sort conversely low instance seem income non-negative observe normal error large sample would contain negative value fine enough quantile would gather negative value together low group negative mean demonstrate bias since true mean must positive true mean bias good way correct bias correct think error independence true value carry independence error observe value include error easy way least characterize ideally correct dependence commonly use index inequality ration mean income high quintile decile mean income low mean bias bias correct result ratio unbiased estimator true ratio",
        "Tags": [
            "ordinal-data",
            "sample",
            "quantiles",
            "relative-distribution"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 1,
        "Topics": {
            "26": 0.17810822,
            "43": 0.23838821
        }
    },
    {
        "Id": 332626,
        "PostTypeId": 2,
        "ParentId": 332590.0,
        "AcceptedAnswerId": "",
        "CreationDate": "09/03/2018",
        "OwnerUserId": 198195.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "estimate inflation type error rate multiple test found inflate type error rate family-wise error rate alpha type error rate per test normally c number test example test alpha approximate inflate error rate family-wise error rate probability type error across test decide reduce per-comparison alpha family-wise error rate would family-wise error rate use alpha per test would give acceptable overall family-wise error rate opinion request use traditional correction approach bonferroni correction per comparison alpha would within round error rate suggest plan execute large number test would use form type error rate correction instead would use approach corrects false discovery rate since reduce statistical power like many multiple comparison approach large number comparison read section benjamini\u2013hochberg procedure say use benjamini\u2013hochberg procedure since test note many test good alternative bryan",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "43": 0.44992396,
            "44": 0.24357311
        }
    },
    {
        "Id": 44317,
        "PostTypeId": 2,
        "ParentId": 44044.0,
        "AcceptedAnswerId": "",
        "CreationDate": "24/11/2012",
        "OwnerUserId": 3919.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "way would approach would start histogram average renewal rate enough give expect number renewal would occur particular day look many would expect day histogram window average renewal rate average seasonality next would accommodate daily seasonality seem indicate renewal likely occur day week others end would baseline look pattern actual due gradual decline increase renewal rate whether promotion increase renewal rate overall accelerate timing forth give idea need model promotion effect introduction competitive product",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "28": 0.12301764,
            "43": 0.33746338
        }
    },
    {
        "Id": 43400,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 89606.0,
        "CreationDate": "12/11/2012",
        "OwnerUserId": 4598.0,
        "Score": 4,
        "ViewCount": 195.0,
        "Title": "quote poem come",
        "Body": "time ago come across somewhere wide wide net statistic limerick like much user beware one must take much care matter statistical right aware l lods lose meaning false positive teem though nothing worth note however lose note come find original source see know come maybe even know proper reference use",
        "Tags": [
            "references",
            "quotation"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "29": 0.17039025,
            "43": 0.512968
        }
    },
    {
        "Id": 300728,
        "PostTypeId": 2,
        "ParentId": 300304.0,
        "AcceptedAnswerId": "",
        "CreationDate": "31/08/2017",
        "OwnerUserId": 7733.0,
        "Score": 4,
        "ViewCount": "",
        "Title": "",
        "Body": "short answer drop kl term reconstruction error plus kl term optimize vae low bound log-likelihood also call evidence low bound elbo log-likelihood one way measure well model explains data make sense try evaluate log-likelihood straight-forward possible use elbo conservative estimate log-likelihood therefore make sense use reconstruction error plus kl term ask training variational autoencoder vae answer question right way evaluate train model become much clearer reconstruction error important application yes reconstruction error would okay use validation would question optimize log-likelihood kingma well auto-encoding variational bayes equation wu et al quantitative analysis decoder base generative model",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "10": 0.13258857,
            "43": 0.24785644
        }
    },
    {
        "Id": 223583,
        "PostTypeId": 2,
        "ParentId": 223579.0,
        "AcceptedAnswerId": "",
        "CreationDate": "13/07/2016",
        "OwnerUserId": 14076.0,
        "Score": 3,
        "ViewCount": "",
        "Title": "",
        "Body": "problem try compare two constant sample -test calculation involves estimate within-group sd denominator wikipedia sample constant lead division",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 5,
        "Topics": {
            "3": 0.11081084,
            "43": 0.19681843,
            "44": 0.2453882
        }
    },
    {
        "Id": 352361,
        "PostTypeId": 2,
        "ParentId": 352269.0,
        "AcceptedAnswerId": "",
        "CreationDate": "20/06/2018",
        "OwnerUserId": 101426.0,
        "Score": 3,
        "ViewCount": "",
        "Title": "",
        "Body": "use mcnemar test test whether table symmetric whether people diagnose sick new method well old versus well new sick old perfectly reasonable scientific question concrete situation suppose two method compare rating mental health problem psychiatrist family physician since see different case mix practice might ask whether affect threshold declare someone ill use cohen kappa evaluate whether agreement method would expect chance perfectly reasonable question different compare two method diagnose mild cognitive impairment gold standard might treat agreement method justify concept mci disagree might wonder whether useful diagnosis calculate sensitivity specificity usual method diagnostic test evaluates performance separately two group well accord gold standard sick accord gold standard reasonable thing different two case two separate thing interested focus practical situation might one instance screen fatal disease might want test high sensitivity since want miss case hand recruiting trial might mind miss cost ground might want high specificity since want full diagnostic work-up people absolutely essential",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 3,
        "Topics": {
            "43": 0.42604548,
            "44": 0.12155146,
            "48": 0.14113766
        }
    },
    {
        "Id": 369477,
        "PostTypeId": 2,
        "ParentId": 369463.0,
        "AcceptedAnswerId": "",
        "CreationDate": "30/09/2018",
        "OwnerUserId": 14076.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "reduce model training error aggressively lead test error increase rather decrease phenomenon call overfitting one primary obstacle select good predictive model example r add uninformative feature decrease training error increase test error",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "5": 0.13423914,
            "10": 0.2634631,
            "26": 0.114804946,
            "43": 0.13267511
        }
    },
    {
        "Id": 308528,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "18/10/2017",
        "OwnerUserId": 167934.0,
        "Score": 0,
        "ViewCount": 739.0,
        "Title": "test serial correlation fix effect model",
        "Body": "currently study panel data use fix effect estimator cluster robust standard error account possibility heteroskedasticity serial correlation error term since error differ quite bit normal fix effect s assume reasonable use however wonder appropriate use cluster robust error fix effect model obviously serial correlation problem model understand problematic serial correlation occurs demean error original demean error matter test use decide whether appropriate use cluster robust standard error fix effect model come across test propose wooldridge pp f test whether original error panel model uncorrelated base residual first difference model appropriate use test decide whether use cluster robust error update reading section wooldridge quite sure propose test sensible situation however anybody know test serial correlation panel data fix effect thank much help thought source wooldridge jeffrey econometric analysis cross section panel data mit press",
        "Tags": [
            "panel-data",
            "autocorrelation",
            "fixed-effects-model",
            "clustered-standard-errors"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "26": 0.12778911,
            "33": 0.3330725,
            "43": 0.16825406,
            "46": 0.113151945
        }
    },
    {
        "Id": 266467,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 266475.0,
        "CreationDate": "09/03/2017",
        "OwnerUserId": 152428.0,
        "Score": 1,
        "ViewCount": 518.0,
        "Title": "monte carlo p-value linear regression",
        "Body": "try understand simple monte carlo application create empirical p-values linear regression sure follow right procedure compare slope explanatory variable real randomize dependent variable understand lmperm package wish faster complicate way wish recreate understand procedure need scale data complicate structure p-values seem high lmp lm wonder right wonder method follow correct shuffle data differently",
        "Tags": [
            "regression",
            "multiple-regression",
            "regression-coefficients",
            "monte-carlo",
            "permutation-test"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "22": 0.11371853,
            "30": 0.192817,
            "43": 0.22403905
        }
    },
    {
        "Id": 351226,
        "PostTypeId": 2,
        "ParentId": 351213.0,
        "AcceptedAnswerId": "",
        "CreationDate": "13/06/2018",
        "OwnerUserId": 35989.0,
        "Score": 3,
        "ViewCount": "",
        "Title": "",
        "Body": "take simpler case presumably mean independently distribute accord distribution notation shortcut describe -th variable separate case often trade-off simplicity formality notation quote strange e g put side usually see variable left hand side distribution parameter right hand side redundant use like",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "19": 0.10566731,
            "20": 0.15543525,
            "43": 0.25419295,
            "47": 0.14425229
        }
    },
    {
        "Id": 283226,
        "PostTypeId": 2,
        "ParentId": 221072.0,
        "AcceptedAnswerId": "",
        "CreationDate": "02/06/2017",
        "OwnerUserId": 108181.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "geophysical time series auto-correlated mean measurement value would similar different like air temperature example pre-whitening use detrend make measurement white namely independent measurement",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "24": 0.27867493,
            "43": 0.27611557
        }
    },
    {
        "Id": 159339,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 161919.0,
        "CreationDate": "30/06/2015",
        "OwnerUserId": 81087.0,
        "Score": 0,
        "ViewCount": 1103.0,
        "Title": "maximum lag selection panel unit root test",
        "Body": "interested conduct panel unit root test panel subregional annual data n specifically depend independent variable include regression varies want implement maddala wu well pesaran panel unit root test use multipurt command stata question maximum lag understand often depends frequency data e lag monthly quarterly data follow annual data max lag give shortness panel concern specify lag necessary",
        "Tags": [
            "hypothesis-testing",
            "econometrics",
            "stata",
            "panel-data",
            "unit-root"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "24": 0.17809601,
            "43": 0.2373778,
            "44": 0.1409547
        }
    },
    {
        "Id": 235504,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 239181.0,
        "CreationDate": "17/09/2016",
        "OwnerUserId": 121267.0,
        "Score": 1,
        "ViewCount": 5774.0,
        "Title": "formula calculate confidence level give confidence interval",
        "Body": "say sample size sample mean sample standard deviation give confidence interval like calculate associate confidence level know general procedure calculate wonder general single formula determine associate confidence level assume normal population distribution",
        "Tags": [
            "confidence-interval"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 3,
        "Topics": {
            "16": 0.34917146,
            "43": 0.17844343
        }
    },
    {
        "Id": 319522,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "19/12/2017",
        "OwnerUserId": 173910.0,
        "Score": 1,
        "ViewCount": 37.0,
        "Title": "reseat passenger reinforcement learn problem",
        "Body": "requirement optimally move passenger one seat map another different configuration move base many rule like family sit together window seat move window seat available kid seat middle seat many many seat map configuration available time need move passenger one another per requirement reinforcement learn problem yes approach",
        "Tags": [
            "optimization",
            "reinforcement-learning",
            "methodology",
            "artificial-intelligence"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 2,
        "Topics": {
            "15": 0.21985157,
            "43": 0.47809395
        }
    },
    {
        "Id": 76455,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 76457.0,
        "CreationDate": "13/11/2013",
        "OwnerUserId": 28940.0,
        "Score": 0,
        "ViewCount": 406.0,
        "Title": "advantage bootstrap aggregate sample",
        "Body": "test robustness predictive model previously establish use withdraw multiple random sample see robust model day ago come across bootstrap sample technique claimed improve stability accuracy machine learn algorithm technique withdraws sample one time completely ignores sample would increase stability accuracy model bit confuse advantage method someone give little idea please",
        "Tags": [
            "variance",
            "sampling",
            "bootstrap",
            "sample"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "10": 0.22860089,
            "43": 0.34958196
        }
    },
    {
        "Id": 325549,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 326166.0,
        "CreationDate": "29/01/2018",
        "OwnerUserId": 13050.0,
        "Score": 10,
        "ViewCount": 588.0,
        "Title": "measure dispersion word frequency data",
        "Body": "quantify amount dispersion vector word count look statistic high document contains many different word occur infrequently low document b contains one word word occur often generally one measure dispersion spread nominal data standard way text analysis community",
        "Tags": [
            "variance",
            "natural-language",
            "gini",
            "dispersion",
            "bag-of-words"
        ],
        "AnswerCount": 5.0,
        "CommentCount": 0,
        "Topics": {
            "43": 0.29495588,
            "47": 0.15454413
        }
    },
    {
        "Id": 371981,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "15/10/2018",
        "OwnerUserId": 223669.0,
        "Score": 0,
        "ViewCount": 57.0,
        "Title": "ass association categorical ordinal variable",
        "Body": "like see correlation biomarker independent variable indicate positive negative number symptom subject state symptom distribution normal test use",
        "Tags": [
            "correlation"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 5,
        "Topics": {
            "43": 0.2315012,
            "44": 0.14203012
        }
    },
    {
        "Id": 355776,
        "PostTypeId": 2,
        "ParentId": 355661.0,
        "AcceptedAnswerId": "",
        "CreationDate": "12/07/2018",
        "OwnerUserId": 73794.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "support actually vector whose non-zero definition nothing prove equation",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "0": 0.11536044,
            "17": 0.23421034,
            "20": 0.18339433,
            "23": 0.14503351,
            "43": 0.10966752
        }
    },
    {
        "Id": 423407,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "23/08/2019",
        "OwnerUserId": 60613.0,
        "Score": 2,
        "ViewCount": 42.0,
        "Title": "follow choice factor loading optimal two-state mle factor analysis",
        "Body": "suppose -dimensional sample diagonal matrix specific variance composes remainder covariance structure term factor loading model denotes orthogonal score pertain factor log-likelihood note equation state start estimate initial guess easily obtain multiple linear regression optimal give lead eigen-vectors b likewise start estimate optimal choice derive definition definition two-stage mle perform iteratively convergence question actually derive make connection way partial derivative log-likelihood unless make mistake begin align frac partial mathcal partial mathcal mathbf psi psi mathbf w tw psi mathbf w tw mathbf v psi mathbf w tw frac partial mathcal partial mathcal mathbf w mathbf w frac partial mathcal partial mathcal mathbf psi end align",
        "Tags": [
            "maximum-likelihood",
            "optimization",
            "factor-analysis",
            "linear-algebra"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 1,
        "Topics": {
            "17": 0.11008444,
            "19": 0.2927808,
            "20": 0.11109651,
            "41": 0.13727821
        }
    },
    {
        "Id": 174878,
        "PostTypeId": 2,
        "ParentId": 174868.0,
        "AcceptedAnswerId": "",
        "CreationDate": "30/09/2015",
        "OwnerUserId": 34874.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "splitting point may may change decision split dependent data node point even change change removal point likely minimal least leaf node probability score change slightly since one less data point",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 4,
        "Topics": {
            "9": 0.19218051,
            "14": 0.114024274,
            "43": 0.45553464
        }
    },
    {
        "Id": 16056,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "26/09/2011",
        "OwnerUserId": 5025.0,
        "Score": 2,
        "ViewCount": 101.0,
        "Title": "density related sparseness measure",
        "Body": "multi-variate continuous distribution whose probability distribution function give high value sparse vector low value dense vector e indicate sparseness vector found one sparseness measure follow give vector one non-zero element element equal give image reference paper seem good candidate potential instead density help case since compute likelihood give data wonder exists know distribution use case generate random vector belonging distribution p hoyer non-negative matrix factorization sparseness constraint journal machine learn research vol pp \u2013",
        "Tags": [
            "distributions",
            "multivariate-analysis",
            "pdf",
            "continuous-data"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 0,
        "Topics": {
            "15": 0.107448086,
            "17": 0.18607864,
            "29": 0.11006643,
            "43": 0.1705285,
            "45": 0.20650324
        }
    },
    {
        "Id": 329553,
        "PostTypeId": 2,
        "ParentId": 74082.0,
        "AcceptedAnswerId": "",
        "CreationDate": "20/02/2018",
        "OwnerUserId": 103153.0,
        "Score": 3,
        "ViewCount": "",
        "Title": "",
        "Body": "bayesian estimate bayesian inference mle type frequentist inference method accord bayesian inference hold notice maximum likelihood estimate treat ratio evidence prior constant set prior distribution uniform distribution play dice instance omits prior belief thus mle consider frequentist technique rather bayesian prior scenario sample large enough mle amount map detailed deduction please refer answer mle alternative bayesian inference call maximum posteriori estimation map short actually mle special case map prior uniform see state wikipedia point view bayesian inference mle special case maximum posteriori estimation map assumes uniform prior distribution parameter detail please refer awesome article mle v map connection maximum likelihood maximum posteriori estimation one difference maximum likelihood overfitting-prone adopt bayesian approach over-fitting problem avoid",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 4,
        "Topics": {
            "41": 0.40500626,
            "43": 0.27964246
        }
    },
    {
        "Id": 1282,
        "PostTypeId": 2,
        "ParentId": 1261.0,
        "AcceptedAnswerId": "",
        "CreationDate": "05/08/2010",
        "OwnerUserId": 279.0,
        "Score": 5,
        "ViewCount": "",
        "Title": "",
        "Body": "follow comment question would simple j k restrict integer reason follow pa pb pc denote observe probability success three group let p pa j log pb log pa k log pc log pa easily satisfy require condition except j k look like ad-hoc simplify assumption instead real constraint fact give data get j k think prompt original question even possible get standard error quantity rather logarithm note pb p j log -log pb log j log -log p one use logistic regression complimentary log-log link number failure complimentary log-log function log -log -x link built statitical software r sa one could check whether ci include integer perhaps run likelihood-ratio test compare fit unrestricted model one j k round near integer assumes something similar could probably do integer probably offset log model thought end want note make sure hypothesis meaningful come play data otherwise statistical test bias picked form null hypothesis possible weird form could imagine likely fit",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "43": 0.26975545
        }
    },
    {
        "Id": 71394,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 71403.0,
        "CreationDate": "29/09/2013",
        "OwnerUserId": 30846.0,
        "Score": 4,
        "ViewCount": 5809.0,
        "Title": "independence multivariate normal distribution",
        "Body": "consider multivariate normal distribution need find follow random variable independent determine see e g covariance normally imply independent learn wikipedia since normally jointly distribute fact apply cope sum minus",
        "Tags": [
            "self-study",
            "normal-distribution",
            "multivariate-analysis",
            "independence"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 7,
        "Topics": {
            "17": 0.10253751,
            "20": 0.30427274,
            "43": 0.25032127,
            "45": 0.12851845,
            "46": 0.11142458
        }
    },
    {
        "Id": 402702,
        "PostTypeId": 2,
        "ParentId": 402563.0,
        "AcceptedAnswerId": "",
        "CreationDate": "12/04/2019",
        "OwnerUserId": 244470.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "least code give suggestion matt barstead use package r result recover loading use generate data side note try use package well work fine specify try use fiml get warn message recover parameter incomplete snippet get",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "3": 0.2937125,
            "43": 0.21498607
        }
    },
    {
        "Id": 131110,
        "PostTypeId": 2,
        "ParentId": 131065.0,
        "AcceptedAnswerId": "",
        "CreationDate": "03/01/2015",
        "OwnerUserId": 28746.0,
        "Score": 3,
        "ViewCount": "",
        "Title": "",
        "Body": "provide purely abstract mathematical answer denote brain volume iq index use index men index woman let assume follow fact note quote text talk correlation brain volume iq general supply image make distinction two trend-lines e show correlation two subgroup separately consider separately correct way go obtain inequality necessitate check assume contrary must case well certainly case inequality hold time equal iq average perfectly compatible initial assumption take fact fact could well happen could high average iq woman men set fact word correlation assumption fact impose constraint whatsoever relation average iq possible relation may hold compatible assumption",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 5,
        "Topics": {
            "43": 0.29168996,
            "47": 0.10830758
        }
    },
    {
        "Id": 193151,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 193153.0,
        "CreationDate": "29/01/2016",
        "OwnerUserId": 95845.0,
        "Score": 0,
        "ViewCount": 27.0,
        "Title": "construct random experiment",
        "Body": "someone help construct random experiment follow density update accord understood give r code please check mikep whether right numeric numeric numeric rnorm number number want generate sample ifelse ifelse",
        "Tags": [
            "pdf",
            "random-generation"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 3,
        "Topics": {
            "3": 0.1437202,
            "22": 0.102248736,
            "43": 0.35867614,
            "45": 0.23249033
        }
    },
    {
        "Id": 68701,
        "PostTypeId": 2,
        "ParentId": 68601.0,
        "AcceptedAnswerId": "",
        "CreationDate": "29/08/2013",
        "OwnerUserId": 29350.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "book use statistical theory class probability statistical inference nitis mukhopadhyay definitely start begin notion probability classic coin toss quickly move statistic statistical distribution moment definitely seem mathematically-oriented pretty much never crack work apply statistic project",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "13": 0.11134138,
            "29": 0.19025855,
            "43": 0.3323464,
            "47": 0.2798952
        }
    },
    {
        "Id": 404095,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "20/04/2019",
        "OwnerUserId": 117574.0,
        "Score": 2,
        "ViewCount": 103.0,
        "Title": "clarify dirichlet process mixture probability term",
        "Body": "suppose dirichlet process mixture model define follow standard gamma distribution stand normal inverse wishart ultimately plate model look like upon probabilistic maniuplation arrive follow term would like clarification wonder interpret seem already implicitly use expression number term allocate cluster sense chinese restaurant process consider conditioning change anything interpret expression feel strange act like switch index bayes net interpret either direction feel strange addition add gamma prior distribution term likelihood term similar suggest escobar west equation possible make claim empirical distribution regularise effect amount cluster form cluster allocation point",
        "Tags": [
            "gaussian-mixture",
            "bayesian-network",
            "dirichlet-distribution",
            "dirichlet-process",
            "infinite-mixture-model"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 7,
        "Topics": {
            "20": 0.15889941,
            "41": 0.14303133,
            "43": 0.3343822
        }
    },
    {
        "Id": 368028,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 375931.0,
        "CreationDate": "21/09/2018",
        "OwnerUserId": 66281.0,
        "Score": 2,
        "ViewCount": 538.0,
        "Title": "make sense use logit softplus loss binary classification problem",
        "Body": "logit class usually binary classification problem use sigmoid cross-entropy compute loss suppose scale directly push logit class class loss see code use like make sense use simpler loss mathematical reason use could point source paper book blog use simpler loss",
        "Tags": [
            "classification",
            "neural-networks",
            "loss-functions",
            "cross-entropy",
            "sigmoid-curve"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 3,
        "Topics": {
            "23": 0.26820764,
            "43": 0.2591552
        }
    },
    {
        "Id": 30368,
        "PostTypeId": 2,
        "ParentId": 30365.0,
        "AcceptedAnswerId": "",
        "CreationDate": "13/06/2012",
        "OwnerUserId": 11032.0,
        "Score": 12,
        "ViewCount": "",
        "Title": "",
        "Body": "expectation average value mean random variable probability distribution discrete random variable weight average value random variable take weight accord relative frequency occurrence individual value absolutely continuous random variable integral value x multiply probability density observe data view value collection independent identically distribute random variable sample mean sample expectation define expectation data respect empirical distribution observe data make simply arithmetic average data",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "13": 0.19475211,
            "20": 0.17106576,
            "43": 0.1111586
        }
    },
    {
        "Id": 153079,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "19/05/2015",
        "OwnerUserId": 77576.0,
        "Score": 5,
        "ViewCount": 117.0,
        "Title": "derivation olsens l selectivity correction",
        "Body": "many estimation procedure correct sample selection famous heckman two-step selectivity correction two equation assumes bivariate normality error term equation olsen least square correction selectivity bias ecta show sufficient assume distributional form error selection equation linearity conditional expectation give error term regression model interest ols selectivity correction derive simple see would appreciate someone writes derivation step step olsen start follow specification assumption regression model interest observe assumes row vector conform column vector unknown coefficient respectively exogenous assumes expect value error intensive regression zero get stuck therefore question show see also p olsen olsen go assume conditional expectation give linear use decomposition uncorrelated follow assume uniformly distribute find therefore question step see also p olsen",
        "Tags": [
            "least-squares",
            "uniform",
            "heckman"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 0,
        "Topics": {
            "20": 0.26808432,
            "26": 0.14750044,
            "43": 0.11622074,
            "46": 0.11175198
        }
    },
    {
        "Id": 377989,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 377996.0,
        "CreationDate": "20/11/2018",
        "OwnerUserId": 45409.0,
        "Score": 2,
        "ViewCount": 104.0,
        "Title": "expectation e g x c",
        "Body": "time series analysis w wei approach find var z since simulated scenario computer answer seem approach correct able move ahead integral help greatly appreciate sa code variance next part b question find independent need check rh zero since lh consider use identity since second term zero mean independent step okay anything wrong please let know",
        "Tags": [
            "time-series",
            "expected-value"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 1,
        "Topics": {
            "19": 0.14662866,
            "20": 0.27157232,
            "43": 0.2446345
        }
    },
    {
        "Id": 415609,
        "PostTypeId": 2,
        "ParentId": 415580.0,
        "AcceptedAnswerId": "",
        "CreationDate": "01/07/2019",
        "OwnerUserId": 244740.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "big deal dtw metric measure paper exactly want however one long time series oppose many short time series look time series snippet b b",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "24": 0.23910965,
            "38": 0.27643767,
            "43": 0.22759463
        }
    },
    {
        "Id": 324111,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "20/01/2018",
        "OwnerUserId": 144764.0,
        "Score": 1,
        "ViewCount": 250.0,
        "Title": "generate synthetic data robust regression",
        "Body": "run test see benefit use robust regression ols purpose create synthetic data follow manner random normal matrix k random normal vector generate use np random normal generate response variable b add outlier choose random entry make call vector set data run ols robust regression use weight least square algo huber loss compare two relative error optimal solution retured algorithm observe relative error ols robust regression seem pretty close wonder well way generate synthetic data make case need robust regression strong try change noise laplace code much difference hint suggestion greatly appreciate thanks lot",
        "Tags": [
            "regression",
            "robust",
            "synthetic-data"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "3": 0.11005227,
            "5": 0.13364258,
            "14": 0.1155635,
            "22": 0.11350801,
            "43": 0.21745846
        }
    },
    {
        "Id": 418799,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "23/07/2019",
        "OwnerUserId": 176257.0,
        "Score": 0,
        "ViewCount": 15.0,
        "Title": "doc vec model compare",
        "Body": "data year year contains set document document consists text corresponds unique id change time id year id year moment create funcion load year process data compute term frequency inverse document frequency word document large matrix colnames id colrows word compute cosine similarity column find similar document part processing function rename column name id year extract cosine similarity score result document baseline approach however would like apply data question possible load year document train model whole sample obtain cosine similarity many document change much therefore model see duplicate data would bias result way interested obtain cosine score time since rank cosine score analysis would simple average word vector model sufficient",
        "Tags": [
            "machine-learning",
            "natural-language",
            "word2vec",
            "doc2vec"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 0,
        "Topics": {
            "27": 0.12719218,
            "28": 0.11048251,
            "38": 0.10797727,
            "42": 0.16161817,
            "43": 0.14561893
        }
    },
    {
        "Id": 261418,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 264942.0,
        "CreationDate": "12/02/2017",
        "OwnerUserId": 106464.0,
        "Score": 2,
        "ViewCount": 243.0,
        "Title": "model selection mixture gaussians",
        "Body": "data want decide whether come -modal-normal distribution -modal-normal distribution word want check peak estimate gaussian know calculate likelihood model would like use bic criterion",
        "Tags": [
            "model-selection",
            "likelihood",
            "gaussian-mixture",
            "bic"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "1": 0.16091904,
            "41": 0.2234388,
            "43": 0.16024049,
            "45": 0.10132246
        }
    },
    {
        "Id": 403874,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "18/04/2019",
        "OwnerUserId": 124272.0,
        "Score": 0,
        "ViewCount": 22.0,
        "Title": "select svm parameter training data oversampled undersampled",
        "Body": "work classification highly imbalanced data let say strategy oversample undersample training data plan use svm classifier perform classification since oversampled undersampled training data distribution different true distribution select hyperparameters svm understand validation data need distribute accord true distribution test data regular situation training data distribute accord true distribution training validation data combine perform cross-validation select hyperparameters situation consider make sense combine oversampled undersampled training data validation data different distribution imagine standard way proceed simply use validation data without combine training data oversampled undersampled select hyerparameters wonder alternative approach select hyperparameters training data general distribute accord true distribution insight appreciate thanks",
        "Tags": [
            "machine-learning",
            "cross-validation",
            "svm",
            "validation",
            "hyperparameter"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 0,
        "Topics": {
            "10": 0.3117807,
            "14": 0.12893812,
            "43": 0.20872298
        }
    },
    {
        "Id": 109466,
        "PostTypeId": 2,
        "ParentId": 109430.0,
        "AcceptedAnswerId": "",
        "CreationDate": "26/07/2014",
        "OwnerUserId": 9568.0,
        "Score": 3,
        "ViewCount": "",
        "Title": "",
        "Body": "first let restate problem vanish gradient suppose normal multilayer perceptron sigmoidal hidden unit train back-propagation many hidden layer error gradient weakens move back network front derivative sigmoid weakens towards pole update move front network contain less information rnns amplify problem train back-propagation time bptt effectively number layer traverse back-propagation grows dramatically long short term memory lstm architecture avoids problem vanish gradient introduce error gate allows learn long term step dependency data point error carousel recent trend training neural network use rectify linear unit robust towards vanish gradient problem rnns sparsity penalization rectify linear unit apparently work well see advance optimize recurrent network historically neural network performance greatly depend many optimization trick selection many hyperparameters case rnn wise also implement rmsprop nesterov \u2019 accelerate gradient thankfully recent development dropout training make neural network robust towards overfitting apparently work towards make dropout work rnns see fast dropout applicability recurrentnetworks",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "6": 0.25550023,
            "23": 0.114584565,
            "43": 0.32063514
        }
    },
    {
        "Id": 252312,
        "PostTypeId": 2,
        "ParentId": 252295.0,
        "AcceptedAnswerId": "",
        "CreationDate": "19/12/2016",
        "OwnerUserId": 40036.0,
        "Score": 14,
        "ViewCount": "",
        "Title": "",
        "Body": "call number piece section interested simple binomial calculation condition two piece multiply together conclude",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "13": 0.38934812,
            "43": 0.3130227
        }
    },
    {
        "Id": 157655,
        "PostTypeId": 2,
        "ParentId": 157639.0,
        "AcceptedAnswerId": "",
        "CreationDate": "18/06/2015",
        "OwnerUserId": 76981.0,
        "Score": 9,
        "ViewCount": "",
        "Title": "",
        "Body": "heuristically think motivation behind bootstrap give large sample sample distribute approximately equal population sample distribute approximately equal population re-sampling sample calculate statistic approximately re-sampling population calculate statistic course exactly sample exactly distribute population example issue drawn sample look different population thus statistic calculate resampling sample look different statistic calculate resampling population despite example bootstrap still valid procedure large sample large deviation distribution sample v distribution population become less less likely say bootstrapping return population statistic make sense",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "16": 0.1057646,
            "40": 0.127942,
            "43": 0.45623943
        }
    },
    {
        "Id": 44988,
        "PostTypeId": 2,
        "ParentId": 44981.0,
        "AcceptedAnswerId": "",
        "CreationDate": "03/12/2012",
        "OwnerUserId": 686.0,
        "Score": 5,
        "ViewCount": "",
        "Title": "",
        "Body": "think question sort inside right path get data ask ask want get data analysis want non-statistical language even data know want frequent occurrence ask statistical method data rather interest question data help answer use answer question choose analytic method",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "29": 0.14712657,
            "43": 0.59565276
        }
    },
    {
        "Id": 132778,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "09/01/2015",
        "OwnerUserId": 43633.0,
        "Score": 0,
        "ViewCount": 38.0,
        "Title": "continuous predictor significant difference another scale",
        "Body": "wonder anybody could point right direction statistical test \u2019 look whether continous variable e g personality variable predicts significant difference score another continous scale take two time point e g anxiety take time time anybody idea type test might appropriate \u2019 use spss primarily simon",
        "Tags": [
            "time-series",
            "repeated-measures",
            "continuous-data"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "31": 0.10040968,
            "43": 0.4028133
        }
    },
    {
        "Id": 284179,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 284918.0,
        "CreationDate": "08/06/2017",
        "OwnerUserId": "",
        "Score": 5,
        "ViewCount": 756.0,
        "Title": "coverage probability credible interval take bayesian model literally",
        "Body": "let say bayesian model proper prior likelihood data distribution assume scalar vector sample value take model seriously hierarchical description data generate process often would credibility interval construct posterior distribution contain generate also would coverage statement conditional collect data would unconditional hence vulnerable usual relevant subset argument direct confidence interval",
        "Tags": [
            "bayesian",
            "mathematical-statistics",
            "confidence-interval",
            "inference"
        ],
        "AnswerCount": 2.0,
        "CommentCount": 0,
        "Topics": {
            "16": 0.14824131,
            "41": 0.18554345,
            "43": 0.3114433
        }
    },
    {
        "Id": 206260,
        "PostTypeId": 2,
        "ParentId": 206095.0,
        "AcceptedAnswerId": "",
        "CreationDate": "08/04/2016",
        "OwnerUserId": 28500.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "follow suggestion nick cox problem take simple form give strictly positive form simplifies think nature error term transform response term variance log-transformed covariance application acoustic power spectral density similar examine difference decibel-type scale might consider use form instead response variable include information original proposal simple interpretation much well property statistical test include use mixed model mention glmm suggests add response comment show use response variable ordinary linear regression much thing analyze ratio transform suggest nickcox response variable via logit generalize linear model glm whatever try measure ratio also measure difference log power spectral density way provide good deal flexibility possibly power use difference log power spectral density response variable might able perform ordinary linear regression avoid use generalize model completely unless need examine random effect repeat measure would make mixed model might need analysis glmm generalize linear mixed model depend experimental design go step work log scale individual power spectral density measurement response variable might make easy examine explain influence joint distribution absolute level something lose ratio propose example say predictor variable might related absolute level ratio originally wish examine stand measurement power spectral density indexed binary factor variable represent whether measurement band could examine follow type model default r cod would baseline value predictor variable difference band baseline value influence baseline value interaction thought influence variable difference two band test significance interaction term would functionally look relation ratio glm perhaps difference model error variance way also take account influence jointly via even need use glmm probably well develop model individual measurement way rather throw away much information first take ratio propose",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "11": 0.10511575,
            "43": 0.32214993
        }
    },
    {
        "Id": 413292,
        "PostTypeId": 2,
        "ParentId": 392673.0,
        "AcceptedAnswerId": "",
        "CreationDate": "16/06/2019",
        "OwnerUserId": 251090.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "read paper benjamin et al harbour porpoise distribution vary small spatiotemporal scale energetic habitat exactly also provide r code use might want check basically generate variance-covariance matrix cyclic covariates include gee piece code use purpose model use last object create tidebasismat avghrbasismat inside gee statistician explain work though forward found exact problem previously hope help even though post quite ago",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "1": 0.1666448,
            "3": 0.17154604,
            "43": 0.25704107
        }
    },
    {
        "Id": 113557,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 113558.0,
        "CreationDate": "28/08/2014",
        "OwnerUserId": 230.0,
        "Score": 0,
        "ViewCount": 75.0,
        "Title": "reproduce simple t-test result anova",
        "Body": "know t-test special case anova try reproduce result follow t-test get different p-values matter parameter combination try please help",
        "Tags": [
            "r",
            "anova",
            "t-test"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 2,
        "Topics": {
            "3": 0.311701,
            "21": 0.108693816,
            "39": 0.12754302,
            "43": 0.288544
        }
    },
    {
        "Id": 205394,
        "PostTypeId": 2,
        "ParentId": 205383.0,
        "AcceptedAnswerId": "",
        "CreationDate": "04/04/2016",
        "OwnerUserId": 78229.0,
        "Score": 3,
        "ViewCount": "",
        "Title": "",
        "Body": "diagnose collinearity notoriously slippery topic since result conditional factor model magnitude pairwise correlation reliable diagnostic metric collinearity regression use partial even semi-partial correlation much reliable indicator course vifs indexed eigenvalue recommend belsey kuh welsch classic book regression diagnostics probably clinical tool even fallible",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "43": 0.36828017,
            "46": 0.126375
        }
    },
    {
        "Id": 17378,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 17411.0,
        "CreationDate": "21/10/2011",
        "OwnerUserId": 25.0,
        "Score": 9,
        "ViewCount": 9478.0,
        "Title": "split plot anova two factor two-way anova repeat measure one factor",
        "Body": "split plot anova two factor identical two-way anova repeat measure one factor distinction",
        "Tags": [
            "anova",
            "repeated-measures",
            "experiment-design",
            "split-plot"
        ],
        "AnswerCount": 2.0,
        "CommentCount": 0,
        "Topics": {
            "21": 0.3280273,
            "43": 0.28405175
        }
    },
    {
        "Id": 281857,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "26/05/2017",
        "OwnerUserId": 162874.0,
        "Score": 7,
        "ViewCount": 229.0,
        "Title": "seemingly simple probability question actually intractable",
        "Body": "good example seemingly simple probability problem actually intractable try motivate use simulation would like come example necessary accessible hope something like intuitively seem easy enough model amount ace left round poker due actually infeasible calculate analytically struggle find good simple example help would appreciate",
        "Tags": [
            "probability",
            "simulation",
            "probabilistic-programming"
        ],
        "AnswerCount": 3.0,
        "CommentCount": 7,
        "Topics": {
            "4": 0.10053847,
            "43": 0.35816425
        }
    },
    {
        "Id": 379287,
        "PostTypeId": 2,
        "ParentId": 375569.0,
        "AcceptedAnswerId": "",
        "CreationDate": "28/11/2018",
        "OwnerUserId": 161201.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "try joss journal open source software academic journal issn formal peer review process design improve quality software submit create doi publication follow covenant code conduct affiliate open source initiative",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "3": 0.15351672,
            "29": 0.35839185,
            "43": 0.31457576
        }
    },
    {
        "Id": 103094,
        "PostTypeId": 2,
        "ParentId": 103082.0,
        "AcceptedAnswerId": "",
        "CreationDate": "12/06/2014",
        "OwnerUserId": 28740.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "use model collinearity derive week number day year former add additional information answer explains detail",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 4,
        "Topics": {
            "0": 0.14789784,
            "28": 0.2851819,
            "43": 0.35344872
        }
    },
    {
        "Id": 394103,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "24/02/2019",
        "OwnerUserId": 73724.0,
        "Score": 1,
        "ViewCount": 406.0,
        "Title": "manuel estimation garch parameter use mle v rugarch package r",
        "Body": "want estimate parameter garch model use rugarch package r manually use maximum likelihood firstly import transfrom data amazon return data estimate parameter garch use rugarch package r fix start value omega alpha beta mu estimate value coefficient try estimate coefficient use maximum likelihood method manually use optim function want fix start value parameter ugarhfit equilize condition estimate value coefficient estimate coeffifients rugarch package manually use mle method result close however difference two estimate hop get result estimate diferent glad help thanks lot",
        "Tags": [
            "r",
            "estimation",
            "garch",
            "volatility-forecasting"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 0,
        "Topics": {
            "3": 0.344899,
            "26": 0.1268474,
            "41": 0.14290272
        }
    },
    {
        "Id": 292159,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "18/07/2017",
        "OwnerUserId": 35571.0,
        "Score": 1,
        "ViewCount": 69.0,
        "Title": "test one directional correlation",
        "Body": "let say two math problem puzzle like problema solve problemb solve would like check similar ask people solve two problem learn k people solve solve p n solve p l solve neither calculate correlation coefficient would problem almost identical problem different independent issue test hypothesis problem identical problem like example different still similar sense everyone solves p solve p though everyone solves p solve p case show clear correlation far different importantly show problem consequence swap problem change wonder criterion show probable follow case problem independent people solve p solve p vice versa people solve p solve p vice versa problem identical everyone solve p solve p vice versa",
        "Tags": [
            "correlation",
            "discrete-data"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 3,
        "Topics": {
            "20": 0.106059484,
            "23": 0.15065195,
            "43": 0.3462466
        }
    },
    {
        "Id": 6819,
        "PostTypeId": 2,
        "ParentId": 6814.0,
        "AcceptedAnswerId": "",
        "CreationDate": "02/02/2011",
        "OwnerUserId": 2592.0,
        "Score": 12,
        "ViewCount": "",
        "Title": "",
        "Body": "kl divergence measure difficult fake one distribution another one assume draw sample size red distribution large may happen empirical distribution sample mimicks blue distribution rare may happen albeit probability vanishingly small behaves like exponent kl divergence blue distribution respect red one say wonder kl divergence ranked order say",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "36": 0.27310628,
            "43": 0.27496374,
            "45": 0.19876608
        }
    },
    {
        "Id": 392313,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 392330.0,
        "CreationDate": "13/02/2019",
        "OwnerUserId": 183621.0,
        "Score": 2,
        "ViewCount": 59.0,
        "Title": "group identify similar name",
        "Body": "large data set case distinct company name however name duplicate misspelling way identify duplicate misspelling algorithm moment use identify duplicate like standardize name like",
        "Tags": [
            "r"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 1,
        "Topics": {
            "3": 0.111536264,
            "14": 0.13076335,
            "15": 0.12413677,
            "27": 0.2257351,
            "43": 0.2175574
        }
    },
    {
        "Id": 31815,
        "PostTypeId": 2,
        "ParentId": 31813.0,
        "AcceptedAnswerId": "",
        "CreationDate": "06/07/2012",
        "OwnerUserId": 11032.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "sample random stratum would call stratify random sample proportional size sample random group would stratify sample proportional size representativeness sample could brought question depend subject select stratum",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "3": 0.10709034,
            "22": 0.13431017,
            "40": 0.21791792,
            "43": 0.2849058
        }
    },
    {
        "Id": 79749,
        "PostTypeId": 2,
        "ParentId": 79740.0,
        "AcceptedAnswerId": "",
        "CreationDate": "15/12/2013",
        "OwnerUserId": 27556.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "view rbf linear regression non linear transformation input could use x x etc g matrix standard linear regression formula first step fix could either take random x x r x r sample input perform unsupervised learn input find prototype t fix calculate g apply phi x j finally perform linear regression namely g",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "6": 0.114072435,
            "20": 0.14553544,
            "43": 0.22013867
        }
    },
    {
        "Id": 433513,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "28/10/2019",
        "OwnerUserId": 141883.0,
        "Score": 0,
        "ViewCount": 15.0,
        "Title": "make time-series stationary apply dynamic time warp",
        "Body": "two time-series x univariate integration order one cointegration analysis conclude series cointegrated therefore use estimate \u03b2 coefficient cointegration relation begin equation \u03b1 \u03b2x \u03b5 end equation quantify similarity two time-series think use dynamic time wrap purpose find similarity two time-series use dtw calculate similarity two time-series make stationary first take first difference example calculate distance straight use original series thank",
        "Tags": [
            "correlation",
            "stationarity",
            "cointegration"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 0,
        "Topics": {
            "24": 0.38089132,
            "43": 0.20014818
        }
    },
    {
        "Id": 3072,
        "PostTypeId": 2,
        "ParentId": 3051.0,
        "AcceptedAnswerId": "",
        "CreationDate": "25/09/2010",
        "OwnerUserId": 795.0,
        "Score": 3,
        "ViewCount": "",
        "Title": "",
        "Body": "easily matlab duck downvote side effect index element sum sure easily translate r idiom matlab give array first first skip first skip first n skip last element array great edit omit average part divide",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 4,
        "Topics": {
            "3": 0.18524083,
            "43": 0.4729963
        }
    },
    {
        "Id": 123182,
        "PostTypeId": 2,
        "ParentId": 123180.0,
        "AcceptedAnswerId": "",
        "CreationDate": "08/11/2014",
        "OwnerUserId": 60242.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "sure word paragraph correct could take likelihood word instead letter example use conditional word probability could get google get question wrong sorry may want specify little",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "42": 0.19921356,
            "43": 0.51471674
        }
    },
    {
        "Id": 153955,
        "PostTypeId": 2,
        "ParentId": 153870.0,
        "AcceptedAnswerId": "",
        "CreationDate": "25/05/2015",
        "OwnerUserId": 7828.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "choose height h k non-trivial non noise cluster cut tree height note may satisfiable data single gaussian may possible find k non-trivial cluster",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "15": 0.12193636,
            "38": 0.4769623,
            "43": 0.24494664
        }
    },
    {
        "Id": 66228,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 66232.0,
        "CreationDate": "01/08/2013",
        "OwnerUserId": 28703.0,
        "Score": 2,
        "ViewCount": 251.0,
        "Title": "run test randomness k element",
        "Body": "say sequence length different element want show sequence random base number run found paper mood explains find expect value variance number run type unsure show sequence random data new hypothesis test statistic general help would appreciate",
        "Tags": [
            "hypothesis-testing",
            "mathematical-statistics",
            "runs"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "3": 0.13158002,
            "22": 0.148795,
            "29": 0.10521009,
            "43": 0.17613839
        }
    },
    {
        "Id": 184078,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 184213.0,
        "CreationDate": "29/11/2015",
        "OwnerUserId": 32212.0,
        "Score": 1,
        "ViewCount": 115.0,
        "Title": "topic pattern recognition newbie pick first",
        "Body": "machine learn wide topic much newbie learn everything start focus relevant suppose interested pattern recognition time-series data topic machine learn pay attention",
        "Tags": [
            "machine-learning",
            "time-series",
            "pattern-recognition"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 1,
        "Topics": {
            "15": 0.38279802,
            "43": 0.25196603
        }
    },
    {
        "Id": 20287,
        "PostTypeId": 2,
        "ParentId": 20088.0,
        "AcceptedAnswerId": "",
        "CreationDate": "27/12/2011",
        "OwnerUserId": 8033.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "guess way anova needful worry normality much anova robust w r non-normality",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "11": 0.10658231,
            "21": 0.12228028,
            "43": 0.35836175
        }
    },
    {
        "Id": 46186,
        "PostTypeId": 2,
        "ParentId": 45762.0,
        "AcceptedAnswerId": "",
        "CreationDate": "18/12/2012",
        "OwnerUserId": 8580.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "parameter describe standard kernel parameter svm classification almost always forget mean found libsvm website solid reference site linear cost parameter appear every kernel help",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "3": 0.1215655,
            "10": 0.12557252,
            "43": 0.3205202
        }
    },
    {
        "Id": 352352,
        "PostTypeId": 2,
        "ParentId": 352347.0,
        "AcceptedAnswerId": "",
        "CreationDate": "20/06/2018",
        "OwnerUserId": 54664.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "seem similar image processing generally object detector object classifier would need design network detects gesture another network classifies could probably combine somehow use multitask learn",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "6": 0.25755218,
            "15": 0.243213,
            "43": 0.37705022
        }
    },
    {
        "Id": 269875,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "26/03/2017",
        "OwnerUserId": 144659.0,
        "Score": 0,
        "ViewCount": 754.0,
        "Title": "glm alternatve model proportion data r",
        "Body": "data experimental setup number two specie amphipod five different substrate record end hour period experiment utilized natural mixed population proportion abundance specie odd rather use count data want test difference substrate preference proportion percentage account difference also additional variable body size anybody help determine would suitable model structure use examines factor substrate specie body size interaction try run binomial proportion non-integer work help would much appreciate spent last day look",
        "Tags": [
            "r",
            "regression",
            "generalized-linear-model",
            "binomial",
            "proportion"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 1,
        "Topics": {
            "3": 0.111947976,
            "18": 0.12844178,
            "43": 0.15135437
        }
    },
    {
        "Id": 385473,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 385500.0,
        "CreationDate": "03/01/2019",
        "OwnerUserId": 203194.0,
        "Score": 1,
        "ViewCount": 89.0,
        "Title": "bayes inference hypothesis test average parameter",
        "Body": "bayesian inference field give dataset assume normal priori distribution average parameter zero mean give variance hypothesis test carry posterior distribution assume certain confidence level average really zero make sense apply t-test statistic posterior distribution test r t-test exclusive hypothesis test sample medium",
        "Tags": [
            "r",
            "hypothesis-testing",
            "bayesian",
            "simulation"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "16": 0.10969261,
            "41": 0.16528675,
            "43": 0.27471697,
            "49": 0.1257329
        }
    },
    {
        "Id": 349931,
        "PostTypeId": 2,
        "ParentId": 275197.0,
        "AcceptedAnswerId": "",
        "CreationDate": "05/06/2018",
        "OwnerUserId": 185038.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "likely depend much data goal analysis ideally specific information specific output always aggregate later dis-aggregate make classification model town level later aggregate everything region level however model region level dis-aggregate town level word often make sense keep town level drop region department cluster prediction bring region department back later say need enough data town level able data many well point data town use analysis amount data town reasonably representative population miss data certain town enough data town may need perform classification analysis department level even region level",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "0": 0.115463756,
            "14": 0.1116266,
            "43": 0.33371252
        }
    },
    {
        "Id": 62847,
        "PostTypeId": 2,
        "ParentId": 62840.0,
        "AcceptedAnswerId": "",
        "CreationDate": "28/06/2013",
        "OwnerUserId": 26997.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "look use stratum sampsize parameter r package random forest could indicate household stratum parameter copy r documentation randomforest package stratum factor variable use stratify sample sampsize size sample draw classification sampsize vector length number stratum sample stratify stratum element sampsize indicate number drawn stratum",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 3,
        "Topics": {
            "3": 0.1392312,
            "9": 0.106326394,
            "43": 0.21638933
        }
    },
    {
        "Id": 355794,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 355824.0,
        "CreationDate": "12/07/2018",
        "OwnerUserId": 81411.0,
        "Score": 1,
        "ViewCount": 47.0,
        "Title": "situation consider row total contingency table fix",
        "Body": "prospective study draw sample size two sttributes present goal ass whether association two attribute data look like follow attribute divide two part sample test whether association attribute since table use sample size attribute common hence correspond row total first table number sample present absent know row total fix data analyst saw table conduct test table conduct test table analyst come table consider row total fix would see table table would consider row total table fix",
        "Tags": [
            "self-study",
            "inference",
            "contingency-tables",
            "fishers-exact",
            "epidemiology"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "18": 0.16883332,
            "43": 0.21344581
        }
    },
    {
        "Id": 237555,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "29/09/2016",
        "OwnerUserId": 120194.0,
        "Score": 1,
        "ViewCount": 26.0,
        "Title": "covariance partition hierarchical linear observation model",
        "Body": "reading paper classical bayesian inference neuroimaging k friston start hierarchical linear observation model vector stand response variable dependent variable parameter vector nth level matrix contain explanatory variable normally distribute error term rcursive substitution give next step understand say covariance partition imply equation come look like expectation value confuse curly bracket mathematical meaning aware secondly covariance partition never heard term",
        "Tags": [
            "generalized-linear-model",
            "covariance",
            "notation"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 1,
        "Topics": {
            "11": 0.103875645,
            "17": 0.13614216,
            "43": 0.12269771,
            "47": 0.10737234
        }
    },
    {
        "Id": 201770,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "15/03/2016",
        "OwnerUserId": 108366.0,
        "Score": 0,
        "ViewCount": 46.0,
        "Title": "usage aic compare model",
        "Body": "aic use compare model exponential smooth linear regression",
        "Tags": [
            "aic",
            "exponential-smoothing"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 3,
        "Topics": {
            "1": 0.5069375,
            "43": 0.11013169
        }
    },
    {
        "Id": 304277,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "21/09/2017",
        "OwnerUserId": 177785.0,
        "Score": 2,
        "ViewCount": 63.0,
        "Title": "theoretically related variable load highly factor use efa",
        "Body": "use efa generate regression factor score subsequent regression anova analysis want use regression factor score instead average score read multiple source example distefano et al well way analysis get variable theoretically represent four construct run efa spss however three factor create follow eigenvalue rule consequently variable combine theoretically different one give spss notion want four factor still mix certain variable guess due measurement error read black et al p okay slightly low even rotation mixed wonder still allow seperately run factor analysis contruct main purpuse summarize score variable one main variable four variable one contruct even without mix still mystery purpose efa run multiple time construct create regression factor score know one correct factor use represent correct factor note aware follow state black et al p one key characteristic differentiates factor score summate scale factor score compute base factor loading variable factor whereas summate scale calculate combine select variable side note two variable dv iv two iv think use barlett method save regression factor score basically question order create regression factor score construct theoretically know need seperate efa spss run variable theoretical construct load different factor reference hair black babin anderson multivariate data analysis th edition pearson new international edition isbn distefano christine zhu min m\u00eendril\u0103 diana understand use factor score consideration apply researcher practical assessment research evaluation",
        "Tags": [
            "regression",
            "factor-analysis"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 5,
        "Topics": {
            "11": 0.17231408,
            "29": 0.10678024,
            "31": 0.27222848,
            "43": 0.26474276
        }
    },
    {
        "Id": 69729,
        "PostTypeId": 2,
        "ParentId": 22347.0,
        "AcceptedAnswerId": "",
        "CreationDate": "11/09/2013",
        "OwnerUserId": 20776.0,
        "Score": 5,
        "ViewCount": "",
        "Title": "",
        "Body": "chi-square test hypothesis variance either one- two-tailed exactly sense t-test hypothesis mean either one- two-tailed",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "43": 0.37456563,
            "44": 0.21065247
        }
    },
    {
        "Id": 320254,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "24/12/2017",
        "OwnerUserId": 163146.0,
        "Score": 1,
        "ViewCount": 229.0,
        "Title": "match curve measure similarity",
        "Body": "firstly sure whether correct site also assume question different assume two curve one consider reference curve another measure curve want transform measure curve way fit reference curve word want calibrate stumble procrustes analysis fr\u00e9chet distance seem usable nevertheless want ask completely unbiased proceed commonly know method even implementation common program language cplusplus octave r thanks advance",
        "Tags": [
            "curve-fitting",
            "calibration",
            "procrustes-analysis"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 2,
        "Topics": {
            "3": 0.13195266,
            "43": 0.44344234
        }
    },
    {
        "Id": 447209,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "31/01/2020",
        "OwnerUserId": 219255.0,
        "Score": 1,
        "ViewCount": 57.0,
        "Title": "independent probability add",
        "Body": "need help follow question hsp chaperone unique binding site different affinity three individual binding site protein denote site b c independent probability hsp bind b c site pa pb pc respectively one chaperone molecule may independently bind single protein probability hsp bind somewhere e least one binding site idea add three probability pa pb pc sum",
        "Tags": [
            "probability"
        ],
        "AnswerCount": 2.0,
        "CommentCount": 3,
        "Topics": {
            "13": 0.3436182,
            "43": 0.21545371
        }
    },
    {
        "Id": 412434,
        "PostTypeId": 2,
        "ParentId": 412414.0,
        "AcceptedAnswerId": "",
        "CreationDate": "11/06/2019",
        "OwnerUserId": 123561.0,
        "Score": 4,
        "ViewCount": "",
        "Title": "",
        "Body": "drop predictor show linear relation response consider alone usually bad idea predictor may useful use predictor try show example let try predict variable x use variable z predictor see variable x independent neither linear relation kind relation correlation predict x z perfect fit however drop predictor linear relationship response x get much bad fit summary linear regression drop predictor linear relationship response interested decide variable include drop model suggest reading variable selection interest topic",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "5": 0.10516332,
            "11": 0.18107794,
            "43": 0.38352144
        }
    },
    {
        "Id": 163794,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "29/07/2015",
        "OwnerUserId": 26564.0,
        "Score": 0,
        "ViewCount": 256.0,
        "Title": "forward algorithm derivation",
        "Body": "self-studying hidden markov model struggle derivation forward algorithm especially definition hadamard product would much appreciate someone share pointer come formula alpha first term multiply struggle see multiply",
        "Tags": [
            "hidden-markov-model"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "17": 0.16594326,
            "20": 0.32867354,
            "43": 0.18348625
        }
    },
    {
        "Id": 375309,
        "PostTypeId": 2,
        "ParentId": 375303.0,
        "AcceptedAnswerId": "",
        "CreationDate": "04/11/2018",
        "OwnerUserId": 217596.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "edit mess notation normal distribution occurrence variance must replace respectively also affect calculation try fix tomorrow since access octave right give try sure think look actually maximum start note formal expression c sum b also state normally distribute like see marginalization also conditional probability c condition b normally distruted per problem formulation use bayes theorem obtain problem p b give normal distribution case c term evaluate constant meaning plug also constant give normal distribution mention matter since interested likely value leaf expression correctly say evaluate maximum let u look leave constant proportionality left expression like plot matlab octave use code give graph show maximum sure everything correct hope help",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "20": 0.19641048,
            "43": 0.27478203
        }
    },
    {
        "Id": 317381,
        "PostTypeId": 2,
        "ParentId": 317379.0,
        "AcceptedAnswerId": "",
        "CreationDate": "06/12/2017",
        "OwnerUserId": 100462.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "thanks neil g comment mistake swap parameter variable convert likelihood probability use",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "41": 0.23434299,
            "43": 0.4329692
        }
    },
    {
        "Id": 388822,
        "PostTypeId": 2,
        "ParentId": 388820.0,
        "AcceptedAnswerId": "",
        "CreationDate": "23/01/2019",
        "OwnerUserId": 126931.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "seem bit misconception mvu mle mutually exclusive maximum likelihood estimator statistic maximizes likelihood function minimum variance unbiased estimator statistic property reason estimator mvu mle instance iid exponential random variable mean mle clearly estimator unbiased e variance turn however cramer rao low bound exponential distribution precisely see first find fisher information cramer-rao low bound state estimator unbiased case since acheives crlb guarantee minimum variance unbiased estimator thus actually fairly common mle mvu difficult define exactly meant optimal estimation depends goal think almost everybody would agree mvu estimator optimal among class unbiased estimator often mean square error use criterion goodness estimator usually lead optimal estimator slightly bias mle lot provably good property largely neyman pearson theory relies think belongs separate question",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "26": 0.20346908,
            "41": 0.15191813,
            "43": 0.2296341,
            "47": 0.15794794
        }
    },
    {
        "Id": 144841,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 144866.0,
        "CreationDate": "04/04/2015",
        "OwnerUserId": 67800.0,
        "Score": 4,
        "ViewCount": 2136.0,
        "Title": "compute component score principal loading directly r",
        "Body": "use polychoric correlation matrix run pca obtain score directly function currently manually plug number matrix calculate factor score like think well way perform calculation way allow compute easy",
        "Tags": [
            "r",
            "factor-analysis"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 3,
        "Topics": {
            "3": 0.25825876,
            "43": 0.45341736
        }
    },
    {
        "Id": 407620,
        "PostTypeId": 2,
        "ParentId": 407581.0,
        "AcceptedAnswerId": "",
        "CreationDate": "10/05/2019",
        "OwnerUserId": 7828.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "special cluster algorithm uncertain data information uncertainty know recall correctly show usually use mean uncertainty region good complex model found rather disappoint dbscan base algorithm maybe fourc check dbscan author later research employ covariance-based context around data point cluster orient fashion think use covariance neighborhood probably use covariance instead",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 3,
        "Topics": {
            "38": 0.25299087,
            "43": 0.29438162
        }
    },
    {
        "Id": 238670,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "05/10/2016",
        "OwnerUserId": 133634.0,
        "Score": 2,
        "ViewCount": 54.0,
        "Title": "test whether population aware something",
        "Body": "prove hypothesis population aware invasive specie survey found population aware invasive specie statistically analyse result stuck idea test use",
        "Tags": [
            "hypothesis-testing",
            "proportion"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 3,
        "Topics": {
            "3": 0.10172954,
            "18": 0.13166477,
            "43": 0.32078904,
            "44": 0.18447025
        }
    },
    {
        "Id": 352449,
        "PostTypeId": 2,
        "ParentId": 336040.0,
        "AcceptedAnswerId": "",
        "CreationDate": "21/06/2018",
        "OwnerUserId": 212157.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "third input enter include h sun without include h strong separate matter general determine whether consistent third input h exist g consistent hypothesis positive input however since third input negative input h contain g must inconsistent input g update set hypothesis satisfy third input first since g g update g one step specific one replace value also third input x rain cold high strong warm change hypothesis candidate include g choose hypothesis actually include g among hypothesis candidate hypothesis include g general version therefore follow list hypothesis satisfy sunny warm strong warm include g uploaded detail candidate elimination learn algorithm github homepage",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "43": 0.47863847
        }
    },
    {
        "Id": 71914,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "04/10/2013",
        "OwnerUserId": 24982.0,
        "Score": 32,
        "ViewCount": 17075.0,
        "Title": "anova command lmer model object",
        "Body": "hopefully question someone answer nature decompose sum square mixed-effects model fit lme r package first say aware controversy use approach practise would likely use bootstrapped lrt compare model suggest faraway however puzzle replicate result sanity thought would ask basically get grip use mixed-effects model fit package know use command give summary sequentially test fixed-effects model far know faraway refers expect mean square approach want know sum square calculate know could take estimate value particular model use assume fix make test use sum square model residual without factor interest fine model contain single within-subject factor however implement split-plot design sum square value get equivalent value produce r use appropriate designation however sum square produce command model object despite fact f-ratios course make complete sense need stratum mixed-model however must mean sum square penalise somehow mixed-model order provide appropriate f-ratios achieve model somehow correct between-plot sum square correct within-plot sum square evidently something necessary classical split-plot anova achieve designate different error value different effect mixed-effect model allow basically want able replicate result command apply lmer model object verify result understand however present achieve normal within-subject design split-plot design seem find case example see f-ratios agree sum square variety also agree however sum square irrigation agree however appear lmer output scale anova command actually",
        "Tags": [
            "r",
            "anova",
            "mixed-model",
            "lme4-nlme"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 6,
        "Topics": {
            "33": 0.15324116,
            "43": 0.2471856
        }
    },
    {
        "Id": 123364,
        "PostTypeId": 2,
        "ParentId": 123277.0,
        "AcceptedAnswerId": "",
        "CreationDate": "10/11/2014",
        "OwnerUserId": 23853.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "yes normal log-linear model main effect control margin table odds ratio design independent margin get equivalent include variable logistic regression also need include interaction term",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "3": 0.10285443,
            "11": 0.21643196,
            "18": 0.15727012,
            "30": 0.14543638,
            "43": 0.11615981
        }
    },
    {
        "Id": 394512,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "26/02/2019",
        "OwnerUserId": 30611.0,
        "Score": 0,
        "ViewCount": 143.0,
        "Title": "trouble size c tree r",
        "Body": "work fraud detection rules-based system found c r predictive power run issue submit around variable model integer binary encode however tree size along way become bloat initially look tree different split balloon way explore prune c model r documentation totally clear control exactly different argument parameter mess try find less split trade accuracy current set tree data sample idea would appreciate thanks",
        "Tags": [
            "r",
            "cart"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 2,
        "Topics": {
            "3": 0.19121374,
            "9": 0.11791212,
            "43": 0.3052239
        }
    },
    {
        "Id": 426851,
        "PostTypeId": 2,
        "ParentId": 426848.0,
        "AcceptedAnswerId": "",
        "CreationDate": "11/09/2019",
        "OwnerUserId": 247274.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "see two possibility make mistake emphatic evidence null hypothesis congratulation report depend field particular journal want submit finding journal format say report tiny p-values way answer question anova test many population mean want explore anova reasonable method consider anova assumption want check usually think small p-values happen little data instance flip coin ten time get head tail particularly convince coin bias towards head flip coin time get head tail strong evidence bias coin could happen small sample size cushion fluke value outlier skew mean number include many number help lessen impact outlier",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 5,
        "Topics": {
            "43": 0.37510085,
            "44": 0.114709206
        }
    },
    {
        "Id": 72153,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 72163.0,
        "CreationDate": "07/10/2013",
        "OwnerUserId": 31182.0,
        "Score": 3,
        "ViewCount": 234.0,
        "Title": "flip identifiable coin batch",
        "Body": "collect data estimate parameter puzzle generate confidence interval setup bag coin coin know probability picked bag coin unknown probability show head flip pick coin bag flip time note number time come head put coin back bag repeat step time possible coin drawn flip would recognise want estimate population mean give confidence interval know could estimate since binomially distribute parameter course possible picked bag point picked would add divide etc realize unbiased estimator estimate parameter coin drawn th round know construct confidence interval around dataset really large computable would rather compute avoid half time",
        "Tags": [
            "confidence-interval",
            "repeated-measures",
            "binomial",
            "mixture",
            "bernoulli-distribution"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "13": 0.15484008,
            "16": 0.16934636,
            "43": 0.36442783
        }
    },
    {
        "Id": 259496,
        "PostTypeId": 2,
        "ParentId": 259494.0,
        "AcceptedAnswerId": "",
        "CreationDate": "02/02/2017",
        "OwnerUserId": 805.0,
        "Score": 3,
        "ViewCount": "",
        "Title": "",
        "Body": "r help say table null conditional independence equivalent hypothesis odds ratio equal one \u2018 exact \u2019 inference base observe general give marginal total fix first element contingency table non-central hypergeometric distribution non-centrality parameter give odds ratio fisher note two fact quote null conditional independence equivalent hypothesis odds ratio equal one first element contingency table non-central hypergeometric distribution non-centrality parameter give odds ratio make odds ratio central hypergeometric see example wikipedia article fisher noncentral hypergeometric distribution state explicitly two distribution equal central hypergeometric distribution odds ratio fisher wallenius noncentral hypergeometrics discuss give ordinary hypergeometric odds ratio contradiction null central hypergeometric r help add word make clear know margin condition",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 4,
        "Topics": {
            "18": 0.2758974,
            "43": 0.20206752,
            "45": 0.1072016
        }
    },
    {
        "Id": 286464,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 286477.0,
        "CreationDate": "21/06/2017",
        "OwnerUserId": 139794.0,
        "Score": 4,
        "ViewCount": 672.0,
        "Title": "test data normal distribution",
        "Body": "maximum temperature data histogram qqplot visually look pretty good would reasonably say normal distribution shapiro-wilk normality test r say normal distribution found post basically say bother test normality get wrong however use test past generate dataset class work well also robust statistical test well visually examine data use test normality make sure assumption valid robust shapiro-wilk normality test curious anyone else issue actual data appose generate data",
        "Tags": [
            "normality-assumption",
            "histogram",
            "qq-plot"
        ],
        "AnswerCount": 2.0,
        "CommentCount": 0,
        "Topics": {
            "43": 0.4105863,
            "44": 0.2077997
        }
    },
    {
        "Id": 375123,
        "PostTypeId": 2,
        "ParentId": 375117.0,
        "AcceptedAnswerId": "",
        "CreationDate": "03/11/2018",
        "OwnerUserId": 197234.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "3": 0.15353417,
            "15": 0.1567645,
            "43": 0.26108497,
            "45": 0.10229703
        }
    },
    {
        "Id": 5184,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "06/12/2010",
        "OwnerUserId": 8.0,
        "Score": 14,
        "ViewCount": 4394.0,
        "Title": "remove factor -way anova table",
        "Body": "recent paper fit three-way fix effect model since one factor significant p remove refit model two fix effect interaction referee comment back quote time significant factor -way anova sufficient criterion pool time factor standard text issue underwood argues p-value non-significant effect must great treatment level factor pool author give relevant p-value justify pool reference underwood question never heard rule anyone else understand remove factor p-value close cut-off rule seem bit extreme referee state underwood standard text really never heard would standard text thing exist unfortunately access underwood advice respond referee background paper submit non-statistical journal fitting three-way model checked interaction effect",
        "Tags": [
            "anova",
            "fixed-effects-model"
        ],
        "AnswerCount": 3.0,
        "CommentCount": 3,
        "Topics": {
            "33": 0.12218922,
            "43": 0.38132674
        }
    },
    {
        "Id": 123115,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 123937.0,
        "CreationDate": "07/11/2014",
        "OwnerUserId": 6108.0,
        "Score": 1,
        "ViewCount": 18.0,
        "Title": "assign value item fix price sell alone combination",
        "Body": "let say set item fix price set transaction item sell alone combination go assign value item take follow example make fly basket x basket x basket z value x z",
        "Tags": [
            "finance"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "28": 0.10513394,
            "43": 0.22964792
        }
    },
    {
        "Id": 41806,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 41809.0,
        "CreationDate": "03/11/2012",
        "OwnerUserId": 13935.0,
        "Score": 3,
        "ViewCount": 1261.0,
        "Title": "bayesian variable selection use categorical variable level",
        "Body": "reading article first approach bayesian variable selection discussion section say one major limitation particular method use class variable two level anyone know",
        "Tags": [
            "regression",
            "bayesian",
            "feature-selection"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "11": 0.28538582,
            "43": 0.24437092
        }
    },
    {
        "Id": 236158,
        "PostTypeId": 2,
        "ParentId": 235981.0,
        "AcceptedAnswerId": "",
        "CreationDate": "21/09/2016",
        "OwnerUserId": 86330.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "speculation reason undesireable favor x variable sex variable due variable normalize found normalize value amount standard deviation mean solve trick still much doubt question e well way perform covariate adaptive randomization instance remove model intercept reasonable think solve problem play around sure valid reason explicitly explain",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "43": 0.37113637
        }
    },
    {
        "Id": 136569,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "06/02/2015",
        "OwnerUserId": 68300.0,
        "Score": 1,
        "ViewCount": 110.0,
        "Title": "analysis bank account record",
        "Body": "new field time series analysis would like look bank account determine spending habit read lot cluster multiple time series think need decomposition break one cashflow time series pattern classify could point right direction recommend r package purpose",
        "Tags": [
            "time-series",
            "machine-learning",
            "pattern-recognition"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 5,
        "Topics": {
            "24": 0.22360276,
            "43": 0.34606877
        }
    },
    {
        "Id": 370757,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "08/10/2018",
        "OwnerUserId": 222932.0,
        "Score": 1,
        "ViewCount": 43.0,
        "Title": "instrumental variable property",
        "Body": "show instrumental variable iv estimator consistent equation use two stage least square method equation come",
        "Tags": [
            "regression",
            "econometrics",
            "instrumental-variables",
            "consistency",
            "law-of-large-numbers"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 2,
        "Topics": {
            "9": 0.12854974,
            "11": 0.10344724,
            "20": 0.31685466,
            "26": 0.17652154,
            "43": 0.13601631
        }
    },
    {
        "Id": 254756,
        "PostTypeId": 2,
        "ParentId": 254734.0,
        "AcceptedAnswerId": "",
        "CreationDate": "05/01/2017",
        "OwnerUserId": 144299.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "experiment block elevation yes one run analysis block factor include elevation block significant drop block factor simplify model alternatively could look run model ancova elevation co-variate measure tree year might need look repeat measure anova",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "21": 0.1421277,
            "33": 0.20189962,
            "43": 0.29346052
        }
    },
    {
        "Id": 388590,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "22/01/2019",
        "OwnerUserId": 234940.0,
        "Score": 0,
        "ViewCount": 83.0,
        "Title": "pl irt model could someone point nice explanation tutorial",
        "Body": "new irt even ask try learn use pl model data completely depth bad run parameter estimation find appropriate app run necessary calculation give end result realize model exactly easy run smoothly likely require in-depth study still would like somewhat quick easy explanation tutorial help could least superficially acquaint deal ideally close formula parameter estimate one two example could someone please point anything like could book paper anything",
        "Tags": [
            "irt"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 2,
        "Topics": {
            "3": 0.111778796,
            "43": 0.44911376
        }
    },
    {
        "Id": 351570,
        "PostTypeId": 2,
        "ParentId": 351567.0,
        "AcceptedAnswerId": "",
        "CreationDate": "15/06/2018",
        "OwnerUserId": 30351.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "question belong forum nevertheless since pattern relatively simple weather nice maybe use follow pattern",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 3,
        "Topics": {
            "3": 0.11330638,
            "42": 0.1408812,
            "43": 0.38901561
        }
    },
    {
        "Id": 336656,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "25/03/2018",
        "OwnerUserId": 197610.0,
        "Score": 1,
        "ViewCount": 474.0,
        "Title": "calculate variable importance multinomial logit model",
        "Body": "analyse data discrete choice experiment sample response respondent present two card choose prefer option survey attribute one categorical two price speed continuous model output base result would like calculate relative variable importance scale base reading chapter understand meant use difference range attribute \u2019 utility value scale multinomial logit model would course first exponentiate utility value however know extract utility value different attribute level use mlogit package somehow possible also go deal fourth categorical value model",
        "Tags": [
            "r",
            "methodology",
            "importance",
            "mlogit"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "3": 0.13492142,
            "11": 0.105662085,
            "43": 0.35640776
        }
    },
    {
        "Id": 6111,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "09/01/2011",
        "OwnerUserId": "",
        "Score": 2,
        "ViewCount": 500.0,
        "Title": "probability two people provide identical answer survey question",
        "Body": "problem survey contains binary question yes response two people answer survey probability answer question match word four match answer consider overall survey response similar people",
        "Tags": [
            "combinatorics"
        ],
        "AnswerCount": 2.0,
        "CommentCount": 3,
        "Topics": {
            "13": 0.3034906,
            "43": 0.3463889
        }
    },
    {
        "Id": 187078,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "16/12/2015",
        "OwnerUserId": 14346.0,
        "Score": 1,
        "ViewCount": 641.0,
        "Title": "use bayesian anova",
        "Body": "study anova extension since week quite confuse look many user struggle difficulty related non-gaussianity inhomogenous-variance miss data need rule-of-thumbs trick sometimes bit complicate explain non statistical related scientific publication easy find site many post related problem moreover use see researcher work hard use standard anova best way time finish publishing result hypothesis test actually really cover hand look exist simple tool allow perform bayesian anova result flexible model lead potentially less restrictive assumption interpretation week reading guess learn not-so-true mainly-wrong however practice look solution far widely use one use bayesian anova",
        "Tags": [
            "hypothesis-testing",
            "bayesian",
            "anova",
            "multivariate-analysis"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 2,
        "Topics": {
            "21": 0.10927473,
            "43": 0.535514
        }
    },
    {
        "Id": 177299,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "16/10/2015",
        "OwnerUserId": 92346.0,
        "Score": 1,
        "ViewCount": 135.0,
        "Title": "meta-analysis standard deviation",
        "Body": "anybody know whether possible conduct meta-analysis outcome interest difference standard deviation two medical intervention instead difference mean assess continuous variable rcts",
        "Tags": [
            "meta-analysis"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 2,
        "Topics": {
            "25": 0.13822517,
            "30": 0.14242269,
            "43": 0.14344253
        }
    },
    {
        "Id": 392098,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 392105.0,
        "CreationDate": "12/02/2019",
        "OwnerUserId": 236855.0,
        "Score": 2,
        "ViewCount": 103.0,
        "Title": "try pair t-test one variable normally distribute log transformation work",
        "Body": "try pair t-test one variable normally distribute vegetarian enhance see boxplot try log transformation make p value shapiro-wilk test even small variable meat enhance however normally distribute rating base likert scale situation",
        "Tags": [
            "normal-distribution",
            "paired-data"
        ],
        "AnswerCount": 2.0,
        "CommentCount": 11,
        "Topics": {
            "39": 0.10550372,
            "43": 0.3521739,
            "45": 0.11256914,
            "46": 0.102684446
        }
    },
    {
        "Id": 14710,
        "PostTypeId": 2,
        "ParentId": 14703.0,
        "AcceptedAnswerId": "",
        "CreationDate": "23/08/2011",
        "OwnerUserId": 5884.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "may misunderstand question want know immediately packet bit error could clarify question clearly state want find ber essentially probability bit error bit error -ber n get probability packet succeed simulation determine give packet successful get number prng less -ber n packet successful otherwise packet contain bit error",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "43": 0.39651605
        }
    },
    {
        "Id": 381176,
        "PostTypeId": 2,
        "ParentId": 381123.0,
        "AcceptedAnswerId": "",
        "CreationDate": "09/12/2018",
        "OwnerUserId": 2958.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "null hypothesis seem standard gaussian distribution mean variance alternative hypothesis seem mixture standard laplace distribution mean variance standard gaussian distribution though would get similar rejection region alternative standard laplace distribution note laplace distribution high density compare gaussian distribution density laplace distribution also high density tail combination lead result observe seem make minor error go instead change inequality later seem reverse inequality later go reach less correct result may also lose factor particularly important think might well choose rejection region becomes depend left hand side quadratic equality say lead rejection great value equal large enough rejection less value equal presentation rather suggests critical value earlier value lead small value cause rejection happen e case example either side key value seem get rejection region make seem get rejection region make without central rejection region",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "43": 0.36086553,
            "44": 0.19113345,
            "45": 0.15000007
        }
    },
    {
        "Id": 61104,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "06/06/2013",
        "OwnerUserId": 25932.0,
        "Score": 6,
        "ViewCount": 3602.0,
        "Title": "covariance matrix",
        "Body": "wonder anyone know really productive way graph map plot visualize covariance matrix data currently use basic job look something well also use shiny interaction user data also seem pretty basic please keep mind matrix x large neat creative idea know package would interested",
        "Tags": [
            "r",
            "self-study",
            "data-visualization",
            "covariance"
        ],
        "AnswerCount": 3.0,
        "CommentCount": 2,
        "Topics": {
            "3": 0.21381138,
            "36": 0.15628661,
            "43": 0.34462744
        }
    },
    {
        "Id": 65439,
        "PostTypeId": 2,
        "ParentId": 64614.0,
        "AcceptedAnswerId": "",
        "CreationDate": "24/07/2013",
        "OwnerUserId": 5739.0,
        "Score": 3,
        "ViewCount": "",
        "Title": "",
        "Body": "go along path stack data set together define super-strata correspond two data set wave know independent thus new stratum cross year stratum psus original design weight original design suggest comment way combine estimate test propose literature wu us empirical likelihood base common variable two data set continuous variable ideally would want use kolmogorov-smirnov test flat data know whether extension work survey data doubt may convert continuous variable ordinal one say percentile group equal width bin variable range function sample size commonly use number bin histogram apply rao-scott",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "11": 0.107026406,
            "39": 0.1034711,
            "43": 0.14826204
        }
    },
    {
        "Id": 185575,
        "PostTypeId": 2,
        "ParentId": 185566.0,
        "AcceptedAnswerId": "",
        "CreationDate": "07/12/2015",
        "OwnerUserId": 74533.0,
        "Score": 4,
        "ViewCount": "",
        "Title": "",
        "Body": "interaction tricky short answer effect v big v calculation follow hold interaction term suggests zero v high v score increase readmission v high v score also increase readmission score v lead somewhat low readmission first case interpretation possible speculative might v associate high v additional effect v less big v though one could say v sole contribute factor big impact",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "30": 0.14774501,
            "43": 0.37615046
        }
    },
    {
        "Id": 262233,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 318815.0,
        "CreationDate": "16/02/2017",
        "OwnerUserId": 40252.0,
        "Score": 18,
        "ViewCount": 464.0,
        "Title": "sample distribution two independent bernoulli population",
        "Body": "let assume sample two independent bernoulli random variable prove assume",
        "Tags": [
            "distributions",
            "sampling",
            "bernoulli-distribution"
        ],
        "AnswerCount": 2.0,
        "CommentCount": 16,
        "Topics": {
            "19": 0.19370833,
            "20": 0.28631625,
            "43": 0.27795646
        }
    },
    {
        "Id": 181659,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "13/11/2015",
        "OwnerUserId": 61923.0,
        "Score": 0,
        "ViewCount": 176.0,
        "Title": "correctly compare bacterial count mouse different genotype",
        "Body": "far know cfu count bacterial suspension approximately follow poisson distribution order perform one-way anova mathematical transformation achieve normality do correct transformation would ideal case issue compare mean colony form unit count group mouse group different genotype shapiro-wilks test normality show count follow normal distribution thank much least count large mean count three different group g g g however unsure wether poisson regression may perform count large variance sample huge e e would constitute dispersion correct",
        "Tags": [
            "anova",
            "poisson-distribution",
            "biostatistics",
            "logarithm"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "18": 0.10842985,
            "39": 0.100536,
            "43": 0.2660007,
            "45": 0.10724795
        }
    },
    {
        "Id": 434235,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "02/11/2019",
        "OwnerUserId": 78564.0,
        "Score": 1,
        "ViewCount": 36.0,
        "Title": "parameter estimation generalize beta distribution",
        "Body": "interested determine explicit formula parameter generalize beta gb distribution mcdonald eludes provide solution wonder someone run idea proceed work description set general beta distribution define follow begin equation gb b c p q frac ap- -c left frac b right q- b ap b p q c left frac b right p q end equation beta function mcdonald show moment gb distribution begin equation mathbb e gb h frac b hb left p frac h q right b p q f left p frac h frac h c p q frac h right end equation hypergeometric series current effort correspond probability density function sample independent identically distribute iid gb random variable begin equation f n b c p q prod j n frac j ap- left -c left frac j b right right q- b ap b p q left c left frac j b right right p q end equation family distribution parameter maximize likelihood five parameter simultaneously possible individually since logarithm function continuous strictly increase function range likelihood value maximize likelihood also maximize logarithm log-likelihood necessarily strictly increase log-likelihood write follow begin equation ln left mathcal l b c p q right ln left prod j n frac j ap- left -c left frac j b right right q- b ap b p q left c left frac j b right right p q right end equation becomes begin equation ln left mathcal l right n ln ap- sum j n ln j q- sum j n ln left -c left frac j b right right apn ln b n ln b p q p q sum j n ln left left c frac j b right q right end equation compute derivative log-likelihood obtain begin equation frac partial mathcal l partial frac n p n ln b p sum j n ln j sum j n frac -c q- left frac b right ln left frac j b right -c left frac j b right end equation begin equation frac partial mathcal l partial b frac n p b sum j n frac -c q- j left frac j b right a- b left -c left frac j b right right sum j n frac c q p q j left frac j b right q- b left c left frac j b right right end equation begin equation frac partial mathcal l partial c sum j n left frac q- left frac j b right -c left frac j b right frac p q left frac j b right q c left frac j b right q right end equation begin equation frac partial mathcal l partial p -a n ln b sum j n left ln j ln left c left frac j b right q right q psi p psi p q right end equation begin equation frac partial mathcal l partial q frac n p sum j n left ln frac j b frac -c q- left frac j b right ln frac j b -c left frac j b right right end equation represent polygamma function notice certain case solve one parameter tersm others unable find approach solve explicitly term observation example look notice solve term parameter certain approach would let solve parameter explicitly even possible solve explicitly assistiance would appreciative mcdonald james b xu yexiao j generalization beta distribution application journal econometrics \u2013 \u2013 doi",
        "Tags": [
            "distributions",
            "mathematical-statistics",
            "estimation",
            "beta-distribution"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 0,
        "Topics": {
            "19": 0.59389776,
            "37": 0.10877691
        }
    },
    {
        "Id": 376606,
        "PostTypeId": 2,
        "ParentId": 376579.0,
        "AcceptedAnswerId": "",
        "CreationDate": "12/11/2018",
        "OwnerUserId": 119261.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "note distribution function begin align f p x le x le ldots x n le p x le n theta n left int b x dx right n quad theta end align density therefore begin align f n theta n left int b x dx right n- b mathbf theta end align know simplify expression suppose unbiased estimator base exist let estimator begin align qquad quad e h theta implies n theta n int theta h b left int b x dx right n- dt theta implies int theta h b left int b x dx right n- dt frac theta n theta n end align differentiate equation wrt begin align h theta b theta left underbrace int theta b x dx theta right n- frac theta -n theta theta n theta n implies h theta quad end align lehmann-scheffe would give umvue assume already exists verify whether actually exists exists problem",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "19": 0.5362014,
            "20": 0.15536705
        }
    },
    {
        "Id": 46983,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 46995.0,
        "CreationDate": "04/01/2013",
        "OwnerUserId": 18338.0,
        "Score": 4,
        "ViewCount": 278.0,
        "Title": "get representative vector large set compare sample",
        "Body": "small team programmer try solve little problem think need advice professional mathematician want know picture card identity card implement algorithm simplify lot focus math problem take sample lot id card make color map one map dimensional vector decimal value array element previous data calculate one representative vector actually make representative vector calculate arithmetic mean dimension take image want identify calculate color map vector compare representative vector use pearson correlation coefficient coefficient near image id card obtain reasonable result question method well arithmetic mean construct representative vector method well pearson correlation coefficient compare vector edit plan representative vector lot document idcards driver license passport residence card etc try identify wich category card compare fit well dimension vector call color map represent percentage pixel image color sum value vector allways main reason use pearson thought result affected number pixel image empiric result seem confirms fact obtain exactly result use percentage simply use count color admit limited knowledge statistic could take wrong decision decision others",
        "Tags": [
            "classification",
            "sampling",
            "data-mining",
            "method-comparison"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 4,
        "Topics": {
            "43": 0.395623
        }
    },
    {
        "Id": 16885,
        "PostTypeId": 2,
        "ParentId": 423.0,
        "AcceptedAnswerId": "",
        "CreationDate": "12/10/2011",
        "OwnerUserId": 3919.0,
        "Score": 7,
        "ViewCount": "",
        "Title": "",
        "Body": "",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "43": 0.5170266,
            "47": 0.13092211
        }
    },
    {
        "Id": 368567,
        "PostTypeId": 2,
        "ParentId": 368531.0,
        "AcceptedAnswerId": "",
        "CreationDate": "25/09/2018",
        "OwnerUserId": 3382.0,
        "Score": -2,
        "ViewCount": "",
        "Title": "",
        "Body": "one thorniest issue transfer function model lack clarity result present autobox take final model express forecast regression-like term e explicit equation show understandable layman presentation unique feature refer rhside txt second example first example use data provide arima independent variable obtain rhside txt presentation express forecast term one input history error obtain multiply denominator structure x var apple suggest ask spss provide useful report others struggle understand model simple term note re-expression get complicate either denominator transfer function structure structure arima model structure perhaps spss present",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 4,
        "Topics": {
            "3": 0.19420402,
            "24": 0.18761876,
            "43": 0.28110513
        }
    },
    {
        "Id": 2026,
        "PostTypeId": 2,
        "ParentId": 2007.0,
        "AcceptedAnswerId": "",
        "CreationDate": "22/08/2010",
        "OwnerUserId": 635.0,
        "Score": 7,
        "ViewCount": "",
        "Title": "",
        "Body": "probably comprehensive list find mloss org",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "3": 0.23015466,
            "15": 0.19777183,
            "43": 0.2847448
        }
    },
    {
        "Id": 256585,
        "PostTypeId": 2,
        "ParentId": 256440.0,
        "AcceptedAnswerId": "",
        "CreationDate": "16/01/2017",
        "OwnerUserId": 78964.0,
        "Score": 3,
        "ViewCount": "",
        "Title": "",
        "Body": "correlation specify value rho ac matrix correlation positive semi-definite possible value rho ac give correlation minimum maximum possible value rho ac code cvx matlab find minimum value rho ac maximum value achieve identical program use maximize rather minimize although trivial find minimum maximum possible value rho ac example approach still work quite well dimension problem increase run program rho ab rho bc minimum possible value rho ac found maximum possible value rho ac found note although example trivially solve without fancy apparatus solution method readily accurately numerically stably also work high dimensional problem complicate variant let positive semi-definite cone guide possible combination correlation bottom line dimensional positive semi-definite cone satisfy transitivity rd parameter respect parameter note rho ab rho bc non-negative unequal maximum value rho ac less edit regard edit answer consider example example corr b corr b c analyze show per mathematics positive semi-definite cone corr c particular corr c might equal",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "43": 0.30612904,
            "46": 0.14446019
        }
    },
    {
        "Id": 386975,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "13/01/2019",
        "OwnerUserId": 43450.0,
        "Score": 4,
        "ViewCount": 65.0,
        "Title": "name mean- median-like value",
        "Body": "consider data point well follow definition definition include median well mean name general class mean median estimator follow image show example data various value red line show value horizontal axis depend value red vertical axis observe value influence outlier converge intuitively make sense compare behaviour -norms also generalize simple definition multiple dimension replace absolute value suitable norm",
        "Tags": [
            "estimation",
            "mean",
            "median"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "25": 0.15832938,
            "36": 0.10286765,
            "43": 0.21856925,
            "47": 0.13245437,
            "49": 0.124547794
        }
    },
    {
        "Id": 242826,
        "PostTypeId": 2,
        "ParentId": 242110.0,
        "AcceptedAnswerId": "",
        "CreationDate": "27/10/2016",
        "OwnerUserId": 136040.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "get issue data vegan package r package syntax calculate jaccard distance matrix must include explicit argument specie abundance input binary input abundance table binary argument absent calculation switch extend jaccard compute bray\u2013curtis dissimilarity result matrix similar bray-curtis argument place jaccard matrix similar bray-curtis also similar jaccard matrix calculate use different r package ecodist many downstream r package use process visualize microbial data rely vegan convenience function often include detailed documentation dependency jaccard issue brought forum one favorite microbial package phyloseq use observe particular conundrum",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "3": 0.2271109,
            "38": 0.14284858,
            "43": 0.22424622
        }
    },
    {
        "Id": 270314,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "28/03/2017",
        "OwnerUserId": 154954.0,
        "Score": 3,
        "ViewCount": 541.0,
        "Title": "model specification glmer lme vary slope",
        "Body": "estimate mixed model use lme need vary intercept term post-stratifying result census category however also want add random slope term effect income allow vary state model estimate question appropriate since include understand calculate random intercept income also calculate random intercept slope income state interested evaluate coefficient variable rather model able produce valid prediction dependent variable different combination demographic model bias estimation multilevel model random intercept random slope intercept term include model",
        "Tags": [
            "mixed-model",
            "lme4-nlme"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 2,
        "Topics": {
            "33": 0.44450492,
            "43": 0.23108217
        }
    },
    {
        "Id": 48117,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "20/01/2013",
        "OwnerUserId": 19850.0,
        "Score": 8,
        "ViewCount": 1507.0,
        "Title": "compare ranked data",
        "Body": "question analyze ranked data data look like group people hiv group people living village ask rank challenge people hiv accord importance f e physical health social acceptance mental health know certain challenge perceive differently people hiv people another question respondent ask pick individually list challenge challenge know people hiv choose different challenge people best way present finding statistical test kruskal wallis possible look internet stuck",
        "Tags": [
            "ordinal-data",
            "ranking",
            "group-differences"
        ],
        "AnswerCount": 3.0,
        "CommentCount": 1,
        "Topics": {
            "39": 0.17554109,
            "43": 0.37856632
        }
    },
    {
        "Id": 373769,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "25/10/2018",
        "OwnerUserId": 201474.0,
        "Score": 0,
        "ViewCount": 40.0,
        "Title": "remove cofounding effect variable",
        "Body": "work team collect data bicycle biometric t-shirt measure ventilation rate problem last data collection participant use mask protect themself air-pollution significantly affected measure variable ventilation rate collect three participant time resolution one minute previous data collection participant new data corrupt effect mask way clean data corrupt know previous data participant variable link ventilation rate like speed participant mean slope terrain minute measure mean acceleration etc found third part blog interest talk contrastive dataset confound signal somone idea welcome best",
        "Tags": [
            "noise",
            "data-cleaning",
            "bias-correction"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 3,
        "Topics": {
            "14": 0.13458572,
            "15": 0.117060155,
            "43": 0.21256208
        }
    },
    {
        "Id": 305297,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "27/09/2017",
        "OwnerUserId": 151862.0,
        "Score": 1,
        "ViewCount": 202.0,
        "Title": "autocorrelation affect regularize regression",
        "Body": "suppose want make prediction response predictor autocorrelation response variable ols would problem residual would autocorrelation want predict response use regularize least square like lasso ridge elastic net care variance coefficient anything nature test hypothesis feel like might miss something",
        "Tags": [
            "regression",
            "time-series",
            "forecasting",
            "autocorrelation",
            "regularization"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 1,
        "Topics": {
            "5": 0.23505983,
            "23": 0.13174914,
            "24": 0.10489213,
            "43": 0.36834162
        }
    },
    {
        "Id": 37679,
        "PostTypeId": 2,
        "ParentId": 37569.0,
        "AcceptedAnswerId": "",
        "CreationDate": "20/09/2012",
        "OwnerUserId": 14230.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "log-normal distribution fully define pair parameter since want fit distribution data sufficient estimate two value normally would access raw data would apply standard maximum likelihood estimator mles straightforward mean logarithm observe data standard deviation logarithm data case raw data instead sketchy information cumulative distribution function cdf roughly know fraction distribution set value still estimate log-normal parameter distribution kind information subtlety two approach come mind first quick dirty one produce entirely accurate parameter estimate get close enough get sense distribution look like want roughly gini coefficient would second complicate accurate kind data quick dirty approximation quick dirty solution information bin version cdf represent set pair fraction distribution value note say ppp average within bin technically distinct cdf calculation distinction make difference recall definition mean probability observe approximate use bin cdf information like size width th bin bin similarly standard deviation definition becomes apply data need let since work log-normal distribution rather normal gaussian distribution cod estimator fairly easy numerical experiment estimator consistently get slight error estimate relative underlie population value use generate synthetic log-normal data use data treat estimate value highly accurate get need apply mathematically sophisticated approach sketch maximum likelihood approach complicate accurate solution derive maximum likelihood parameter estimate particular representation log-normal distribution e bin cdf definition log-normal pdf cdf error function let short-hand representation cdf normally would say indicate depends parameter choice go drop notation henceforth remember imply relevant want assume quantile data drawn bin version distribution cdf e integral let integral mathematically log-likelihood observe quantile information sophisticated approach would estimate maximize function parameter would give maximum likelihood estimate log-normal model give observe information arbitrary choice analytic solution mle possible regularly space choice bin boundary may regardless however may always numerically maximize function many numerical software package whisper right word make approach complicate need get mathematics correct code numerical routine estimation data accuracy answer really important approach might worth extra effort",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 5,
        "Topics": {
            "43": 0.33219987,
            "45": 0.10091911
        }
    },
    {
        "Id": 299799,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 299804.0,
        "CreationDate": "25/08/2017",
        "OwnerUserId": 173310.0,
        "Score": 2,
        "ViewCount": 207.0,
        "Title": "proceed find cointegration johansen",
        "Body": "try determine stock index cointegrated run cointegration test fails reject cointegration use aic determine appropiate number lag different lag get cointegration would next step know vecm use cointegrating relationship",
        "Tags": [
            "cointegration"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "24": 0.40767577,
            "43": 0.17776476,
            "44": 0.11876185
        }
    },
    {
        "Id": 371251,
        "PostTypeId": 2,
        "ParentId": 280665.0,
        "AcceptedAnswerId": "",
        "CreationDate": "10/10/2018",
        "OwnerUserId": "",
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "give answer context paragraph cite k n cross-validation estimator approximately unbiased true expect prediction error high variance n training set similar one another cv estimator true expect prediction error base training set example expectation training set sample understand correctly paragraph regard high variance say high difference expect error error estimate cv average fold make sense model fit particular training set training fold similar within leave-one-out however training fold similar within cv round estimate probably differs lot swap training sample cv k-fold cv since diversify training fold average affect across k-folds estimate vary less word leave-one-out cv estimator basically almost like holdout method rotate fold base error estimate one validation set training example high variance compare estimate k-fold average fold already training somewhat diverse model within k-fold round word swap training set estimate error via k-fold probably vary much edit read answer cross-validated internet general think seem confusion estimator refer think people refer model high variance ml talk loss dominate variance component v high variance k-fold cv estimator another set answer refer variance sample variance regard fold someone say k-fold high variance suggest specific answer different either case",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 4,
        "Topics": {
            "10": 0.2506714,
            "43": 0.28798968
        }
    },
    {
        "Id": 81022,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 81030.0,
        "CreationDate": "02/01/2014",
        "OwnerUserId": 23606.0,
        "Score": 3,
        "ViewCount": 3018.0,
        "Title": "interpret interaction continuous variable logistic regression",
        "Body": "struggle understand interpret interaction term logistic regression explanatory variable categorical continuous continuous reduce model interpret interaction set variable three category low medium high graph probability fully spawn temperature level try understand output correct",
        "Tags": [
            "regression",
            "logistic",
            "interaction",
            "interpretation",
            "continuous-data"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 2,
        "Topics": {
            "11": 0.32825586,
            "30": 0.288911
        }
    },
    {
        "Id": 329069,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "16/02/2018",
        "OwnerUserId": 4426.0,
        "Score": 3,
        "ViewCount": 54.0,
        "Title": "test statistical significance necessary mean surely chart experiment",
        "Body": "purely pedagogical purpose observe experiment design workplace design consumer face system b test tweak small thing see high impact whatever measure possible reproduce outside organization control group thing left experiment one change control rest experiment b another one thing change control rest different let say mean performance whatever mean value experiment run control b verdict winner question make sense even test statistical significance high variation mean get think historically test statistical significance always apply thing even marginal improvement matter example medication liquor distillation etc variation high example perhaps reason create since observe difference per se analyze data ascertain fair observation practitioner statistician observe high variation seem good enough get work do long experiment change single thing okay perform statistical analysis since go look risk statistician do situation like loose get wonder people really understand go really miss something simple obvious",
        "Tags": [
            "hypothesis-testing",
            "statistical-significance",
            "experiment-design"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 4,
        "Topics": {
            "43": 0.42567322
        }
    },
    {
        "Id": 448344,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 448371.0,
        "CreationDate": "07/02/2020",
        "OwnerUserId": 124694.0,
        "Score": 3,
        "ViewCount": 58.0,
        "Title": "simple way compute follow enumeration problem",
        "Body": "friend mine give problem solve n unique bowl draw replacement probability see least one bowl draw make tedious computation find answer validate monte-carlo simulation small value n get impression miss something question follow simpler way compute enumeration problem",
        "Tags": [
            "probability"
        ],
        "AnswerCount": 2.0,
        "CommentCount": 0,
        "Topics": {
            "13": 0.11938056,
            "20": 0.15765476,
            "27": 0.10504646,
            "43": 0.34372702
        }
    },
    {
        "Id": 11260,
        "PostTypeId": 2,
        "ParentId": 11256.0,
        "AcceptedAnswerId": "",
        "CreationDate": "26/05/2011",
        "OwnerUserId": 2645.0,
        "Score": 7,
        "ViewCount": "",
        "Title": "",
        "Body": "normality assumption convenient property model residual since enables correct inference estimate parameter critical value many test also dependent assumption therefore correction make may roughly take strict rule-of-thumb criterion increase acceptable range test however ruin regression estimator thus may still need check assumption produce well behave prediction data-mining hypothesis test would bit difficult point agree huber need clarify purpose model regard tip first glance seem distribution transformation could approximate truncate version continuous distribution exponential gamma log-normal pareto log-normal case still may move something close normality another option could try something like fitting combo generalize logistic function logistic regression since know upper low limit seem feasible",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 5,
        "Topics": {
            "43": 0.44793567,
            "45": 0.10373787
        }
    },
    {
        "Id": 35247,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "28/08/2012",
        "OwnerUserId": 13564.0,
        "Score": -1,
        "ViewCount": 592.0,
        "Title": "adk test sample size",
        "Body": "use function ad test adk package estimate data sample follow distribution family question value simulate distribution value many want edit use function package get follow result ex check validity function conclusion must compare also get different conclusion use",
        "Tags": [
            "r",
            "anderson-darling"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 6,
        "Topics": {
            "3": 0.27077588,
            "43": 0.34945384,
            "44": 0.1417307
        }
    },
    {
        "Id": 267764,
        "PostTypeId": 2,
        "ParentId": 267475.0,
        "AcceptedAnswerId": "",
        "CreationDate": "16/03/2017",
        "OwnerUserId": 133886.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "look model allow time-varying coefficient e g state-space model dynamic linear model",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "3": 0.14514889,
            "28": 0.14809997,
            "33": 0.21345107,
            "43": 0.11426417
        }
    },
    {
        "Id": 296745,
        "PostTypeId": 2,
        "ParentId": 296743.0,
        "AcceptedAnswerId": "",
        "CreationDate": "08/08/2017",
        "OwnerUserId": 86652.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "answer main question twice much data arisen completely independently support particular model another one unsurprising well support model favor even course use set data twice thing duplicate original data occur independently term detail quantity proportional probability probability start bound",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 3,
        "Topics": {
            "1": 0.14679655,
            "13": 0.11286234,
            "43": 0.3049171,
            "47": 0.105062924
        }
    },
    {
        "Id": 133928,
        "PostTypeId": 2,
        "ParentId": 133886.0,
        "AcceptedAnswerId": "",
        "CreationDate": "18/01/2015",
        "OwnerUserId": 24905.0,
        "Score": 5,
        "ViewCount": "",
        "Title": "",
        "Body": "give let random variable pdf let random variable pdf kullback-leibler divergence true distribution lognormal approximation give first term second term solution note function mathstatica package mathematica denotes derivative digamma function",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 11,
        "Topics": {
            "20": 0.15173566,
            "43": 0.17679738,
            "45": 0.4184453,
            "47": 0.10400237
        }
    },
    {
        "Id": 248176,
        "PostTypeId": 2,
        "ParentId": 247183.0,
        "AcceptedAnswerId": "",
        "CreationDate": "27/11/2016",
        "OwnerUserId": 48591.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "difficult ass fit negative binomial integer-valued glm matter deviance residual also perfectly fitting nb model may exhibit inhomogeneous deviance residual however use dharma r package transform residual gl standardize space do visually ass test residual problem deviation distribution residual dependency predictor heteroskedasticity autocorrelation normal way see package vignette worked-through example",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 6,
        "Topics": {
            "1": 0.13794789,
            "3": 0.14814128,
            "43": 0.24431652
        }
    },
    {
        "Id": 128877,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 128878.0,
        "CreationDate": "12/12/2014",
        "OwnerUserId": 56352.0,
        "Score": 2,
        "ViewCount": 1505.0,
        "Title": "find pmf bivariate distribution roll black red four-sided die",
        "Body": "roll pair four-sided dice one red one black let equal outcome red die let equal sum two dice define joint pmf space far outcome die probability roll thus outcome combine roll two way make value example make could red die black red die black die therefore pmf answer however idea",
        "Tags": [
            "probability",
            "self-study",
            "bivariate"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 7,
        "Topics": {
            "13": 0.2846577,
            "20": 0.17678511,
            "43": 0.19395372
        }
    },
    {
        "Id": 31597,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 31600.0,
        "CreationDate": "03/07/2012",
        "OwnerUserId": 3310.0,
        "Score": 12,
        "ViewCount": 27055.0,
        "Title": "graph probability curve logit model multiple predictor",
        "Body": "follow probability function model look like visualize via probability curve look like one consider add couple variable original regression equation let say add gender categorical f age categorical model end r generate similar probability curve tell probability accounting three predictor lose want find probability every possible permutation variation bid gender age probability similarly bid gender f age probability want generate probability curve allow visualize anyone help may completely misunderstand kind information one glean logit model please tell also misunderstand theory",
        "Tags": [
            "r",
            "probability",
            "data-visualization",
            "logistic"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 4,
        "Topics": {
            "3": 0.115785845,
            "11": 0.1189992,
            "36": 0.12278758,
            "43": 0.3189738
        }
    },
    {
        "Id": 434309,
        "PostTypeId": 2,
        "ParentId": 434172.0,
        "AcceptedAnswerId": "",
        "CreationDate": "02/11/2019",
        "OwnerUserId": 919.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "think formula might harder describe simple algorithm appear characterize multiset notation refers copy element number sum go order element follow etc order part usual notion multiset useful analysis write natural number let number permutation length clearly depends object might permutation consists element appear time two permutation consider permutation re-ordering first element next one let call sequence permutation let number time appear entail obviously sum consideration outline post make clear number re-orderings divide product justifies last calculation function define may therefore count permutation enumerate sequence permutation naturally fall group accord many copy appear permutation namely know element permutation sequence within permutation number distinct possible position copy binomial coefficient yield recursion base case worst-case computational effort proportional product good practicable small multisets represent say word natural language example consider string mississippi consider order multiset let instance find nine sequence permutation ii ip im s sp sm pp pm correspond sequence letter like ii one permutation correspond sequence distinct letter like two permutation si result total recursion find therefore similarly work whence agree previous enumeration implementation example check brute-force count assumes name vector great sum element reason name element otherwise unnecessary apparent upon inspect table row sequence permutation example produce output aficionado may notice multisets natural platform neatly represent array non-negative integer unique component name test may list count permutation possible length mississippi multiset agree length permutation",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "20": 0.12977271,
            "27": 0.25096694,
            "43": 0.3462908
        }
    },
    {
        "Id": 308920,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "19/10/2017",
        "OwnerUserId": 181513.0,
        "Score": 0,
        "ViewCount": 13.0,
        "Title": "probability experience give outcome n time",
        "Body": "let assume distribution outcome also probability function let assume run experiment time talk real experiment know full range detail try reconstruct try estimate probability contain single element run least different outcome know experiment deterministic good time result conclude probability experiment deterministic achieve low bound probability probable event",
        "Tags": [
            "probability"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "13": 0.35799313,
            "43": 0.30551964
        }
    },
    {
        "Id": 240290,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "14/10/2016",
        "OwnerUserId": 134780.0,
        "Score": 0,
        "ViewCount": 209.0,
        "Title": "appropriate treatment logistic regression variable whose interpretation varies use spline",
        "Body": "work healthcare dataset binary outcome discharge yes one explanatory variable would like include model resource intensity score whose interpretation varies implies expect resource use implies less expect resource use implies great expect resource use variable continuous approximate range dataset scale score implies double resource use score long tail patient use even time resource typical patient currently use r rcs package use linear spline knot set incorporate variable implementation get improve model performance fit use variety diagnostics want sure interpret result correctly however variable identify significant report odds ratio fit cubic spline variable knot get odds ratio transform variable odds ratio remain predictor essentially unchanged two model either approach make sense pursue another approach transformation variable interpret report odds ratio respect resource intensity example currently linear spline odds ratio understand mean linear spline-transformed resource intensity score effect probability discharge correct edit attempt log transformation suggest good thought however plot partial residual transformation show clear u-shaped distribution comparison residual untransformed variable understand harrell regression model strategy appearance suggests variable would amenable model either transformation try unsuccessfully spline use spline simply report odds ratio variable report quartile basis etc understand summary rcs help report odds ratio data range",
        "Tags": [
            "r",
            "regression",
            "logistic",
            "data-transformation"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 7,
        "Topics": {
            "1": 0.11669303,
            "11": 0.16852652,
            "43": 0.230495
        }
    },
    {
        "Id": 72736,
        "PostTypeId": 2,
        "ParentId": 72709.0,
        "AcceptedAnswerId": "",
        "CreationDate": "14/10/2013",
        "OwnerUserId": 29187.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "say data stationary find stationary transform data differencing checked unit root test e g augment dickey-fuller test elliott-rothenberg-stock test kp test phillips-perron test schmidt-phillips test zivot-andrews test talk arma model confirm stationarity classical way identify arma p q acf plot pacf plot arma arma told another method identify p q eacf widely use univariate time series empirical study show aic usually tends overfitting advantage use aic automatic algorithm find best model usually recommend traditional time series textbook",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "24": 0.31991893,
            "43": 0.33288872
        }
    },
    {
        "Id": 159874,
        "PostTypeId": 2,
        "ParentId": 159865.0,
        "AcceptedAnswerId": "",
        "CreationDate": "04/07/2015",
        "OwnerUserId": 805.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "situation find distribution set data drawn sometimes quite confident drawn distribution e g clearly bimodal might able pretty sure random sample unimodal distribution test whether data come specific distribution say normal distribution use something shapiro-wilk test rejection might lead confident data drawn random normal population failure reject imply data drawn normal population large enough sample detect deviation test specific null hypothesis e parameter specify kolmogorov-smirnov anderson-darling test specific distributional form parameter unspecified e g shapiro-wilk test normality kind test call goodness fit test many post site one might worth start normality test essentially useless",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "43": 0.29982468,
            "44": 0.27023238,
            "45": 0.16407837
        }
    },
    {
        "Id": 248300,
        "PostTypeId": 2,
        "ParentId": 4551.0,
        "AcceptedAnswerId": "",
        "CreationDate": "28/11/2016",
        "OwnerUserId": 71287.0,
        "Score": 5,
        "ViewCount": "",
        "Title": "",
        "Body": "rush model spending enough time understand preprocessing data",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 6,
        "Topics": {
            "43": 0.37409574
        }
    },
    {
        "Id": 445451,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "19/01/2020",
        "OwnerUserId": 271296.0,
        "Score": 0,
        "ViewCount": 16.0,
        "Title": "heckman selection selection v outcome equation",
        "Body": "common example heckman selection model involves wage observe individual chooses participate labor force first stage probit model dependent variable labor force participation indicator second stage model wage w observe conditional work correction term add \u2019 read couple work paper recently employ heckman model selection outcome equation seem less directly related \u2019 wonder creates issue instance second-stage total individual spending base consumption data first stage model labor force participation situation something like might reasonable personally \u2019 work heckman model quite time clarification much appreciate",
        "Tags": [
            "heckman",
            "selection-bias"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 0,
        "Topics": {
            "1": 0.11186347,
            "43": 0.3603688
        }
    },
    {
        "Id": 77153,
        "PostTypeId": 2,
        "ParentId": 77130.0,
        "AcceptedAnswerId": "",
        "CreationDate": "20/11/2013",
        "OwnerUserId": 13555.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "follow none option perfect aggregate identical sequence propose see compute low half distance matrix use argument reduce granularity sequence see modify time granularity state sequence use say week instead day low number distinct sequence hence impact proposition aggregate sequence use computer ram",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "3": 0.1276888,
            "34": 0.11691276,
            "43": 0.23614107
        }
    },
    {
        "Id": 14171,
        "PostTypeId": 2,
        "ParentId": 14140.0,
        "AcceptedAnswerId": "",
        "CreationDate": "12/08/2011",
        "OwnerUserId": 183.0,
        "Score": 8,
        "ViewCount": "",
        "Title": "",
        "Body": "g power free software available mac window nice graph feature power analysis main graph broadly consistent graph show chl us simple straight line indicate null hypothesis alternate hypothesis test statistic distribution colour beta alpha separate colour nice feature g power support large number common power analysis scenario gui make simple student apply researcher explore screen shot slide take presentation give descriptive statistic section power analysis multiple graph show left chose one-tail t-test version would look like example also possible produce graph show functional relationship factor relevant statistical power hypothesis test e g alpha effect size sample size power etc present example graph one example graph",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "3": 0.13979733,
            "36": 0.16073515,
            "43": 0.34566808
        }
    },
    {
        "Id": 12899,
        "PostTypeId": 2,
        "ParentId": 12845.0,
        "AcceptedAnswerId": "",
        "CreationDate": "11/07/2011",
        "OwnerUserId": 4505.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "statement well bad percentage others refer percentile quantiles limited percent calculate direct comparison large number measure subject base theoretical distribution standard deviation mean population approximately normal th percentile",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "16": 0.11531432,
            "43": 0.3445656
        }
    },
    {
        "Id": 97074,
        "PostTypeId": 2,
        "ParentId": 97039.0,
        "AcceptedAnswerId": "",
        "CreationDate": "09/05/2014",
        "OwnerUserId": 44839.0,
        "Score": 6,
        "ViewCount": "",
        "Title": "",
        "Body": "john answer correct try explain bit clearly covariance give consider value f covariance significantly different zero first consider limit covariance range covariance possible standard deviation datasets prove result use cauchy-schwarz inequality case see hard define covariance significant range value depends variance datasets logical thing normalise covariance remove effect define correlation range make much easy determine value close zero obviously exact value consider significant depend exact setup much correlation think ignore",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "17": 0.11762225,
            "25": 0.11186834,
            "43": 0.3349819
        }
    },
    {
        "Id": 443098,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "02/01/2020",
        "OwnerUserId": 125405.0,
        "Score": 2,
        "ViewCount": 46.0,
        "Title": "question setup adversarial example paper intrigue property neural network",
        "Body": "paper intrigue property neural network process find adversarial example set follow section denote classifier mapping image pixel value vector discrete label set also assume associate continuous loss function denote give image target label aim solve follow box-constrained optimization problem minimize subject minimizer might unique denote one arbitrarily chosen minimizer informally closest image classify obviously task non-trivial general exact computation hard problem approximate use box-constrained l-bfgs concretely find approximation perform line-search find minimum minimizer follow problem satisfies minimize subject two question passage interpretation definition sentence equal minimum magnitude vector appear misunderstand something look minimum intuition term problem serf pull towards vector term serf pull towards perfect input image represent label likely away vector intuition true increase pull minimizer towards vector reduce magnitude would think would want find maximum minimizer expression satisfies would lead small perturbation still cause adversarial example wrong logic",
        "Tags": [
            "machine-learning",
            "neural-networks"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "20": 0.13691399,
            "23": 0.22605632,
            "43": 0.25189152
        }
    },
    {
        "Id": 263476,
        "PostTypeId": 2,
        "ParentId": 263469.0,
        "AcceptedAnswerId": "",
        "CreationDate": "22/02/2017",
        "OwnerUserId": 140300.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "problem plan use anova stock market data statistical distribution involve stock market data dispute since end recently someone first principle derivation return would exist variety circumstance narrow exception firm merge existence cash use anova distribution return equity security go concern must form transformation cauchy distribution truncate show cdf cauchy distribution limit one integral problem convert polar coordinate circumstance solution either cauchy distribution deformation cauchy distribution difficulty cauchy distribution lack mean therefore also lack variance first strange idea add number divide sample size therefore sample average course sample average mean nothing population average return truncate center location mode standardize term use scale parameter truncation disrupts simple descriptor like inter-quartile range well think variance feature distribution something must exist like nose feature vertebrate see something without nose probably vertebrate also render algorithmic trading method invalid field year clean-up master anova useful tool use stock anything else grows exponential rate cancer want master tool intrinsic value nonetheless",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 3,
        "Topics": {
            "28": 0.12090153,
            "43": 0.35028923
        }
    },
    {
        "Id": 211498,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "08/05/2016",
        "OwnerUserId": 114955.0,
        "Score": 0,
        "ViewCount": 251.0,
        "Title": "baum welch calculate transition probability",
        "Body": "try understand baum welch algorithm implement xl chosen simple example observation load l v fair f die calculate forward backward probability l f state calculate probability observation know calculation till point correct since get p observation row data e p observation forward probability l forward probability f backward probability l backward probability f row proceed calculate transition probability find value transition probability state transition l- l come state value seem correct calculate transition probability l- l row follow l- l forward probability l transition l- l emission observation l backward probability l probability observation estimate probability sum l- l sum l observation unable find transition probability value help appreciate",
        "Tags": [
            "machine-learning",
            "hidden-markov-model",
            "baum-welch"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "13": 0.24650967,
            "19": 0.10219542,
            "23": 0.11106438
        }
    },
    {
        "Id": 291707,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "15/07/2017",
        "OwnerUserId": 168971.0,
        "Score": 0,
        "ViewCount": 1554.0,
        "Title": "r calculate p-value random distribution",
        "Body": "want get p-value two randomly distribute observation x example let say test-statistic define mean x mean h p-value define p-value p observe h hold try calculate p-value try obviously seem work miss",
        "Tags": [
            "r",
            "hypothesis-testing",
            "p-value",
            "monte-carlo"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 8,
        "Topics": {
            "3": 0.20288633,
            "43": 0.1652637,
            "44": 0.16251469
        }
    },
    {
        "Id": 362066,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "13/08/2018",
        "OwnerUserId": 169609.0,
        "Score": 1,
        "ViewCount": 36.0,
        "Title": "kera nn loss get stuck",
        "Body": "mean neural network always get stuck exact number use binary-crossentropy loss strange local minimum happens regardless learn rate initialization activation function regularizer choice optimizer thing change change architecture new loss function thing want change",
        "Tags": [
            "neural-networks",
            "conv-neural-network",
            "loss-functions",
            "keras"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 4,
        "Topics": {
            "6": 0.38911536,
            "23": 0.17812358,
            "43": 0.30641788
        }
    },
    {
        "Id": 446724,
        "PostTypeId": 2,
        "ParentId": 446707.0,
        "AcceptedAnswerId": "",
        "CreationDate": "28/01/2020",
        "OwnerUserId": 67799.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "time series short fit many parameter recall var p model begin equation x x t- x t- dot p x t-p epsilon end equation parameter matrix dimension number parameter estimation simply procedes ols equation var model mean parameter per equation assume deterministic component constant case variable although two appear identically zero hence useful hence fit coefficient equation appear observation well-known matrix ols estimator invertible small number regressors",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "3": 0.14306028,
            "5": 0.12637317,
            "19": 0.13981834
        }
    },
    {
        "Id": 439584,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "06/12/2019",
        "OwnerUserId": 109055.0,
        "Score": 0,
        "ViewCount": 15.0,
        "Title": "hac confidence interval interrupt time series",
        "Body": "dataset quarterly death last year q -q q policy change might lead increase number death fit interrupt time series ols estimation follow specification time elapse since start study period whether intervention period give model bit autocorrelation want go arima model several reason correct standard error hac std error however would like know significant change slope v e v could derive confidence interval post-intervention period found way estimate hac confidence interval suggestion regard significance check slope like use anova compare seem work different sample would really appreciate suggestion thank",
        "Tags": [
            "regression",
            "time-series",
            "anova",
            "standard-error",
            "robust"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 0,
        "Topics": {
            "24": 0.11443393,
            "43": 0.30055225
        }
    },
    {
        "Id": 440471,
        "PostTypeId": 2,
        "ParentId": 440468.0,
        "AcceptedAnswerId": "",
        "CreationDate": "12/12/2019",
        "OwnerUserId": 16367.0,
        "Score": 4,
        "ViewCount": "",
        "Title": "",
        "Body": "let six side dice binary even inverse image set value wiki definition inverse image event even definition event event probability probability course summarize want find probability give value stochastic variable define function take value must case take value set value bring particular value definition inverse image",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "13": 0.15664971,
            "20": 0.25263637,
            "45": 0.17728466,
            "49": 0.13555789
        }
    },
    {
        "Id": 310323,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "27/10/2017",
        "OwnerUserId": 178953.0,
        "Score": 0,
        "ViewCount": 351.0,
        "Title": "bonferroni correction v adjust bonferroni p value",
        "Body": "r use follow option post hoc bonferroni p value base understand bonferroni correction alpha divide number comparison mean use compare nonadjusted p value alpha number test wonder first function adjusts value correction divide alpha number test also two way analysis provide result",
        "Tags": [
            "bonferroni"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 5,
        "Topics": {
            "3": 0.15084314,
            "39": 0.1248288,
            "44": 0.36566138
        }
    },
    {
        "Id": 57863,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "02/05/2013",
        "OwnerUserId": 24073.0,
        "Score": 3,
        "ViewCount": 359.0,
        "Title": "conditional expectation reciprocal normal",
        "Body": "know expect value normal random variable exist suppose condition interval contain zero example know numerical integration approximation use analytic formula found technique use evaluate integral",
        "Tags": [
            "mathematical-statistics"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 3,
        "Topics": {
            "20": 0.44344428,
            "43": 0.23667787
        }
    },
    {
        "Id": 371883,
        "PostTypeId": 2,
        "ParentId": 34396.0,
        "AcceptedAnswerId": "",
        "CreationDate": "15/10/2018",
        "OwnerUserId": 223612.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "compute l distance successive eigenvectors normalize matrix choose z score threshold e g new roll change threshold flip eigenvector factor loading order consistency roll window personally like force give sign correlation since volatile depend macro driver",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "35": 0.18335207,
            "43": 0.42932528
        }
    },
    {
        "Id": 401117,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "04/04/2019",
        "OwnerUserId": 239093.0,
        "Score": 2,
        "ViewCount": 242.0,
        "Title": "non-response bias affect confidence interval",
        "Body": "ask earlier idea behind put table follow question non-response bias identify justify different size confidence interval ci figure reason assume p-value small exercise add systematic error formation ci size account vary size ci research figure also remain follow-up data analyse sample size exercise large analyse sample would generally mean small random error systematic random error account differ size ci exercise online copy research article",
        "Tags": [
            "confidence-interval",
            "sampling",
            "p-value",
            "odds-ratio",
            "non-response"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "26": 0.14515227,
            "43": 0.35848916
        }
    },
    {
        "Id": 445822,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "21/01/2020",
        "OwnerUserId": 261852.0,
        "Score": 1,
        "ViewCount": 27.0,
        "Title": "try understand bayes classifier bayes error rate",
        "Body": "instance outcome class b c assume follow alternatively bayes classifier bayes error change different distribution x say",
        "Tags": [
            "bayesian",
            "classification"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "0": 0.27349833,
            "41": 0.20216522,
            "43": 0.16121082
        }
    },
    {
        "Id": 147,
        "PostTypeId": 2,
        "ParentId": 145.0,
        "AcceptedAnswerId": "",
        "CreationDate": "19/07/2010",
        "OwnerUserId": 142.0,
        "Score": 5,
        "ViewCount": "",
        "Title": "",
        "Body": "amazon free public data set use ec list",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "14": 0.20691448,
            "29": 0.19966832,
            "43": 0.25984165
        }
    },
    {
        "Id": 372406,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 372529.0,
        "CreationDate": "17/10/2018",
        "OwnerUserId": 138757.0,
        "Score": 1,
        "ViewCount": 113.0,
        "Title": "thing composition two gaussian process",
        "Body": "basic understand gaussian process understand guassian process set assignment gaussian distribution every element set meant expand idea function case total information function two function natural notion composition ie function composition gaussian process approximation function notion composition two gaussian process give two gaussian process composition define",
        "Tags": [
            "gaussian-process"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "20": 0.1687749,
            "34": 0.10041033,
            "43": 0.21564294,
            "45": 0.117582634,
            "47": 0.10394868
        }
    },
    {
        "Id": 244985,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "09/11/2016",
        "OwnerUserId": 137952.0,
        "Score": 0,
        "ViewCount": 1280.0,
        "Title": "binomial distribution way calculate error rate ensemble classifier",
        "Body": "learn ensemble classifier lot website book state assumption classifier independent error rate correlate error probability calculate however binomial distribution probability distribution use case binomial distribution modify even change another one like poisson example",
        "Tags": [
            "machine-learning",
            "binomial",
            "ensemble"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 1,
        "Topics": {
            "0": 0.15643889,
            "19": 0.18928367
        }
    },
    {
        "Id": 178918,
        "PostTypeId": 2,
        "ParentId": 178915.0,
        "AcceptedAnswerId": "",
        "CreationDate": "27/10/2015",
        "OwnerUserId": 67168.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "must load library call mass use library mass write code-line",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "3": 0.39291015,
            "43": 0.22470216
        }
    },
    {
        "Id": 242189,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "24/10/2016",
        "OwnerUserId": "",
        "Score": 0,
        "ViewCount": 28.0,
        "Title": "goodness fit two histogram",
        "Body": "two histogram want good match method available test goodness fit anyone suggest method say good fit thanks advance",
        "Tags": [
            "goodness-of-fit",
            "histogram"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 1,
        "Topics": {
            "36": 0.13278386,
            "43": 0.36614236
        }
    },
    {
        "Id": 182529,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "19/11/2015",
        "OwnerUserId": 88857.0,
        "Score": 1,
        "ViewCount": 131.0,
        "Title": "pick value normal distribution",
        "Body": "follow formula give follow sample sample assume generate value n sample posterior value edit forgot include",
        "Tags": [
            "probability",
            "normal-distribution",
            "chi-squared",
            "mean"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 1,
        "Topics": {
            "20": 0.10934237,
            "41": 0.1898686,
            "43": 0.33687323,
            "45": 0.1303191
        }
    },
    {
        "Id": 124425,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "17/11/2014",
        "OwnerUserId": 60913.0,
        "Score": 1,
        "ViewCount": 17.0,
        "Title": "filter outlier maintain consistency time newly add record",
        "Body": "item table price assign region filter outlier identify delete record whose price standard deviation away average region future add new record table apply filter rule de facto criterion different average standard deviation per zone change previously delete record guarantee consistency case",
        "Tags": [
            "outliers"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 1,
        "Topics": {
            "14": 0.13650914,
            "27": 0.17368215,
            "28": 0.11215036,
            "43": 0.156496
        }
    },
    {
        "Id": 19684,
        "PostTypeId": 2,
        "ParentId": 19568.0,
        "AcceptedAnswerId": "",
        "CreationDate": "11/12/2011",
        "OwnerUserId": 3382.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "give arima filter restate pure ar simply multiply ari divide would pure auto-regressive formulation call pi weight see book time series analysis right-hand side constant might take time figure sure use pure right-hand side equation conjunction residual get realization process acf series use identify arima model course realization sample acf course would depend input error noise vector",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "20": 0.117591865,
            "24": 0.32349518,
            "43": 0.30721068
        }
    },
    {
        "Id": 297360,
        "PostTypeId": 2,
        "ParentId": 284912.0,
        "AcceptedAnswerId": "",
        "CreationDate": "10/08/2017",
        "OwnerUserId": 99274.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "tercile contains one-third data cut low third middle third upper third tercile element particular subset data divide three group either low middle upper third generally number data entry multiple three interpolation use calculate tercile parameter term op want merely data divide three group although imply interpolation face value exclude either",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "27": 0.13636883,
            "43": 0.32552832,
            "47": 0.118588224
        }
    },
    {
        "Id": 186665,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "14/12/2015",
        "OwnerUserId": 44548.0,
        "Score": 1,
        "ViewCount": 61.0,
        "Title": "calculate confidence interval difference mean sample without individual sample value",
        "Body": "sample contain individual measurement know individual measurement group mean sample possible construct confidence interval difference mean two group would need know individual value least standard deviation among sample",
        "Tags": [
            "confidence-interval"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 3,
        "Topics": {
            "16": 0.1722928,
            "25": 0.11339035,
            "39": 0.20840454,
            "40": 0.15372036,
            "43": 0.14930959
        }
    },
    {
        "Id": 239399,
        "PostTypeId": 2,
        "ParentId": 239376.0,
        "AcceptedAnswerId": "",
        "CreationDate": "10/10/2016",
        "OwnerUserId": 686.0,
        "Score": 3,
        "ViewCount": "",
        "Title": "",
        "Body": "graph common science business think use exclusively scientist closest would probably highly specialized graph use almost exclusively scientist particular specialty however even might use business industry example chart use map temperature sun probably use almost exclusively people field would bet business somewhere use graph ought chosen represent data model specific field",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "36": 0.14367075,
            "43": 0.37565216,
            "47": 0.19872321
        }
    },
    {
        "Id": 304578,
        "PostTypeId": 2,
        "ParentId": 76815.0,
        "AcceptedAnswerId": "",
        "CreationDate": "23/09/2017",
        "OwnerUserId": 178233.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "bumped tread chance original answer formula factor ismissing",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "5": 0.13247257,
            "43": 0.3165088,
            "46": 0.13153385
        }
    },
    {
        "Id": 5834,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "30/12/2010",
        "OwnerUserId": 2516.0,
        "Score": 2,
        "ViewCount": 726.0,
        "Title": "first passage time distribution irreducible transient discrete-time markov chain dtmc",
        "Body": "markov chain state transient probability ever visit state start state suppose irreducible transient dtmc mean state transient want prove dtmc state space e probability ever reach state start state less clear prove thanksprasenjit",
        "Tags": [
            "markov-process"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 4,
        "Topics": {
            "20": 0.2332718,
            "34": 0.37171152,
            "43": 0.16039935
        }
    },
    {
        "Id": 34421,
        "PostTypeId": 2,
        "ParentId": 34418.0,
        "AcceptedAnswerId": "",
        "CreationDate": "16/08/2012",
        "OwnerUserId": 13138.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "check book tail risk hedge fund extreme value application chapter section mention limit distribution maximum follow either gumbel frechet weibull distribution whatever parent distribution f",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "29": 0.25094023,
            "43": 0.20111352,
            "45": 0.21607038
        }
    },
    {
        "Id": 358216,
        "PostTypeId": 2,
        "ParentId": 358092.0,
        "AcceptedAnswerId": "",
        "CreationDate": "20/07/2018",
        "OwnerUserId": 85665.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "comment make stripcharts variation default sometimes usefulfor visualize data stripcharts data somewhat similar b begin wonder whether two independent sample orwhether pair data high p-value suspicious view high p-values always worth second look p-value small reject null hypothesis large suspect model computation get fake independent data fake pair data effect pair perhaps somewhat exaggerated check pair look correlation plot pair data show p-value near output abridge question data pair consider wilcoxon signed-rank test instead wilcoxon rank-sum test question pleaseprovide detail data collect purpose study perhaps one u offer comment advice",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 5,
        "Topics": {
            "39": 0.1231435,
            "43": 0.30623403,
            "44": 0.17999868
        }
    },
    {
        "Id": 89676,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 89964.0,
        "CreationDate": "12/03/2014",
        "OwnerUserId": 40128.0,
        "Score": 3,
        "ViewCount": 1944.0,
        "Title": "k-medians formula compute median",
        "Body": "run k-medians distance metric l norm derive center centroid median data point assign second compute geometric median third implementation k-medians algorithm",
        "Tags": [
            "clustering",
            "k-means",
            "median"
        ],
        "AnswerCount": 2.0,
        "CommentCount": 4,
        "Topics": {
            "20": 0.14892218,
            "38": 0.23066531,
            "43": 0.14185876,
            "49": 0.15617424
        }
    },
    {
        "Id": 383352,
        "PostTypeId": 2,
        "ParentId": 383317.0,
        "AcceptedAnswerId": "",
        "CreationDate": "17/12/2018",
        "OwnerUserId": 178898.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "sadly textbook leave topic already see difficult use linear regression model non-stationary time series look economic data happens often related happen t\u2013 true disturbance term regression serial correlation parameter beta might consistent standard error reliable can\u00b4t trust test include t-test solution would test stationarity occurs could differentiate data could solve problem aware fact problem omit variable persists think gdp think many variable could influence could distort estimate",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "11": 0.10402147,
            "24": 0.13274652,
            "43": 0.2851765
        }
    },
    {
        "Id": 81615,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 81627.0,
        "CreationDate": "08/01/2014",
        "OwnerUserId": 28053.0,
        "Score": 2,
        "ViewCount": 1567.0,
        "Title": "cfa fit measure threshold new scale",
        "Body": "well establish questionnaire measure interpersonal relation modify measure interorganizational relation parameter relation determine item item remove best possible per scale achieve result scale item scale item confirmatory factor analysis apply calculate estimate item verify internal structure questionnaire expect low cfa fit well since kind new scale ok use prediction model report fit index bad edit real question probably whether continue estimate base bad fit rather whether scale new edit barrett recommends rely approximate fit index criterion use fit actually abstract concept majority sem model clearly predictive accuracy fact whether model approximately fit rmsea literally meaningless scientific statement bibliography barrett p structural equation model adjudge model fit personality individual difference pdf",
        "Tags": [
            "scales",
            "confirmatory-factor",
            "validity"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "1": 0.11954846,
            "31": 0.17969921,
            "43": 0.42721087
        }
    },
    {
        "Id": 81586,
        "PostTypeId": 2,
        "ParentId": 81497.0,
        "AcceptedAnswerId": "",
        "CreationDate": "08/01/2014",
        "OwnerUserId": 9731.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "kolmogorov-smirnov test non-parametric test tell sample come distribution k particular require equally number sample statistic convert p-value",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "14": 0.15483855,
            "43": 0.2901228,
            "44": 0.14343795
        }
    },
    {
        "Id": 220505,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 220542.0,
        "CreationDate": "24/06/2016",
        "OwnerUserId": 40447.0,
        "Score": 1,
        "ViewCount": 369.0,
        "Title": "multicollinearity highly safe t-statistics vif",
        "Body": "coefficient logsitic model really perfect t-statistics show sufficiently high significance two coefficient high vif like sample size independent variable ignore multicollinearity give way propose use result logistic regression model independent variable independent variable dependent variable sample size logistic regression produce prediction base change value independent variable let say prediction increase independent variable two unit independent variable increase one unit prediction increase independent variable unit independent variable increase one unit make total different value need use purpose independent variable produce different predict value accordingly see two-unit increase different independent variable time make total different value independent variable different predict value base change independent variable separately course intercept coefficient highly significant levelor less main objective use numerical value different predict value input separate function produce numerical output utility value utility value one need want show final utility value different model differs produce logistic regression model function change different emphasis independent-variable increase two unit",
        "Tags": [
            "regression",
            "logistic",
            "regression-coefficients",
            "multicollinearity",
            "vif"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 20,
        "Topics": {
            "11": 0.2551679,
            "43": 0.366831
        }
    },
    {
        "Id": 183065,
        "PostTypeId": 2,
        "ParentId": 183058.0,
        "AcceptedAnswerId": "",
        "CreationDate": "23/11/2015",
        "OwnerUserId": 67822.0,
        "Score": 7,
        "ViewCount": "",
        "Title": "",
        "Body": "think range intuitively equate variance think mathematical formulation concept see lead mistaken conclusion zero point data identical mean zero consequently one square distance zero bring calculation variance word data spread either side mean one data point lie precisely mean",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "13": 0.109983124,
            "25": 0.2891397,
            "43": 0.22299773,
            "47": 0.11966933
        }
    },
    {
        "Id": 184173,
        "PostTypeId": 2,
        "ParentId": 184008.0,
        "AcceptedAnswerId": "",
        "CreationDate": "29/11/2015",
        "OwnerUserId": 6768.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "think mix two unrelated concept instrumental variable sl variable multi-equation model make possible estimate model presence simultaneity assignment term use mean variable control affect dependent variable",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "11": 0.1395923,
            "37": 0.110263675,
            "43": 0.3499615
        }
    },
    {
        "Id": 416031,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "04/07/2019",
        "OwnerUserId": 231914.0,
        "Score": 1,
        "ViewCount": 21.0,
        "Title": "identify difficult parameter measure",
        "Body": "use different tool measure x object tool test small set object true x know predict value found accurate object none three tool accurate accurately measure object others problem want know object easy accurately measure x one difficult larget set test set \u2019 know true x object best way proceed table contains measure x object set tool think rank variance across different tool object tool agree may indicate difficulty capture x object",
        "Tags": [
            "ranking",
            "measurement-error",
            "measurement",
            "ranks"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 4,
        "Topics": {
            "27": 0.17723662,
            "43": 0.30417064
        }
    },
    {
        "Id": 87593,
        "PostTypeId": 2,
        "ParentId": 87572.0,
        "AcceptedAnswerId": "",
        "CreationDate": "23/02/2014",
        "OwnerUserId": 6633.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "number problem small enough solve problem simply listing possible case counting something start offfor part b call novel n n n n n make list possiblepairs novel start n n n n end n n list less pair go back correct add p p p pair novel",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "13": 0.14087325,
            "43": 0.2727421
        }
    },
    {
        "Id": 237358,
        "PostTypeId": 2,
        "ParentId": 237119.0,
        "AcceptedAnswerId": "",
        "CreationDate": "28/09/2016",
        "OwnerUserId": 113959.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "possible answer fact wiki article already sum pretty well general need partial derivative multidimensional case jacobian matrix know function rather straightforward exercise calculus case linear function derivative become trivial general case arbitrary function also chose several option article list five variant practical knowledge extends two accuracy feasibility depend type function kind approximation need series evolution taylor expansion stop choice order introduces shift expectation value propagate error w r true error infinite series becomes bias give well behave function might negligible monte-carlo simulation benefit might even know actual analytical exist form function becomes branch research depend function keywords efficiency accuracy calculation method specialized mostly problem well approximate two concern original question disentangle uncertainty simply write full taylor expansion term first order sufficient case look wiki page",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 3,
        "Topics": {
            "20": 0.19694479,
            "43": 0.39624846
        }
    },
    {
        "Id": 130995,
        "PostTypeId": 2,
        "ParentId": 130992.0,
        "AcceptedAnswerId": "",
        "CreationDate": "02/01/2015",
        "OwnerUserId": 23801.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "absolutely good practice true relationship response explanatory variable logarithmic model ought reflect course never know sure nature true relationship thing like plot data fit model different way without log transformation ass fit",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 4,
        "Topics": {
            "43": 0.4863812,
            "46": 0.15240127
        }
    },
    {
        "Id": 139964,
        "PostTypeId": 2,
        "ParentId": 139959.0,
        "AcceptedAnswerId": "",
        "CreationDate": "02/03/2015",
        "OwnerUserId": 29783.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "method anything clever term structure algorithm quicker matlab follow notice begin equation hat x sigma sigma sigma end equation even well see alexey answer begin equation hat x left frac sigma sigma right end equation compare matlab initial naive implementation use code get answer order get really significant speedup guess would start take advantage structure matrix possible",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "3": 0.10155229,
            "15": 0.13578236,
            "17": 0.14107865,
            "43": 0.28097928
        }
    },
    {
        "Id": 96641,
        "PostTypeId": 2,
        "ParentId": 96607.0,
        "AcceptedAnswerId": "",
        "CreationDate": "06/05/2014",
        "OwnerUserId": 14188.0,
        "Score": 3,
        "ViewCount": "",
        "Title": "",
        "Body": "impossible say sure guess simply pairwise t-tests look average response group interested t-tests option data would two-way anova unbalanced doubt smoking exposure group factor evidence abstract researcher attempt thing advantage model would degree freedom error well estimate error assume constant variance experience lot researcher go pair-wise t-tests",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "29": 0.12412412,
            "43": 0.2908849,
            "44": 0.12530538
        }
    },
    {
        "Id": 402330,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "10/04/2019",
        "OwnerUserId": 188032.0,
        "Score": 0,
        "ViewCount": 57.0,
        "Title": "proof one distribution glm general linear model",
        "Body": "give random variable proof distribution glm general linear model first guess proof distribution ef exponential family becuse ef know glm cassela statistical inference quote say family pdfs pmfs call exponential family express try apply definition conditional distribution step take replace step take exponential function get stuck usually would take square try find term e f sure go right path proof distribution glm",
        "Tags": [
            "normal-distribution",
            "generalized-linear-model",
            "exponential-family"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 4,
        "Topics": {
            "1": 0.12668571,
            "20": 0.29661834,
            "43": 0.14793636,
            "45": 0.22554372
        }
    },
    {
        "Id": 381923,
        "PostTypeId": 2,
        "ParentId": 381917.0,
        "AcceptedAnswerId": "",
        "CreationDate": "13/12/2018",
        "OwnerUserId": 119015.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "ideally want bit image order train model effectively manage train couple resnet- model around mr image scratch without much problem however try solve relatively easy task want train model generic task might encounter issue hand strict format mri augment image large degree",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "15": 0.13452025,
            "43": 0.38023105
        }
    },
    {
        "Id": 228898,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "09/08/2016",
        "OwnerUserId": 127060.0,
        "Score": 7,
        "ViewCount": 95.0,
        "Title": "professor samuel wilks daniel wilks related",
        "Body": "statistic professor related",
        "Tags": [
            "history"
        ],
        "AnswerCount": 2.0,
        "CommentCount": 2,
        "Topics": {
            "29": 0.19127555,
            "43": 0.28915733,
            "47": 0.3480694
        }
    },
    {
        "Id": 72495,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "10/10/2013",
        "OwnerUserId": 31358.0,
        "Score": 1,
        "ViewCount": 124.0,
        "Title": "separate regression effect v treatment effect without control group",
        "Body": "look dataset pre-post test measurement user stress depression anxiety level collect website online health assessment average healthier participant baseline get bad time sicker participant baseline get much well middle group get little well definitely regression effect go also treatment effect data collect base website usage really control group post measurement come people use online program probably way could synthesize control group use people guess make much use treatment base number logins length time logins way separate treatment effect regression effect use difference-in-difference technique use control group anything like thanks",
        "Tags": [
            "regression"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 3,
        "Topics": {
            "21": 0.11277069,
            "30": 0.19020787,
            "43": 0.31616563
        }
    },
    {
        "Id": 271103,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "31/03/2017",
        "OwnerUserId": 8013.0,
        "Score": 8,
        "ViewCount": 682.0,
        "Title": "non-parametric structural equation model",
        "Body": "reading work judea pearl excellent paper suggests non-parametric sem structural equation model way estimate association dag write suggests agree upon method exists may exist time way fit model irrelevant take non-parametric bit differentiate approach plain vanilla sem ala muthen-muthen m-plus r lavaan package parametric sense estimate come maximize joint normal likelihood presume implement method however relevant like know exactly less model complex high dimensional structural equation part barrier understand understand computationally sems fit except despite common misinterpretation sequence regression model know time non-parametric simply matter interpretation instance linear regression see non-parametric simply summarizes first-order trend summary intrinsic bivariate relationship two variable distribution possibly curvilinear trend hand interest lie determine non-linear relationship two variable penalize spline provide excellent non-linear smooth however sem focus mean difference look covariance feature acceptable method constitutes non-linear sem matter interpretation need use non-linear model spline penalty robust covariance",
        "Tags": [
            "nonparametric",
            "sem"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 1,
        "Topics": {
            "1": 0.13632542,
            "43": 0.313079
        }
    },
    {
        "Id": 423204,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "22/08/2019",
        "OwnerUserId": 257024.0,
        "Score": 0,
        "ViewCount": 13.0,
        "Title": "linear mixed model r repeat measure non-randomized group baseline difference",
        "Body": "question analyze result study conduct study occur remote island small unique population child order minimally invasive group determine randomly instead assign school school island school control group intervention school intervention group take baseline measure week post-intervention measure week ran repeat measure anova outcome group time two factor believe insufficient participant randomize clearly different baseline measurement slope time believe best way account would run linear mixed model however trouble r time run model get follow error error number observation number random effect term timepoint id random-effects parameter residual variance scale parameter probably unidentifiable id refers individual subject id however run correct code try run assume time group fix effect need also account individual subject id difference intercept slope sure correctly integrate model avoid message example code correct model would much appreciate",
        "Tags": [
            "mixed-model",
            "repeated-measures"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 3,
        "Topics": {
            "21": 0.19051272,
            "33": 0.27928638,
            "43": 0.16199619
        }
    },
    {
        "Id": 83513,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "27/01/2014",
        "OwnerUserId": 38049.0,
        "Score": 0,
        "ViewCount": 23.0,
        "Title": "need build model see submersion time",
        "Body": "need help since long time use r one year every day twice day every hour tide would like model order check submersion time area lagoon could please tell thank",
        "Tags": [
            "modeling"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 4,
        "Topics": {
            "28": 0.2877687,
            "43": 0.40784436
        }
    },
    {
        "Id": 322542,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "11/01/2018",
        "OwnerUserId": 191128.0,
        "Score": 4,
        "ViewCount": 1792.0,
        "Title": "influence plot potential outlier detection logistic regression r",
        "Body": "look identify extreme value contribution binary outcome model unbalanced set extreme value part small set predict e predit really remove ideally use cook distance residual influence leverage plot linear model would look make sense logistic regression form guess get residual seem able chi square influence plot also proportional influence plot like figure look like cross show also find similar diagnostic analysis sa anyone know r way would allow equivalent analysis linear regression logistic regression would calculate leverage cook distance make sense thanks add-on great answer gung confirm lm analysis really apply glm turn research topic include also case multiple influence researchgate",
        "Tags": [
            "r",
            "logistic",
            "outliers",
            "diagnostic"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 3,
        "Topics": {
            "5": 0.10878138,
            "11": 0.11362379,
            "14": 0.14435032,
            "43": 0.33369192
        }
    },
    {
        "Id": 259985,
        "PostTypeId": 2,
        "ParentId": 19048.0,
        "AcceptedAnswerId": "",
        "CreationDate": "04/02/2017",
        "OwnerUserId": 147928.0,
        "Score": 14,
        "ViewCount": "",
        "Title": "",
        "Body": "typical machine learn task visualize follow nest loop typically outer loop perform human validation set inner loop machine training set need rd test set ass final performance model word validation set training set human",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "10": 0.5224384,
            "43": 0.25430146
        }
    },
    {
        "Id": 130784,
        "PostTypeId": 2,
        "ParentId": 105475.0,
        "AcceptedAnswerId": "",
        "CreationDate": "31/12/2014",
        "OwnerUserId": 49081.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "model approximate know r code would require initialize e loop",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "3": 0.1655395,
            "24": 0.15161832,
            "43": 0.34966487
        }
    },
    {
        "Id": 30421,
        "PostTypeId": 2,
        "ParentId": 20341.0,
        "AcceptedAnswerId": "",
        "CreationDate": "13/06/2012",
        "OwnerUserId": 8374.0,
        "Score": 4,
        "ViewCount": "",
        "Title": "",
        "Body": "f-score ratio two variable f f f f variability group f variability within group word high f value lead significant p-value depend alpha mean least one group significantly different rest tell group typically select feature return high f-values use analysis",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "0": 0.21616311,
            "43": 0.3136623
        }
    },
    {
        "Id": 88170,
        "PostTypeId": 2,
        "ParentId": 88166.0,
        "AcceptedAnswerId": "",
        "CreationDate": "27/02/2014",
        "OwnerUserId": 4505.0,
        "Score": 9,
        "ViewCount": "",
        "Title": "",
        "Body": "see follow paper null hypothesis true p-values follow uniform something approach uniform finite number possible test statistic distribution would expect see p-values place null false distribution p-values heavily weight towards hopefully would consistent result guess test null true case false enough power little variation also see function teachingdemos package r quick way simulate help see go",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 3,
        "Topics": {
            "43": 0.38483557,
            "44": 0.30471778
        }
    },
    {
        "Id": 89060,
        "PostTypeId": 2,
        "ParentId": 89059.0,
        "AcceptedAnswerId": "",
        "CreationDate": "06/03/2014",
        "OwnerUserId": 8580.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "know one import textual dataset r trivial understand corpus layout bias textual data set use biomedical informatics dissertation might good place start answer question provide additional detail need",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "3": 0.17357838,
            "42": 0.1612243,
            "43": 0.30433318
        }
    },
    {
        "Id": 101008,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "03/06/2014",
        "OwnerUserId": 46051.0,
        "Score": 1,
        "ViewCount": 32.0,
        "Title": "pmf function random variable",
        "Body": "independent sequence random variable p p k n n define new sequence random variable otherwise find p far proven tail equivalent prove lyapunov condition sum able find e stuck",
        "Tags": [
            "probability"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 4,
        "Topics": {
            "19": 0.18920802,
            "20": 0.31941783,
            "43": 0.11268434,
            "45": 0.13618447
        }
    },
    {
        "Id": 238684,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "05/10/2016",
        "OwnerUserId": 41749.0,
        "Score": 9,
        "ViewCount": 9426.0,
        "Title": "difference dice jaccard overlap coefficient",
        "Body": "come across three different statistical measure compare two set particular segmentation image e g compare similarity ground truth segment result difference measurement quite similar mathematically dice jaccard overlap see paper use dice often others also suggest use jaccard overlap coefficient difference",
        "Tags": [
            "machine-learning",
            "similarities",
            "dice",
            "segmentation",
            "jaccard-similarity"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 2,
        "Topics": {
            "38": 0.17012605,
            "39": 0.21576786,
            "43": 0.16778915,
            "47": 0.11353269
        }
    },
    {
        "Id": 123631,
        "PostTypeId": 2,
        "ParentId": 90080.0,
        "AcceptedAnswerId": "",
        "CreationDate": "11/11/2014",
        "OwnerUserId": 52092.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "recommend proposal expect value data point variance take give new mean variance poisson model hence proposal equivalent assume fix dispersion quasi-poisson model since unless good prior information would hard defend compare typical approach quasi -poisson model estimate mean square pearson residual example r another recommend approach would use robust sandwich estimate case comment ugly residual suggests might model mis-specification mean function specification mean function determines residual look like assumption mean-variance relationship apart default non-intrinsic use link function poisson model buy change efficiency estimator validity estimate standard error fix mean-model mis-specification could include add additional covariates get others measure include interaction spline polynomial covariates use different link function transform eg box-cox transforms zero-inflation look zero-inflated zero-altered poisson negative binomial distribution available r anyways difference two zero-inflated assumes failure mixture normal rate zero underlie distribution plus additional special process generates zero zero-altered distribution assume two-part sequential model initial hurdle need clear get positive count",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "43": 0.3551789
        }
    },
    {
        "Id": 226565,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "31/07/2016",
        "OwnerUserId": 30351.0,
        "Score": 29,
        "ViewCount": 11745.0,
        "Title": "bootstrap prediction interval",
        "Body": "bootstrap technique available compute prediction interval point prediction obtain e g linear regression regression method k-nearest neighbour regression tree etc somehow feel sometimes propose way bootsrap point prediction see e g prediction interval knn regression provide prediction interval confidence interval example r obviously basic bootstrap interval match confidence interval prediction interval question properly",
        "Tags": [
            "bootstrap",
            "prediction-interval"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 4,
        "Topics": {
            "5": 0.10424336,
            "16": 0.19229464,
            "19": 0.2375224,
            "43": 0.23515981
        }
    },
    {
        "Id": 305985,
        "PostTypeId": 2,
        "ParentId": 305946.0,
        "AcceptedAnswerId": "",
        "CreationDate": "02/10/2017",
        "OwnerUserId": 2669.0,
        "Score": 3,
        "ViewCount": "",
        "Title": "",
        "Body": "course plot imply somewhat linear data question one degree rather answer whether linear fit ok whether get away suppose describe consequence use linear fit oppose quadratic one r-squared compare build square term linear residual much highly associate fit value leverage value data conform less well theoretical line q-q plot exclude term whose predictive power unquestionably statistically significant p trillion stand outside situation quadratic fit seem preferable every way yet clearly reason favor linear solution say ultimate criterion superior quadratic fit would sway direction transformation take square root case number substitute place original case number linear model yield r-squared",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 3,
        "Topics": {
            "1": 0.14837131,
            "5": 0.100758515,
            "14": 0.11029673,
            "43": 0.3534111
        }
    },
    {
        "Id": 447013,
        "PostTypeId": 2,
        "ParentId": 443154.0,
        "AcceptedAnswerId": "",
        "CreationDate": "29/01/2020",
        "OwnerUserId": 179143.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "believe manage model stan code use also experiment estimate distribution within group replace parameter appropriate model seem give reasonable result although additional constraint seem overwhelm ability estimate p interest",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "43": 0.385091
        }
    },
    {
        "Id": 104635,
        "PostTypeId": 2,
        "ParentId": 104624.0,
        "AcceptedAnswerId": "",
        "CreationDate": "25/06/2014",
        "OwnerUserId": 2126.0,
        "Score": 25,
        "ViewCount": "",
        "Title": "",
        "Body": "look like author make mathematical error somewhere expand sum-of-squares deviation get reduces author expression except constant term matter anyway need try minimize set derivative respect zero solve system solve r say indeed base link seem coursera course maybe mis-transcription data somewhere independent way calculation know estimate regression slope equal sum cross product divide sum square think shoe size instead slope would come",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "3": 0.1328792,
            "20": 0.14964196,
            "43": 0.3609573
        }
    },
    {
        "Id": 133363,
        "PostTypeId": 2,
        "ParentId": 133362.0,
        "AcceptedAnswerId": "",
        "CreationDate": "14/01/2015",
        "OwnerUserId": 53690.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "respective contribution explain variance different explanatory variable unchanged use robust standard error yes unchanged point estimate regression coefficient remain note geometric picture ols projection still thus also thus assume contribution explain variance become small know quantify change contribution become small however statistical significance decrease mean less sure effect due fundamental reason rather pure chance magnitude effect still",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "5": 0.13432479,
            "25": 0.10470657,
            "26": 0.17585516,
            "43": 0.32403877
        }
    },
    {
        "Id": 231302,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "23/08/2016",
        "OwnerUserId": 128682.0,
        "Score": 1,
        "ViewCount": 81.0,
        "Title": "proper p value cutoff point multiple comparison case",
        "Body": "condition control stim- stim- stim- different protein expression level condition sample number protein expression level condition individual would like set significant level order control overall significance level consider nine simultaneous test protein expression test significant level modify pair-wise comparison adjustment divide pair number con-stim con-stim con-stim stim -stim stim -stim stim -stim right comment would appreciate",
        "Tags": [
            "p-value",
            "multiple-comparisons",
            "kruskal-wallis",
            "adjustment"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 3,
        "Topics": {
            "21": 0.22110228,
            "27": 0.14582631,
            "43": 0.15944959,
            "44": 0.20645739
        }
    },
    {
        "Id": 423519,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 423532.0,
        "CreationDate": "24/08/2019",
        "OwnerUserId": 120566.0,
        "Score": 1,
        "ViewCount": 22.0,
        "Title": "re-train final model use oversampling",
        "Body": "bit puzzle process experiment model oversampling translate final version model use oversample data training dataset tune parameter everything else next properly take data oversample re-train model take data retrain model without oversampling take model train oversampled training data final model correct way thing thanks",
        "Tags": [
            "machine-learning",
            "oversampling"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "10": 0.3322606,
            "14": 0.10857918,
            "43": 0.4329596
        }
    },
    {
        "Id": 359339,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 359423.0,
        "CreationDate": "27/07/2018",
        "OwnerUserId": 214965.0,
        "Score": 0,
        "ViewCount": 35.0,
        "Title": "take log supress hetroskedasticity work dependent variable",
        "Body": "told log variable regression hetroskedasticity error reduce case also dependent variable log",
        "Tags": [
            "regression",
            "heteroscedasticity",
            "logarithm",
            "dependent-variable"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 1,
        "Topics": {
            "43": 0.41191402,
            "46": 0.2038713
        }
    },
    {
        "Id": 340394,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 340637.0,
        "CreationDate": "13/04/2018",
        "OwnerUserId": 183208.0,
        "Score": 1,
        "ViewCount": 143.0,
        "Title": "multiple linear regression skewed likert data justified",
        "Body": "try run regression likert data dependent variable customer satisfaction independent variable rating various attribute price ease use etc data come survey problem everyone either satisfied satisfied make data extremely left-skewed also less behave like result regression seem sensible keep wonder justified run data like know linear regression require residual normally distribute thought skewness mess ols function perhaps automatically take care think make sense apply transformation ordinal scale like feel uneasy result wonder could improve like stick linear model possible familiar look suggestion reference welcome linear model really salvageable guess try kind ordinal regression would grateful advice best way go edit add mostly interested qualitative result e attribute significant roughly rank",
        "Tags": [
            "regression",
            "data-transformation",
            "likert"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 5,
        "Topics": {
            "43": 0.5464139
        }
    },
    {
        "Id": 413969,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 413980.0,
        "CreationDate": "20/06/2019",
        "OwnerUserId": 61389.0,
        "Score": 2,
        "ViewCount": 35.0,
        "Title": "high-accuracy rng",
        "Body": "need simulate sample size x white noise normal null mean unitary variance actually use mersenne-twister discuss professor seem stable size need since go perform via program language suggestion rng also found thanks help update stable mean pretty different value run rnorm give pretty independent element since covariance equal identity matrix would need sample result pretty close identity matrix even round way discard sample close identity matrix condition use",
        "Tags": [
            "random-generation"
        ],
        "AnswerCount": 2.0,
        "CommentCount": 6,
        "Topics": {
            "43": 0.39573509,
            "45": 0.12495
        }
    },
    {
        "Id": 45364,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 45367.0,
        "CreationDate": "07/12/2012",
        "OwnerUserId": 1566.0,
        "Score": 0,
        "ViewCount": 83.0,
        "Title": "best fit k k k w -w i- k c k w i- k c w know",
        "Body": "end day record weight w number calorie consume day c believe weight change w -w i- proportional net calorie intake w -w i- k net calorie intake number calorie burn linear function prior day weight k w i- k net calorie intake thus c k w i- k weight change thus w -w i- k c k w i- k give know w c find best fit value k k k note try solve data-fitting problem realize map reality perfectly",
        "Tags": [
            "curve-fitting"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "19": 0.1620085,
            "20": 0.12262803,
            "43": 0.20387392
        }
    },
    {
        "Id": 155602,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "05/06/2015",
        "OwnerUserId": 38348.0,
        "Score": 3,
        "ViewCount": 2559.0,
        "Title": "calculate joint density function brownian motion",
        "Body": "read book today regard calculation joint density function brownian motion process go follow define brownian motion process mean variance obtain joint density function note set equality equivalent joint density confuse allow directly input second set equality density function yield different result indirectly calculate would anyone able help thank",
        "Tags": [
            "probability",
            "markov-process",
            "brownian"
        ],
        "AnswerCount": 2.0,
        "CommentCount": 4,
        "Topics": {
            "19": 0.24024759,
            "20": 0.23465604,
            "43": 0.12763084,
            "45": 0.15579516
        }
    },
    {
        "Id": 107925,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 107946.0,
        "CreationDate": "14/07/2014",
        "OwnerUserId": 52063.0,
        "Score": 3,
        "ViewCount": 249.0,
        "Title": "test aggregation binary event success binomial glm",
        "Body": "vex seem solidify answer beyond vague thought poisson distribution think simple problem miss something obvious thought appreciate particularly r snippet chris experiment replicate assay tissue sample call binary event without use tissue origin call outcome success per bernoulli trial convention score tissue concordance replicates successful one otherwise assay work expect non-random number concordance tissue e effect replicate give total number success test distribution tissue perhaps proportion tissue deviate expectation thanks",
        "Tags": [
            "poisson-distribution",
            "binomial",
            "binary-data"
        ],
        "AnswerCount": 3.0,
        "CommentCount": 0,
        "Topics": {
            "43": 0.31404555
        }
    },
    {
        "Id": 379782,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 379816.0,
        "CreationDate": "01/12/2018",
        "OwnerUserId": 228455.0,
        "Score": 0,
        "ViewCount": 30.0,
        "Title": "multiple regression interpret plot model",
        "Body": "multiple regression analysis observation without prior hypothesis aic bic variable selection generate ideal model difficulty interpret thanks much time help",
        "Tags": [
            "multiple-regression",
            "residuals",
            "model-interpretation"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "1": 0.20804918,
            "11": 0.1522831,
            "43": 0.32623452
        }
    },
    {
        "Id": 393928,
        "PostTypeId": 2,
        "ParentId": 393875.0,
        "AcceptedAnswerId": "",
        "CreationDate": "22/02/2019",
        "OwnerUserId": 97925.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "dimension section collins et al write first concentrate simplest case single component make vector regular -norm vector matter case experience text paper refer norm matrix often operator norm",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "17": 0.27442127,
            "29": 0.17703313,
            "35": 0.11235625,
            "43": 0.1321873
        }
    },
    {
        "Id": 378615,
        "PostTypeId": 2,
        "ParentId": 378609.0,
        "AcceptedAnswerId": "",
        "CreationDate": "25/11/2018",
        "OwnerUserId": 158565.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "fix let number appear among play number appear among play number appear among play random vector follow multinomial distribution parameter mean vector varaince covariance matrix money game random follow distribution",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 8,
        "Topics": {
            "13": 0.3265535,
            "20": 0.13745725,
            "43": 0.24799512
        }
    },
    {
        "Id": 265655,
        "PostTypeId": 2,
        "ParentId": 265650.0,
        "AcceptedAnswerId": "",
        "CreationDate": "06/03/2017",
        "OwnerUserId": 124637.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "reckon do homework carefully criterion component extraction well write many paper popular rule thumb select eigenvalue large besides also criterion base boostrapping cross-validation read detail wold esbensen geladi cite paper reference wold esbensen k geladi p principal component analysis chemometrics intelligent laboratory system \u2013 doi",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "29": 0.15960482,
            "35": 0.26343176,
            "43": 0.23836417
        }
    },
    {
        "Id": 124945,
        "PostTypeId": 2,
        "ParentId": 124496.0,
        "AcceptedAnswerId": "",
        "CreationDate": "21/11/2014",
        "OwnerUserId": 28188.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "receiver operating characteristic curve associate area curve measure work binary classifier complexity multiple value label possible see wikipedia link believe r package support could take advantage rapidminer r extension get access",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "0": 0.20988572,
            "3": 0.144176,
            "43": 0.1528026,
            "48": 0.28849187
        }
    },
    {
        "Id": 164723,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "05/08/2015",
        "OwnerUserId": 84146.0,
        "Score": 1,
        "ViewCount": 1529.0,
        "Title": "fisher exact test bonferroni correction",
        "Body": "compare survivability live die use tibial intraosseous io humerus io tibial io intravenous epinephrine patient cardiac arrest control group receive drug need bonferroni correction",
        "Tags": [
            "statistical-significance",
            "multiple-comparisons",
            "fishers-exact",
            "bonferroni"
        ],
        "AnswerCount": 2.0,
        "CommentCount": 2,
        "Topics": {
            "43": 0.36874855,
            "44": 0.29553604
        }
    },
    {
        "Id": 384275,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 384276.0,
        "CreationDate": "23/12/2018",
        "OwnerUserId": 193492.0,
        "Score": 1,
        "ViewCount": 60.0,
        "Title": "use non-linear ordinal variable dependent variable",
        "Body": "use world value survey answer independent variable follow dailyweeklymonthlynever answer dependent variable alwaysusuallynever see category ordinal variable equally space would possible use variable regression analysis",
        "Tags": [
            "regression",
            "stata",
            "ordinal-data",
            "continuous-data"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "11": 0.31220615,
            "28": 0.11271447,
            "43": 0.3959245
        }
    },
    {
        "Id": 258650,
        "PostTypeId": 2,
        "ParentId": 185346.0,
        "AcceptedAnswerId": "",
        "CreationDate": "28/01/2017",
        "OwnerUserId": 17250.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "probably ought plot time series line chart rather bar column chart thought best aspect ratio chart one bank \u00b0 average slope line segment connect adjacent point \u00b0 second chart first third bit slope rest rule thumb would tell make second chart bit taller reference hand could search",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "36": 0.33510554,
            "43": 0.383304
        }
    },
    {
        "Id": 92801,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 92865.0,
        "CreationDate": "07/04/2014",
        "OwnerUserId": 35186.0,
        "Score": 2,
        "ViewCount": 221.0,
        "Title": "cascade combination kernel function",
        "Body": "question regard machine learn specifically kernel function suppose kernel function say also another distinct one say want know kernel function well one feed output kernel function another kernel mean make sense another question expect behavior linear combination well-known kernel rbf polynomial mlp suppose mlp kernel yield accuracy classification task rbf yield necessarily rbf mlp yield well accuracy compare one result mlp",
        "Tags": [
            "machine-learning",
            "neural-networks",
            "pattern-recognition",
            "kernel-trick"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "15": 0.18203306,
            "43": 0.35773993
        }
    },
    {
        "Id": 90541,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 90542.0,
        "CreationDate": "19/03/2014",
        "OwnerUserId": 9162.0,
        "Score": 1,
        "ViewCount": 56.0,
        "Title": "context factor analysis term factor latent variable synonymous",
        "Body": "realise term factor additional application area e g anova seem factor analysis two term use synonymously",
        "Tags": [
            "factor-analysis"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "31": 0.18311167,
            "43": 0.5525228
        }
    },
    {
        "Id": 156636,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "12/06/2015",
        "OwnerUserId": 45188.0,
        "Score": 1,
        "ViewCount": 54.0,
        "Title": "terminology mean introduction likelihood ratio",
        "Body": "currently reading likelihood yudi pawitan edition make way second chapter likelihood function part likelihood function first introduce read compare likelihood different value parameter say verse suppose one-to-one transformation observe data continuous little confuse look like something along line chain rule fill detail also see would want take absolute value partial derivative point anyone help realise go thanks",
        "Tags": [
            "likelihood",
            "likelihood-ratio"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 1,
        "Topics": {
            "41": 0.14693758,
            "43": 0.39801154
        }
    },
    {
        "Id": 337153,
        "PostTypeId": 2,
        "ParentId": 336940.0,
        "AcceptedAnswerId": "",
        "CreationDate": "27/03/2018",
        "OwnerUserId": 86794.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "would stick mixed design repeat measure take mean median sample variable since time appear irrelevant",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "21": 0.20183438,
            "39": 0.1113298,
            "43": 0.35031843
        }
    },
    {
        "Id": 83244,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 83253.0,
        "CreationDate": "24/01/2014",
        "OwnerUserId": 30503.0,
        "Score": 2,
        "ViewCount": 242.0,
        "Title": "point estimate posterior distribution use gaussian process",
        "Body": "classic regression method ridge regression lasso predict posterior mean gaussian process give full posterior distribution would useful posterior distribution low variance region posterior mean actually close true value high variance region happen useful estimate posterior distribution",
        "Tags": [
            "regression",
            "bayesian",
            "gaussian-process",
            "posterior"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 1,
        "Topics": {
            "41": 0.32153663,
            "43": 0.21800984
        }
    },
    {
        "Id": 448424,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "07/02/2020",
        "OwnerUserId": 29107.0,
        "Score": 1,
        "ViewCount": 11.0,
        "Title": "okcupid formula way estimate similarity limited data",
        "Body": "say test receives string return either belongs belong set know true answer sample belongs set want evaluate quality test specifically want evaluate precision answer belongs percentage time test correct however test sample say string test return belongs could say precision frequency correct answer would prefer formula could also take account size sample mention okcupid remember read ago use apply formula decrease match percentage people insufficient sample create good estimate mean something similar prefer consider rule sample bad rather good technique",
        "Tags": [
            "machine-learning",
            "similarities"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 0,
        "Topics": {
            "43": 0.38366193
        }
    },
    {
        "Id": 266371,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 266422.0,
        "CreationDate": "09/03/2017",
        "OwnerUserId": 16542.0,
        "Score": 1,
        "ViewCount": 48.0,
        "Title": "calculate deltaaic",
        "Body": "advise colleague perform backwards elimination model selection use aic criterion remove term individually start complex interaction work main effect compare aic reduce model compare full model original model contain term example main effect interaction model could write full model remove three way interaction make model find remove term reduces aic remove three way interaction reduce remove first two way interaction follow advice give colleague would compare aic reject aic reduces however strike three way interaction miss extremely uninformative aic remove interaction unless highly informative overcomes effect remove three way interaction brings question backwards elimination one compare aic reduce model full model e g b model upon reduce model base e g c something else",
        "Tags": [
            "mixed-model",
            "aic"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "1": 0.31612796,
            "30": 0.11269875,
            "43": 0.46155083
        }
    },
    {
        "Id": 169831,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "02/09/2015",
        "OwnerUserId": 25759.0,
        "Score": 1,
        "ViewCount": 50.0,
        "Title": "spatial regression misalign predictor point outcome point",
        "Body": "outcome type traffic crash event measure every intersection street grid covariates measure location event predictor main interest x count measure arbitrary location grid wish measure every intersection say interested effect x need figure best way incorporate x regression model assume find x strongly spatially autocorrelated hand imagine two way one would use method like kriging guess x event location incorporate estimate somehow along error regression model however x already measure without error seem like add error would result glop even relationship perhaps implement method directly thought thanks",
        "Tags": [
            "regression",
            "spatial"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 5,
        "Topics": {
            "5": 0.101473376,
            "7": 0.109465115,
            "43": 0.369084
        }
    },
    {
        "Id": 250917,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "11/12/2016",
        "OwnerUserId": 141419.0,
        "Score": 8,
        "ViewCount": 4120.0,
        "Title": "complete statistic",
        "Body": "would like know statistic complete set depend whether previously know complete lehmann-scheff\u00e9 umvue know could consider whose variance equal cramer-rao bound strictly less could umvue",
        "Tags": [
            "normal-distribution",
            "estimation",
            "inference",
            "umvue"
        ],
        "AnswerCount": 2.0,
        "CommentCount": 4,
        "Topics": {
            "43": 0.32706994,
            "47": 0.28248793
        }
    },
    {
        "Id": 122835,
        "PostTypeId": 2,
        "ParentId": 122834.0,
        "AcceptedAnswerId": "",
        "CreationDate": "05/11/2014",
        "OwnerUserId": 60059.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "yes note take note also",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "20": 0.5478921,
            "43": 0.2781516
        }
    },
    {
        "Id": 308894,
        "PostTypeId": 2,
        "ParentId": 308633.0,
        "AcceptedAnswerId": "",
        "CreationDate": "19/10/2017",
        "OwnerUserId": 3411.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "want build regression model proper way use transfer function model process described chapter box-jenkins textbook summarize procedure transfer function model identification outline box jenkins us cross correlation two prewhitened series tentatively identify model form first step process develop arima model user-specified input time series equation series must make stationary apply appropriate differencing transformation parameter arima model stationary time series turn prewhitened prewhitening refers process apply give set autoregressive move average factor stationary series stationary output series prewhitened input series ar factor one input series stationary output series prewhitened different input prewhitening necessary remove intrarelationship individual series allows accurately ass interrelationship input output series cross correlation prewhitened input output reveal extent interrelationship cross correlation convert estimate impulse response weight regression weight pattern impulse response weight indicate suggest tentative transfer function model apply impulse response weight input series predict output series one generate preliminary estimate noise series follow rule arima model identification pattern autocorrelations partial autocorrelations tentative noise process give clue initial form noise model give identify transfer function noise model one proceed model estimation diagnostic check phase",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "24": 0.31446564,
            "43": 0.2323016
        }
    },
    {
        "Id": 123461,
        "PostTypeId": 2,
        "ParentId": 118390.0,
        "AcceptedAnswerId": "",
        "CreationDate": "10/11/2014",
        "OwnerUserId": 2074.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "look like eq confuse way write chain rule probability accord definition consider case one exponent zero whole exponent zero since probability raise zeroth power equal factor would contribute nothing product contributes equal apply argument pattern value show factor contributes product one get equation",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 3,
        "Topics": {
            "20": 0.25487286,
            "43": 0.2509975
        }
    },
    {
        "Id": 3584,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 3585.0,
        "CreationDate": "14/10/2010",
        "OwnerUserId": 588.0,
        "Score": 10,
        "ViewCount": 1007.0,
        "Title": "deal survey question multiple response",
        "Body": "dataset ask people whether certain place e g b c make one choice specimen take nose see infect disease need find relative risk get infect one go certain place think logistic regression right suggestion thanks",
        "Tags": [
            "logistic"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "11": 0.16854966,
            "43": 0.45912573
        }
    },
    {
        "Id": 12849,
        "PostTypeId": 2,
        "ParentId": 12823.0,
        "AcceptedAnswerId": "",
        "CreationDate": "09/07/2011",
        "OwnerUserId": 4797.0,
        "Score": 4,
        "ViewCount": "",
        "Title": "",
        "Body": "want test whether accuracy classifier test need estimate without know sample classifier get right wrong able estimate covariance thus statistically compare classifier",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "0": 0.15623875,
            "10": 0.2809037,
            "43": 0.22277847
        }
    },
    {
        "Id": 168111,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "20/08/2015",
        "OwnerUserId": 75283.0,
        "Score": 1,
        "ViewCount": 86.0,
        "Title": "distance hypershere",
        "Body": "want ass distance pair high-dimensional vector feature vector normalize l norm unit length point surface hypershpere know particular instance curse dimensionality say increase dimension difference minimum maximum distance tends zero euclidean distance becomes useless metric however problem also hold particular case normalize vector length see discussion note bulk volume high-dimensional space concentrate outer shell precisely situation deal euclidean distance somehow become valid alternatively would say distance two vector touch surface hypersphere simply theta pi r theta angle vector radian radius r consider circle way point b traverse surface circle however uncertain whether simple intuition carry high dimension whether unexpected behavior occurs",
        "Tags": [
            "distance",
            "high-dimensional",
            "euclidean",
            "cosine-distance"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 2,
        "Topics": {
            "35": 0.17372654,
            "38": 0.14538057,
            "43": 0.35167733
        }
    },
    {
        "Id": 238624,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "05/10/2016",
        "OwnerUserId": 127821.0,
        "Score": 0,
        "ViewCount": 331.0,
        "Title": "make sense plot impulse response function insignificant variable granger-causality test",
        "Body": "endogenous variable call w x z interested reduce form var w dependent variable run granger test found x granger-cause w certain significance level give z found granger-cause w infer anything meaningful plot irf impulse variable z response variable w",
        "Tags": [
            "var",
            "granger-causality",
            "impulse-response"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "11": 0.17010315,
            "19": 0.17451915,
            "43": 0.27582556
        }
    },
    {
        "Id": 99953,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 99957.0,
        "CreationDate": "24/05/2014",
        "OwnerUserId": 46129.0,
        "Score": 1,
        "ViewCount": 51.0,
        "Title": "test influence monthly fluctuation",
        "Body": "let say dataset look like airquality case consider number part per million something question would test factor month influence airquality much influence trouble think right method test besides monthly difference may al influence year factor ie airquality may get bad year traffic intensity increase hope point right direction test model use example",
        "Tags": [
            "time-series",
            "model",
            "seasonality"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "28": 0.111012995,
            "43": 0.3951753
        }
    },
    {
        "Id": 27329,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 27331.0,
        "CreationDate": "29/04/2012",
        "OwnerUserId": 7603.0,
        "Score": 2,
        "ViewCount": 77.0,
        "Title": "inequality involve regression function approximation",
        "Body": "proof theorem book devroye et al understand implies note regression function approximation bayes classifier finally define like replace",
        "Tags": [
            "regression",
            "classification"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 1,
        "Topics": {
            "20": 0.5429807
        }
    },
    {
        "Id": 241916,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "23/10/2016",
        "OwnerUserId": 134057.0,
        "Score": 1,
        "ViewCount": 106.0,
        "Title": "markov chain example",
        "Body": "new topic didnt understand solve kind problem center call may revert prob travel btw successive center let xnbe digit receive n th center communication system",
        "Tags": [
            "probability",
            "markov-process"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "13": 0.17123963,
            "27": 0.15869112,
            "34": 0.13650441,
            "43": 0.27790922
        }
    },
    {
        "Id": 351326,
        "PostTypeId": 2,
        "ParentId": 351325.0,
        "AcceptedAnswerId": "",
        "CreationDate": "14/06/2018",
        "OwnerUserId": 211594.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "simple linear regression define model single explanatory variable e independent variable accord answer univariate linear regression refers model single response variable e dependent variable answer corroborates theory claim say simple regression necessarily single dependent variable verify claim model one explanatory variable one response variable still call simple multivariate think see term simple univariate use interchangeably impression difference reckon best keep distinction",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "5": 0.2192951,
            "11": 0.25723985,
            "43": 0.27426744,
            "47": 0.16118288
        }
    },
    {
        "Id": 268723,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "20/03/2017",
        "OwnerUserId": 153886.0,
        "Score": 1,
        "ViewCount": 13.0,
        "Title": "logical find difference question people yes answer different time",
        "Body": "patient certain symptom yes treatment logical find difference symptom treatment example symptom treatment symptom statistically significant difference thank advance",
        "Tags": [
            "group-differences"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 0,
        "Topics": {
            "30": 0.12090373,
            "31": 0.13709374,
            "39": 0.14928988,
            "43": 0.3993944
        }
    },
    {
        "Id": 442669,
        "PostTypeId": 2,
        "ParentId": 80809.0,
        "AcceptedAnswerId": "",
        "CreationDate": "30/12/2019",
        "OwnerUserId": 246418.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "student-t distribution special case generalise hyperbolic distribution close affine transform accord wikipedia page linear transforms affine transforms hence would think linear transformation student-t random variable degree freedom student-t distribute believe understand reading reference add answer post user also quantitative risk management concept technique tool section equation also reference paper hu w kercheval n portfolio optimization student skewed return",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "20": 0.1258062,
            "43": 0.2569529,
            "45": 0.16501848
        }
    },
    {
        "Id": 122749,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "05/11/2014",
        "OwnerUserId": 46168.0,
        "Score": 2,
        "ViewCount": 1138.0,
        "Title": "fix random hausman test plm v lrt lmer best approach decide use random effect",
        "Body": "unbalanced panel data would like know method well estimation want capture heterogeneity sample use plm package lme sample year variable firm id variable bhoth use index plm model also location variable use lmer function random intercept next question use panel data technique plm decide fix random effect run ahausman test null hypothesis prefer model random effect v fix effect green hausman test r indicates random effect inconsistent use random intercept model like lmer lme make lrt see group effect random effect significant doubt correct approach thanks help sorry format mistake",
        "Tags": [
            "r",
            "lme4-nlme",
            "hausman",
            "plm"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 0,
        "Topics": {
            "33": 0.52640724,
            "43": 0.15196626
        }
    },
    {
        "Id": 416439,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "08/07/2019",
        "OwnerUserId": 18075.0,
        "Score": 0,
        "ViewCount": 30.0,
        "Title": "denote simple slope coefficient interaction",
        "Body": "use sslope stata test linear relationship x different value moderator output sslope provide significance level coefficient far know simple slope coefficient represent slope regression line specific value moderator report standardize coefficient beta b regular regression table however sure report coefficient provide sslope denote coefficient also b",
        "Tags": [
            "regression",
            "multiple-regression",
            "interaction"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 5,
        "Topics": {
            "5": 0.4000238,
            "43": 0.22727351
        }
    },
    {
        "Id": 229944,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "15/08/2016",
        "OwnerUserId": 142957.0,
        "Score": 2,
        "ViewCount": 1671.0,
        "Title": "comparison mean r",
        "Body": "follow boxplots quantitative trai medication group would like compare med group statistical test r would appropriate case med basically healthy individual med case different medication group contain independent sample thanks",
        "Tags": [
            "r",
            "mean"
        ],
        "AnswerCount": 2.0,
        "CommentCount": 7,
        "Topics": {
            "39": 0.2500454,
            "43": 0.25726935,
            "44": 0.10006908
        }
    },
    {
        "Id": 123345,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 123354.0,
        "CreationDate": "10/11/2014",
        "OwnerUserId": 45965.0,
        "Score": 4,
        "ViewCount": 343.0,
        "Title": "close form variance sum two estimate logistic regression",
        "Body": "logistic regression intercept term least one dependent variable categorical close form variance sum intercept coefficient categorical variable sample multivariate distribution mean variance intercept coefficient get reliable measure variance sum x categorical would formula variance sum two random variable applicable reason ask comment get impression close form exist variance question advice sample multivariate distribution mean variance logit true red case variance estimate exactly approximation accurate measure found sample e g simple example sample method seem provide accurate estimate variance use e sample state correspondance covariance matrix fit parameter \u03b4\u03c7 confidence region case gaussian uncertainty input measurement reason rely close form perhaps another reason advice sample instead derive variance analytically case like edit response answer give stask advice originally get simulate full model code simulate full model",
        "Tags": [
            "regression",
            "logistic",
            "variance",
            "interaction"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 1,
        "Topics": {
            "43": 0.34926376
        }
    },
    {
        "Id": 368240,
        "PostTypeId": 2,
        "ParentId": 368227.0,
        "AcceptedAnswerId": "",
        "CreationDate": "22/09/2018",
        "OwnerUserId": 116440.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "bilinear mapping mean look like vector matrix bilinear map refer produce real value think function bilinear map whatever vector space live real number mapping mean look like possible typo mapping meant mapping think matrix define mapping second equation quote text say matrix give linear combination matrix could think define mapping similar sense assume one single matrix different matrix appear summation index matrix purpose matrix form linear combination matrix could think basis element combine different strength form final matrix since coefficient function represent mlp different mlps well sound plausible hard say sure base short excerpt provide check rest paper",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "17": 0.16256891,
            "20": 0.10324843,
            "35": 0.1426242,
            "43": 0.26170084
        }
    },
    {
        "Id": 105574,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "02/07/2014",
        "OwnerUserId": 14529.0,
        "Score": 1,
        "ViewCount": 806.0,
        "Title": "perform seasonal adjustment time series",
        "Body": "assume follow data set represent month year correspond consumption natrual gas heat flat respective mean temperature seasonal adjust normalize time series would like take account colder winter warmer summer know need less energy comparison especially term compare year example know use r make easy explain",
        "Tags": [
            "time-series",
            "seasonality"
        ],
        "AnswerCount": 2.0,
        "CommentCount": 5,
        "Topics": {
            "24": 0.10702485,
            "28": 0.16647294,
            "43": 0.3468831
        }
    },
    {
        "Id": 340003,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "11/04/2018",
        "OwnerUserId": 203993.0,
        "Score": 3,
        "ViewCount": 90.0,
        "Title": "dimension cluster result",
        "Body": "phd research computational organic chemistry often generates large data set entire conformers conformer basically spatial arrangement atom space conformers generally around unique shape roughly correspond one shape order try sort conformers look possibility cluster generate variable data set measurement various angle ultimately determine shape molecule ran two step cluster analysis spss work fine sort conformers sensible cluster issue visualise cluster moment thing plot variable import spss begin depend two choose get random graph dimension angle case example example see similar work axis tend dimensionless unsure achieve even approach right way advice would appreciate",
        "Tags": [
            "clustering",
            "data-visualization",
            "spss",
            "dimensionality-reduction",
            "chemistry"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "35": 0.10650657,
            "38": 0.2846101,
            "43": 0.3009121
        }
    },
    {
        "Id": 30198,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 30203.0,
        "CreationDate": "10/06/2012",
        "OwnerUserId": 5931.0,
        "Score": 3,
        "ViewCount": 900.0,
        "Title": "generate volcano scatterplot",
        "Body": "might look silly question many please answer interested generate volcano plot dataset four column value log column name column value treat untreated state fold change intrested generate scatter plot value mark outlier fold bit confuse puzzle command r ask one parameter like p-value thing kindly help generate scatterplot dataset solution python also welcome thank time consideration",
        "Tags": [
            "r",
            "data-visualization",
            "python",
            "scatterplot"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "43": 0.41943073,
            "44": 0.15723269
        }
    },
    {
        "Id": 180229,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "04/11/2015",
        "OwnerUserId": 19859.0,
        "Score": 1,
        "ViewCount": 226.0,
        "Title": "use z-scores independent sample t-test",
        "Body": "two datasets different group interested compare difference attitudinal outcome group however outcome measure different scale standardize z-scores make comparison wonder still valid use z-scores t-test give scale zero mean unit standard deviation",
        "Tags": [
            "t-test",
            "standardization",
            "z-score"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 0,
        "Topics": {
            "25": 0.30259308,
            "39": 0.3223267,
            "43": 0.14540924
        }
    },
    {
        "Id": 401670,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "07/04/2019",
        "OwnerUserId": 242709.0,
        "Score": 0,
        "ViewCount": 143.0,
        "Title": "application box-cox transformation consecutively",
        "Body": "far search even obtain optimal lambda value transform data normal distribute constant variance box cox transformation method may proper normal distribute data point short end closer form normal distribution well happens apply box cox transformation multiple time make data much closer normal distribution previous case sufficient information internet book",
        "Tags": [
            "time-series",
            "normal-distribution",
            "data-transformation",
            "heteroscedasticity"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 7,
        "Topics": {
            "43": 0.2514011,
            "45": 0.18377745,
            "46": 0.12215791
        }
    },
    {
        "Id": 415129,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "28/06/2019",
        "OwnerUserId": 236318.0,
        "Score": 1,
        "ViewCount": 22.0,
        "Title": "inner product norm represent variability random variable",
        "Body": "studyng coeficient ols regression would like understand follow statement one measure variability dependent variable sum square mathematician see simple inner product understand intuition represent variability help",
        "Tags": [
            "least-squares",
            "regression-coefficients"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "43": 0.179884,
            "47": 0.22208641
        }
    },
    {
        "Id": 85976,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 86036.0,
        "CreationDate": "09/02/2014",
        "OwnerUserId": 6015.0,
        "Score": 1,
        "ViewCount": 176.0,
        "Title": "predictive accuracy correlation input",
        "Body": "reading judgement uncerntainty p state normal linear model correlation input variable decrease predictive accuracy contrast human perception exactly opposite put specific quote meaning scatter whole page trouble understand statement anyone help",
        "Tags": [
            "regression",
            "correlation",
            "prediction"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 2,
        "Topics": {
            "43": 0.2568447,
            "46": 0.14136651
        }
    },
    {
        "Id": 441819,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 441827.0,
        "CreationDate": "21/12/2019",
        "OwnerUserId": 267542.0,
        "Score": 3,
        "ViewCount": 90.0,
        "Title": "move average slide window smooth technique forecasting technique",
        "Body": "roll average method mostly use produce smooth series remove noise ex- window move average general practice output fourth period window move average first period process forecast one step ahead take average point move average function r basically produce smooth series original series data point jan dec move average series data point feb nov case forecast dec use forecast function produce ets model want",
        "Tags": [
            "r",
            "machine-learning",
            "time-series",
            "mathematical-statistics",
            "moving-window"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "24": 0.32059458,
            "43": 0.21795975
        }
    },
    {
        "Id": 376924,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "14/11/2018",
        "OwnerUserId": 87066.0,
        "Score": 0,
        "ViewCount": 884.0,
        "Title": "correct standardise z-score feature within sample pca",
        "Body": "give data set different measure feature unit subject example number different cell type feature tumour subject n tumour feature want see cell type feature explain variation across tumour subject correct z-score value feature within subject e subject distribution value centre around thanks",
        "Tags": [
            "pca",
            "standardization"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 1,
        "Topics": {
            "0": 0.10017446,
            "21": 0.20168488,
            "25": 0.13571952,
            "35": 0.100549676,
            "43": 0.23906085
        }
    },
    {
        "Id": 315666,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 315701.0,
        "CreationDate": "25/11/2017",
        "OwnerUserId": 186166.0,
        "Score": 2,
        "ViewCount": 138.0,
        "Title": "relation variance bernoulli logistic regression",
        "Body": "reading article come across likelihood function logistic regression define discussion reason please assume discrete case try make sense equation derive might article may understood search around found right hand side similarly resembles variance bernoulli trial since discrete logistic regression use multiple bernoulli trial case thought might something related two linear regression one metric use calculate good model measure much variance dataset explain model thought maximize likelihood may something similar maximize explain variance bernoulli trial intuition right path fundamental misunderstand",
        "Tags": [
            "machine-learning",
            "logistic",
            "mathematical-statistics"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "11": 0.11079405,
            "13": 0.12996952,
            "43": 0.30361
        }
    },
    {
        "Id": 374649,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "31/10/2018",
        "OwnerUserId": 146327.0,
        "Score": 1,
        "ViewCount": 28.0,
        "Title": "classification problem continous target",
        "Body": "big dataset around k sample k real-value feature target variable distribution highly concentrate around zero around value histogram want train model predict whether output give sample positive negative try couple classification regression method result satisfactory suggestion handle problem appreciate information value near important value near zero convert target good idea hand since majority target value around zero treat problem regression problem obtain good solution since try reduce error related target large absolute value find true boundary small value try hierarchical method e first decide whether target small large decide whether positive negative result satisfactory",
        "Tags": [
            "regression",
            "classification",
            "high-dimensional"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "0": 0.25033823,
            "43": 0.26468167,
            "49": 0.12044415
        }
    },
    {
        "Id": 415711,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 415893.0,
        "CreationDate": "02/07/2019",
        "OwnerUserId": 252563.0,
        "Score": 3,
        "ViewCount": 86.0,
        "Title": "individual significance data point correlation",
        "Body": "question stackoverflow ask possible find individual significance correlation coefficient node answer question later state use normalize value product variable identify individual significance however sure whether correct summation lead final correlation coefficient depends single value product normalize x single value high others lead individual significance reason ask data indexed base node show table assume data node effect example node contribute positive correlation node reason correlation coefficient know isolate investigate node background statistic mediocre please correct well method described viable mathematically way go reference appreciate",
        "Tags": [
            "correlation",
            "sensitivity-analysis",
            "leverage"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 2,
        "Topics": {
            "43": 0.31902686,
            "46": 0.10002331
        }
    },
    {
        "Id": 243316,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 300462.0,
        "CreationDate": "31/10/2016",
        "OwnerUserId": 136854.0,
        "Score": 3,
        "ViewCount": 1257.0,
        "Title": "design matrix one-way anova",
        "Body": "note homework question feel free prod towards answer want also pretty bad statistic sorry advance stupid ask write differential effect version one-way anova give overall mean linear model also level observation per level design matrix contain element sum j k alpha j although really sure something else could thing could ask",
        "Tags": [
            "self-study",
            "anova"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 1,
        "Topics": {
            "21": 0.12756203,
            "33": 0.14415754,
            "43": 0.35802403
        }
    },
    {
        "Id": 327917,
        "PostTypeId": 2,
        "ParentId": 327414.0,
        "AcceptedAnswerId": "",
        "CreationDate": "10/02/2018",
        "OwnerUserId": 144604.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "compare interaction model multiple regression really substantive difference probe simple slope measure variable path analysis within sem framework first let state think one way tackle question instance could choose drop non-significant term model prior probe slope alternatively perhaps want keep interaction term model theoretical reason maybe something relevant able make statement interaction authoritarian parent temperament control interaction parent style dimension temperament whether drop keep term model really three coefficient focus mind note look like center variable include intercept model equal alternatively care make statement change relation authoritarian parent outcome variable function child temperament would swap apply exist formula calculation simple slope involve two-way interaction tend generally dislike selection static value probe simple slope sort pet peeve colleague take slightly different approach probe simple slope along range continuous plausible value barstead et al smith et al main reason dislike selection static value simple slope analysis happens simple slope significant sd mean sd mean know much meaning place transition across threshold associate change sd case good site provide necessary formula calculate simple slope kristopher preacher site take approach advocate keep n interaction term parent covariates essentially make statement estimate simple slope different level moderator control dimension parent good luck reference barstead g smith k laursen b booth-laforce c king rubin k h association motivation social withdrawal internalize problem transition high school role positive parent best friend relationship journal research adolescence doi jora smith k barstead g rubin k h neuroticism conscientiousness moderator relation social withdrawal internalize problem adolescence journal youth adolescence doi -z",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "30": 0.11897558,
            "43": 0.31156147
        }
    },
    {
        "Id": 168132,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "20/08/2015",
        "OwnerUserId": 77940.0,
        "Score": 2,
        "ViewCount": 416.0,
        "Title": "posterior stationary distribution gibbs chain",
        "Body": "trouble understand setup follow probabilistic graphical model koller friedman say wish generate sample posterior distribution go define transition probability mean understand correctly order show stationary distribution must show confuse thing show transition probability suppose define fix actually many different transition function one need account somewhere",
        "Tags": [
            "mcmc",
            "graphical-model",
            "gibbs"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 1,
        "Topics": {
            "20": 0.19311987,
            "41": 0.24567021,
            "43": 0.23653746
        }
    },
    {
        "Id": 124662,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "19/11/2014",
        "OwnerUserId": 55846.0,
        "Score": 6,
        "ViewCount": 3161.0,
        "Title": "group fixed-effects individual-fixed effect use plm r",
        "Body": "analyze data evaluate impact causal effect program deliver group level village outcome interest measure individual level individual within village program placement random several observation intervention treatment control village want use difference-in-differences approach estimate impact program try control unobservable factor hence capture error term may influence placement program want use extension multiple period multiple group bertrand duflo mullainathan hansen follow imbens wooldridge want estimate equation like indicator unit group time period expose treatment since data panel village individual equation may least two problem first recognize observation grouped cluster village second ignores observation different time period come village although individual thought ok probably use panel-data method particular fixed-effects within estimator include fix effect village level control unobserved factor village-level may affect outcome participation program first question way go use different approach ok try use r plm package use command like plm formula gtreat time impact data df index c id time model within work though throw error say duplicate couple time-id error pdim default index index try find solution seem possible plm package duplicate observation time id see example post case duplication natural outcome measure individual level panel individual level village level second question workaround estimate panel data fixed-effects model within estimator r give data solution found consist aggregate data village level work sure seem ignore lot valuable information variability individual level addition sample big individual observation -nearly much village level -we village treatment control- many thanks advance thought answer question",
        "Tags": [
            "r",
            "fixed-effects-model",
            "causality",
            "difference-in-difference"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "3": 0.103413284,
            "30": 0.1285426,
            "33": 0.22710265,
            "43": 0.23217686
        }
    },
    {
        "Id": 267458,
        "PostTypeId": 2,
        "ParentId": 267393.0,
        "AcceptedAnswerId": "",
        "CreationDate": "14/03/2017",
        "OwnerUserId": 125405.0,
        "Score": 3,
        "ViewCount": "",
        "Title": "",
        "Body": "somebody tell simple case use typical case logistic regression logistic regression technique use often machine learn classify data point example logistic regression use classify whether email spam spam classify whether person disease specifically logistic regression model say probability data point class follow parameter vector typically estimate use mle specifically use optimization method find estimator expression minimize expression negative log likelihood minimize equivalent maximize likelihood",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "41": 0.19418345,
            "43": 0.31919882
        }
    },
    {
        "Id": 404090,
        "PostTypeId": 2,
        "ParentId": 404087.0,
        "AcceptedAnswerId": "",
        "CreationDate": "20/04/2019",
        "OwnerUserId": 204068.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "obvious reason first come mind fact actually minimize training process via explicit solve iteratively like gradient descent approach example linear regression find coefficient minimizes mse explicitly take derivative equate zero solve e find guarantee minimization case final error metric e g -accuracy different training loss function normally assume loss function good representative final error metric",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 3,
        "Topics": {
            "10": 0.2977049,
            "43": 0.2918456
        }
    },
    {
        "Id": 406701,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "05/05/2019",
        "OwnerUserId": 128555.0,
        "Score": 0,
        "ViewCount": 12.0,
        "Title": "non homogonous data interaction",
        "Body": "experiment design variable level variable level demographic information collect participant demographic level demographic level demographic level demographic level dependant variable score perception test experimental variable show homogeneity residual demographic variable levene bartlett test give outcome ideal test would tail anova regression different variable type one demographic one look separately experimental variable use anova tukey demographic use kruskall-wilk dunn also look demographic anova tukey significant significant marking match set test get regression next want see interaction specifically variable demographic variable categorical interval group length stimulus syllable syllable syllable accumulative categorical ordinal self identify level awareness exposure phenomenon test know someone etc test would allow guess take chance anova would nice confirm think make new group group variable level demographic level group b variable level demographic level group c variable level demographic level group variable level demographic level test score group score group b would option",
        "Tags": [
            "normal-distribution",
            "interaction",
            "residuals"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 0,
        "Topics": {
            "2": 0.12224131,
            "11": 0.18297812,
            "21": 0.13238929,
            "43": 0.1715598
        }
    },
    {
        "Id": 207595,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 207598.0,
        "CreationDate": "15/04/2016",
        "OwnerUserId": 9136.0,
        "Score": 10,
        "ViewCount": 7087.0,
        "Title": "distribution ratio two gamma random variable",
        "Body": "assume define distribution",
        "Tags": [
            "probability",
            "distributions",
            "mathematical-statistics",
            "gamma-distribution"
        ],
        "AnswerCount": 2.0,
        "CommentCount": 5,
        "Topics": {
            "20": 0.29707932,
            "43": 0.15806301,
            "45": 0.35019305
        }
    },
    {
        "Id": 336105,
        "PostTypeId": 2,
        "ParentId": 336066.0,
        "AcceptedAnswerId": "",
        "CreationDate": "22/03/2018",
        "OwnerUserId": 53456.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "multiple way set scale latent variable reference marker variable fix load one item per factor value one default cfa sem software fixed-factor fix variance factor one essence standardize effects-coding lesser-known approach see little et al overview loading fix average one intercept fix average zero give latent variable non-arbitrary little word scale indicator described useful strategy compare latent mean primary goal analysis anyways reference marker variable scale-setting default software prevents estimate factor loading fix since focus efa generally estimate exploratory factor loading latent variable standardize instead efa software usually provide standardize factor loading reference little slegers w card n non-arbitrary method identify scale latent variable sem mac model structural equation model",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "31": 0.20710565,
            "43": 0.25675067
        }
    },
    {
        "Id": 339232,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "07/04/2018",
        "OwnerUserId": 203424.0,
        "Score": 1,
        "ViewCount": 175.0,
        "Title": "understand equation coupon collector problem",
        "Body": "collect new world cup panini album album sticker approx different sticker packet would like know probability miss sticker buying packet answer eqperes show analytical solution base collector problem group drawing wolfgang stadje use fully understand equation associate description answer would set possible sticker would subset interest op go draw replacement random subset different sticker number element appear least one subset difference variable equation",
        "Tags": [
            "probability",
            "coupon-collector-problem"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 2,
        "Topics": {
            "20": 0.13374762,
            "27": 0.1326141,
            "43": 0.3694426
        }
    },
    {
        "Id": 126618,
        "PostTypeId": 2,
        "ParentId": 125833.0,
        "AcceptedAnswerId": "",
        "CreationDate": "04/12/2014",
        "OwnerUserId": 12492.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "inclusion random intercept group would appear necessary difference necessarily explain difference among group inclusion use test hypothesis difference among group due difference among note answer might entirely incorrect",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "33": 0.4008912,
            "39": 0.18829161,
            "43": 0.19339246
        }
    },
    {
        "Id": 49780,
        "PostTypeId": 2,
        "ParentId": 49730.0,
        "AcceptedAnswerId": "",
        "CreationDate": "11/02/2013",
        "OwnerUserId": 17230.0,
        "Score": 3,
        "ViewCount": "",
        "Title": "",
        "Body": "simply mean unit change predictor presumably unit day log odds ratio response change step e odds ratio response change factor change predictor week log odds ratio response change step e odds ratio response change factor course worth check presume linear relationship predictor log odds ratio response seem reasonable ordinary least square regression mutatis mutandis",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "5": 0.20082772,
            "11": 0.23504956,
            "43": 0.20282023
        }
    },
    {
        "Id": 200071,
        "PostTypeId": 2,
        "ParentId": 111654.0,
        "AcceptedAnswerId": "",
        "CreationDate": "05/03/2016",
        "OwnerUserId": 103270.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "short regular adaboost decision stump decision tree weak learner usually outperform linear svms course dependent data data normalization etc certainly general rule reason fairly clear adaboost learn non-linear decision boundary almost always helpful especially data linearly separate question empircal result much say adaboost v svm except svm build upon statistical learn theory slt entirely sure say adaboost give svm maximizes margin point cloud give linearly separable data outperform adaboost since would expect well generalization svm adaboost point svm us proper non-linear kernel fit data well generalizes well usually outperforms adaboost adaboost commonly use fast viola jones use year ago realize fast object detection especially face recognition addition please see comment question",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "0": 0.13870743,
            "9": 0.12258286,
            "43": 0.36317372
        }
    },
    {
        "Id": 348496,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "27/05/2018",
        "OwnerUserId": 146224.0,
        "Score": 0,
        "ViewCount": 192.0,
        "Title": "optimal neural net weight use cross entropy loss",
        "Body": "try understand cross-entropy work find optimal weight neural network accord eli bendersky website neural network deeplearning tutorial find optimal weight use gradient descent use cross-entropy loss function without use gradient descent mean way find derivative cross-entropy respect w ij make result derivative equal zero like winer resource available try find optimal weight neural network use way",
        "Tags": [
            "neural-networks",
            "backpropagation",
            "derivative",
            "softmax",
            "cross-entropy"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "6": 0.15913157,
            "23": 0.39126095,
            "43": 0.21622038
        }
    },
    {
        "Id": 126603,
        "PostTypeId": 2,
        "ParentId": 126602.0,
        "AcceptedAnswerId": "",
        "CreationDate": "04/12/2014",
        "OwnerUserId": 1352.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "r vectorizes operation addition automatically question well stackoverflow r tag",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "3": 0.23121706,
            "27": 0.2176595,
            "43": 0.38145947
        }
    },
    {
        "Id": 257204,
        "PostTypeId": 2,
        "ParentId": 257167.0,
        "AcceptedAnswerId": "",
        "CreationDate": "19/01/2017",
        "OwnerUserId": 145974.0,
        "Score": 3,
        "ViewCount": "",
        "Title": "",
        "Body": "checked everything zip file problem seem simple enough base jag model post discrepancy sd jag output due sensitivity prior test use data clone relies mcmc machinery get mle posterior mean different mle likely due prior expect mle n might want ponder really want bayesian answer discrepancy bother see gist reproducible example result",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "3": 0.106359124,
            "41": 0.19101787,
            "43": 0.2495068
        }
    },
    {
        "Id": 314977,
        "PostTypeId": 2,
        "ParentId": 92752.0,
        "AcceptedAnswerId": "",
        "CreationDate": "21/11/2017",
        "OwnerUserId": 140608.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "seem two main approach decision make bayesian b test first one base paper john kruschke indiana university k kruschke bayesian estimation supersedes test journal experimental psychology general decision rule use paper base concept region practical equivalence rope another possibility use concept expect loss propose chris stucchio c stucchio bayesian b test vwo another approach would consider approach suggest cam davidson pilon look posterior distribution make lot sense would fit well within rope method use rope method add advantage give also rule experiment inconclusive b variant declare winner find blog post bayesian b test step-by-step guide also include python code snippet mostly base python project host github",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "41": 0.10277932,
            "43": 0.42215076
        }
    },
    {
        "Id": 11514,
        "PostTypeId": 2,
        "ParentId": 11498.0,
        "AcceptedAnswerId": "",
        "CreationDate": "03/06/2011",
        "OwnerUserId": 4253.0,
        "Score": 3,
        "ViewCount": "",
        "Title": "",
        "Body": "fully data-driven model selection result standard error p-values small confidence interval narrow overstate effect remain term model time effect usually model use restrict cubic spline detailed case study context generalize least square correlate serial data may found see two attachment bottom name course pdf rms pdf us r rms package case study contains information choice basis function time component",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 9,
        "Topics": {
            "1": 0.15670943,
            "24": 0.19330223,
            "43": 0.2501916
        }
    },
    {
        "Id": 35474,
        "PostTypeId": 2,
        "ParentId": 35472.0,
        "AcceptedAnswerId": "",
        "CreationDate": "31/08/2012",
        "OwnerUserId": 11032.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "constraint see inherent multinomial distribution namely one one rest linear constraint mean say sum take collinearity effect notice nothing unusual disturb",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "11": 0.2932639,
            "43": 0.20896561
        }
    },
    {
        "Id": 107728,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "12/07/2014",
        "OwnerUserId": 51895.0,
        "Score": 1,
        "ViewCount": 462.0,
        "Title": "single instrument reduce ovb measurement error model",
        "Body": "endogenous variable likely measure error also confound unobservables instrument rise value endogenous variable time exogenous error outcome equation",
        "Tags": [
            "econometrics"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "9": 0.16230625,
            "11": 0.14132085,
            "26": 0.18361641,
            "43": 0.30641475
        }
    },
    {
        "Id": 448784,
        "PostTypeId": 2,
        "ParentId": 448540.0,
        "AcceptedAnswerId": "",
        "CreationDate": "10/02/2020",
        "OwnerUserId": 4598.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "imho confusion spot difficulty arises fact mix variance two different source one standard deviation variance number independent test case determines certain performance one particular surrogate model refer variance may variance surrogate model model instability well use may really want pull two variance apart different implication hyperparameter optimization high mean need change model towards stable training high mean need get case order say observe variance across full repetition fold pool observe variance across fold repetition allows calculate may well way estimate though currently work currently work different loss function optimization heuristic use variance yet ready meanwhile use follow heuristic actual sample size course improve repeat cross validation iff model sufficiently stable e sample size limit factor denominator standard error calculation stay repeat cross validation add new independent test case see first fold data set work limit factor indeed almost always number independent case unless hyperparameter optimization contains heavily overfitting region fine also easy check whether stability ok apparent optimum look sd prediction case across repetition case also low complexity anyways go back think aggregate drastically change model approach analogue heuristic may built model instability limit factor",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "10": 0.37849462,
            "43": 0.2780646
        }
    },
    {
        "Id": 175726,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 175744.0,
        "CreationDate": "06/10/2015",
        "OwnerUserId": 30451.0,
        "Score": 1,
        "ViewCount": 53.0,
        "Title": "analyze full data set v residual",
        "Body": "say data set want model dependent variable function specify model specify interaction base prior knowledge however r value single regression model fair still test interaction well effect well analysis residual model problem forsee analysis residual test main effect test interaction thought best handle type problem would greatly appreciate",
        "Tags": [
            "r",
            "regression",
            "multiple-regression"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 2,
        "Topics": {
            "1": 0.10254989,
            "30": 0.118398376,
            "43": 0.39096862
        }
    },
    {
        "Id": 259427,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 260075.0,
        "CreationDate": "01/02/2017",
        "OwnerUserId": 147566.0,
        "Score": 10,
        "ViewCount": 182.0,
        "Title": "compare",
        "Body": "discuss task achievement rate way show attempt bad attempt",
        "Tags": [
            "probability",
            "sampling"
        ],
        "AnswerCount": 3.0,
        "CommentCount": 6,
        "Topics": {
            "13": 0.1875552,
            "16": 0.110689394,
            "41": 0.10269709,
            "43": 0.24962954
        }
    },
    {
        "Id": 432670,
        "PostTypeId": 2,
        "ParentId": 432516.0,
        "AcceptedAnswerId": "",
        "CreationDate": "22/10/2019",
        "OwnerUserId": 241137.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "mode symmetrical unimodal distribution always equal mean also median distribution unimodal distribution one one peak normal distribution fall category mode mean identical",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 6,
        "Topics": {
            "20": 0.11528304,
            "43": 0.34844157,
            "45": 0.16601016,
            "47": 0.13129595
        }
    },
    {
        "Id": 125683,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 125686.0,
        "CreationDate": "27/11/2014",
        "OwnerUserId": 61594.0,
        "Score": 1,
        "ViewCount": 337.0,
        "Title": "pearson correlation quizzy result",
        "Body": "investigate experiment spss follow value question case possible get feature group b significant p either group b low significance feature hardly interpret group low correlation group b also combine group correlation high significance p",
        "Tags": [
            "statistical-significance",
            "pearson-r"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "39": 0.2296351,
            "43": 0.23160818
        }
    },
    {
        "Id": 167702,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "18/08/2015",
        "OwnerUserId": 44424.0,
        "Score": 1,
        "ViewCount": 2549.0,
        "Title": "fitting varmax model use mt library r",
        "Body": "try fit varmax vector autoregressive moving-average exogenous variable model synthetically generate data use mt library available r found one function fitting model exogenous variable design var model call varx reading literature found method find varx representation varmax model straightforward thus find varx representation varmax model varx function could use question method already implement r transforms varmax varma vmax correspond var representation varx function must use fitting model type follow reproducible example generate varmax model intend estimate parameter varx function aforementioned wonder estimate varmax coefficient use varx function",
        "Tags": [
            "r",
            "time-series",
            "multivariate-analysis",
            "var"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 3,
        "Topics": {
            "1": 0.15418425,
            "3": 0.307653,
            "43": 0.10194512
        }
    },
    {
        "Id": 191742,
        "PostTypeId": 2,
        "ParentId": 191725.0,
        "AcceptedAnswerId": "",
        "CreationDate": "21/01/2016",
        "OwnerUserId": 805.0,
        "Score": 7,
        "ViewCount": "",
        "Title": "",
        "Body": "since sample kernel density estimate solve twice already focus sample histogram-as-population-pdf idea simply example r completeness since sample kernel density estimate simple note kernel like fourth order kernel density assumes kernel density r gaussian kernel bandwidth h data x",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "3": 0.15317135,
            "43": 0.28203648,
            "45": 0.18032397
        }
    },
    {
        "Id": 417548,
        "PostTypeId": 2,
        "ParentId": 416338.0,
        "AcceptedAnswerId": "",
        "CreationDate": "15/07/2019",
        "OwnerUserId": 28500.0,
        "Score": 3,
        "ViewCount": "",
        "Title": "",
        "Body": "problem general throw away recommendation make randomization process fit constraint rejection sample example get random sequence integer roll die repeatedly accept value range throw away value appear problem constraint sequential participant condition pose risk selection bias regardless proceed deterministic aspect permuted-block design lead selection bias example size- block initially suggest comment without think enough selection bias first assignment make within block third determine clinician recommendation patient inclusion unblinded trial unconsciously influence sense trial choice patient might right wrong base clinical judgement partial information next random treatment line pose problem matt lachin discus two type selection bias permuted-block design potential bias excess correct guess treatment assignment beyond expect chance know next treatment condition certainty smartphone trial could face problem know model ever assign sequentially trial example last phone distribute trial samdroidx model salesperson think samdroidx right next customer could lead push sale rather propose trial zhao weng paper link manuscript freely available provide nice overview problem demonstrates way minimize selection bias provide balance allocation block urn design impose constraint row choose among multiple random seed risk selection bias increase use permuted-block design large block size use different block size might preferable fix size- block example alternate size- size- block would prevent run provide harder pattern recognize could design larger-size block discard block rearrange order among block meet constraint however proceed constraint sequence condition risk selection bias magnitude risk depend assignment scheme detail trial would implement might help use type analysis described matt lachlin zhao weng compare propose scheme respect risk bias size- permute block would never model assign row suppose implement trial expect limit know inherent limit selection bias could minimize less implement trial know design less chance selection bias people pretty good recognize pattern might unconsciously internalize design",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "21": 0.16959842,
            "43": 0.38937
        }
    },
    {
        "Id": 435886,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "13/11/2019",
        "OwnerUserId": 5509.0,
        "Score": 1,
        "ViewCount": 19.0,
        "Title": "std err difference v std err mean difference analogy unpaired v pair t-test",
        "Body": "say element correspond group mean standard error look pairwise difference try compare zero see use standard error difference difference significant whereas use standard error mean difference significant difference direct correspondence unpaired v pair t-test respectively see t-test look pretty similar see sure statistically principially thing link together",
        "Tags": [
            "t-test",
            "mean",
            "standard-error"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "25": 0.16826124,
            "39": 0.39083958,
            "43": 0.17230773
        }
    },
    {
        "Id": 33438,
        "PostTypeId": 2,
        "ParentId": 33308.0,
        "AcceptedAnswerId": "",
        "CreationDate": "31/07/2012",
        "OwnerUserId": 12948.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "distinction identify far hidden one original paper neil lawrence two version sparse bayesian compression scheme informative vector machine kernel workshop nip one microsoft research site one laurence site m version extra sentence statement select data point close decision boundary characteristic share svm original view ivm vector represent middle wrong point compression scheme sense look sparse representation data set ivm also seek retain vector provide information analysis re-used data set computation repeat ability reduce size data set useful computation bigo rvm select middle vector weight act basis function e g gaussians see ch relevance vector machine bishop pattern recognition machine learn ok explanation bit hand wave description fully complete hopefully help relaxed compact matrix formulation feedback would still welcome",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "15": 0.16229114,
            "43": 0.2525053
        }
    },
    {
        "Id": 262491,
        "PostTypeId": 2,
        "ParentId": 101571.0,
        "AcceptedAnswerId": "",
        "CreationDate": "17/02/2017",
        "OwnerUserId": 98942.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "understand correctly generate null-distribution one focal parameter subsequently calculate one p-value base permutation still one one-sided statistical test matter large dataset many permutation multiple test correction apply result",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "39": 0.17394555,
            "43": 0.24465553
        }
    },
    {
        "Id": 18788,
        "PostTypeId": 2,
        "ParentId": 18775.0,
        "AcceptedAnswerId": "",
        "CreationDate": "22/11/2011",
        "OwnerUserId": 3874.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "maxtc discus need know study help analyze tell however skeptical meaning -point scale scale allow much precision harder figure perception product determines rating people give example maybe people round number unexpected way use visual analog scale maybe size hand affect rating also validate scale way relate gold standard difficult explain small difference like versus mean validate somehow say less important advise choose scale whose result convincingly related unobserved variable interest perception value also help choose scale easily convincingly transform interval data rather ordinal data william moroney adverb intensifier list might help",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "31": 0.23182121,
            "43": 0.4546096
        }
    },
    {
        "Id": 8690,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 8760.0,
        "CreationDate": "23/03/2011",
        "OwnerUserId": 795.0,
        "Score": 7,
        "ViewCount": 357.0,
        "Title": "linear model random coefficient",
        "Body": "suppose want fit model form data regressors regressand observe bernoulli random variable equal one unknown probability zero otherwise assume kind regularity error independent regressors independent etc question name one throw data data perform ordinary multiple least square least square coefficient correspond term converge add observation least square regression advisable model distribution least square coefficient correspond term null hypothesis deterministic regression coefficient certain t-distribution parameter depend design matrix look analogue random coefficient least square regression advisable presence random coefficient affect distribution sample regression coefficient",
        "Tags": [
            "regression"
        ],
        "AnswerCount": 2.0,
        "CommentCount": 0,
        "Topics": {
            "19": 0.5545121
        }
    },
    {
        "Id": 317140,
        "PostTypeId": 2,
        "ParentId": 19009.0,
        "AcceptedAnswerId": "",
        "CreationDate": "04/12/2017",
        "OwnerUserId": 8013.0,
        "Score": 4,
        "ViewCount": "",
        "Title": "",
        "Body": "paper koch abel urbach reject randomization summarily mean achieve rather claim neither sufficient necessary achieve criterion take-home message rct must necessarily do answer every scientific question b publish rct may gold-standard evidence efficacy alternative rct open label trial olts obvious choice since presumptive purpose say trial evaluate novel therapy readily accessible patient population analysis rct olt similar principal analyze observational study apply control causal factor block randomization improve efficiency reduce bias study mean validate certain statistical test randomize participant independent identically distribute per assumption t-test log-rank test rct pro cluster correlate participant likely broken study randomization without contamination dependence structure similar within treatment assignment method independent data estimate correct standard error anyway similarly prognostic factor likely balance study group time randomization rct con randomization address contamination participant consequence indication even participation study likely relate one another influence participation outcome result even block distribution prognostic factor heterogeneous arm receive high risk treatment high risk baseline likely die sooner lead healthy risk set future event time survivor bias lead cross hazard inefficient log-rank test basis causal inference estimate effect rewind-time instance assign treat participant control subtract difference rct assignment treatment completely random confound indication blinding possible may reduce risk differential treatment discontinuation rct- differential non-differential follow-up due attrition contribute imbalanced participant upon study completion non-blinded study introduce risk differential treatment discontinuation study parameter around randomization blinding invasive therapy necessarily restrict eligible study pool small subset consent parameter healthy participant bias facilitation mask treatment randomly assign possible administer treatment way participant know arm randomize rct appropriate placebo available do note appropriate use placebo participant receives standard care soc instance suppose ind administer injection soc pill control participant receive soc unlabeled pill form saline injection active arm participant receive ind injection identical sugar pill rct- placebo may available instance provenge monoclonal antibody therapy high grade prostate cancer administration treatment require invasive procedure call leukapheresis leukapheresis invasive costly ethically perform control arm provenge-assigned participant know receive ind method balance comparison group expect distribution covariates analysis sample equal distribution ind-treated control participant rct time randomization sample balance treatment control group note well expect probabilistic balance possible prognostic factor re-randomization possible batch-entry design although far less prevalent day rct- efficient design still require control prognostic factor optimal design presence treatment effect balance analysis attrition unequal cluster size due loss-to-follow-up commonly mean balance design guaranteed randomization guarantee balance prognostic factor",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "30": 0.27705902,
            "43": 0.19336158
        }
    },
    {
        "Id": 133205,
        "PostTypeId": 2,
        "ParentId": 11859.0,
        "AcceptedAnswerId": "",
        "CreationDate": "13/01/2015",
        "OwnerUserId": 36601.0,
        "Score": 28,
        "ViewCount": "",
        "Title": "",
        "Body": "multiclass classification mean classification task two class e g classify set image fruit may orange apple pear multiclass classification make assumption sample assign one one label fruit either apple pear time multilabel classification assigns sample set target label thought predict property data-point mutually exclusive topic relevant document text might religion politics finance education time none take",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "0": 0.28716785,
            "43": 0.32689816
        }
    },
    {
        "Id": 372051,
        "PostTypeId": 2,
        "ParentId": 372048.0,
        "AcceptedAnswerId": "",
        "CreationDate": "16/10/2018",
        "OwnerUserId": 120831.0,
        "Score": 107,
        "ViewCount": "",
        "Title": "",
        "Body": "well distribute term obtain interpret law total probability apply event use bayesian statistic correctly use bayesian statistic correctly recover bayes law left fraction ignore data use prior suppose rejoinder criticism principle bayesians adjust prior support whatever conclusion want whereas bayesians would argue bayesian statistic actually work yes successfully nerd-snipe neither mathematician physicist though sure many point worth",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 11,
        "Topics": {
            "41": 0.15032697,
            "43": 0.4330539,
            "47": 0.12739338
        }
    },
    {
        "Id": 272701,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "08/04/2017",
        "OwnerUserId": "",
        "Score": -1,
        "ViewCount": 329.0,
        "Title": "data imply sublinear linear superlinear convergence",
        "Body": "know definition sublinear superlinear convergence data seem neither sublinear superlinear unless mistaken sublinear convergence superlinear convergence data algorithm seem take big step seem like go outcome next iteration take tiny step seem like outcome end reach zero consistency step size use steihaug algorithm minimise rosenbrock function result get alternative start point question mathoverflow guy kind convergence data suggest",
        "Tags": [
            "convergence"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "22": 0.1493582,
            "43": 0.4370858
        }
    },
    {
        "Id": 31087,
        "PostTypeId": 2,
        "ParentId": 31083.0,
        "AcceptedAnswerId": "",
        "CreationDate": "25/06/2012",
        "OwnerUserId": 221.0,
        "Score": 58,
        "ViewCount": "",
        "Title": "",
        "Body": "example help base latter plot could decide initial variable plot maybe variable huge try principal component analysis pca use first two three component pca perform cluster analysis",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 5,
        "Topics": {
            "35": 0.15348274,
            "36": 0.112553135,
            "38": 0.10274241,
            "43": 0.3641698
        }
    },
    {
        "Id": 327381,
        "PostTypeId": 2,
        "ParentId": 327378.0,
        "AcceptedAnswerId": "",
        "CreationDate": "07/02/2018",
        "OwnerUserId": 131407.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "right series one-way anova best way model instead consider repeat measure anova mixed effect model lot resource cv elsewhere web",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "21": 0.21687898,
            "43": 0.38752785
        }
    },
    {
        "Id": 68372,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "26/08/2013",
        "OwnerUserId": 3406.0,
        "Score": 2,
        "ViewCount": 227.0,
        "Title": "method base trim mean require homoscedasticity",
        "Body": "understand wilcox book method use trim mean robust distributional assumption method e g package wrs assume distribution although normal across condition model word data heteroscedasticity variance condition others indicate distibutions across condition example could anyone recommend alternative analyze kind data e unequal variance wilcox r modern statistic social behavioral science crc press",
        "Tags": [
            "r",
            "heteroscedasticity",
            "trimmed-mean"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 7,
        "Topics": {
            "14": 0.109728925,
            "29": 0.20586711,
            "43": 0.14721607
        }
    },
    {
        "Id": 50794,
        "PostTypeId": 2,
        "ParentId": 50348.0,
        "AcceptedAnswerId": "",
        "CreationDate": "25/02/2013",
        "OwnerUserId": 19731.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "think figure state comment think indeed problem estimator found another answer use linear estimator similar problem make mistake estimator case since actually know guess could use estimate base sample base wikipedia article weight mean subtract covariance influence estimate covariance",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "26": 0.20888615,
            "43": 0.42669615,
            "45": 0.13347165
        }
    },
    {
        "Id": 172772,
        "PostTypeId": 2,
        "ParentId": 172202.0,
        "AcceptedAnswerId": "",
        "CreationDate": "16/09/2015",
        "OwnerUserId": 17230.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "matrix computation especially finicky plain old arithmetical operation concisely represent define number carry regression observation miss value predictor need either substitute number omit predictor omit observation may assuage someone doubt work consequence cod scheme predictor let expect value response give model coefficient estimate pre-greenspan post-greenspan intercept equal slope v differ two time period gather want say method equivalent wayne method without indicator variable appear isolated term multiply note constraint zero time period give equal value predictor people would want violation marginality principle theoretical support well discrepant data",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "11": 0.10443996,
            "43": 0.35886952
        }
    },
    {
        "Id": 413393,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 413747.0,
        "CreationDate": "17/06/2019",
        "OwnerUserId": 12147.0,
        "Score": 3,
        "ViewCount": 513.0,
        "Title": "somers model validation",
        "Body": "somers define example predictive accuracy credit rating measurement statistical inference walter orth define case predict x count concordant pair discordant pair else far rather simple denominator define thus count pair one y-values equal thus tie count tie reduce denominator therefore increase carry case validation model set mean model prediction realize value look concordant discordant par prediction realization denominator count realization pair author say case one judge performance prediction default indicator credit score model would interpret probability default risk class model default observation paper link mention case give accuracy ratio gini coeffient question agree useful interpretation somers prediction observe realization state consequence especially case tie prediction count denominator tie relization",
        "Tags": [
            "predictive-models",
            "prediction",
            "validation",
            "somers-d"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "43": 0.22546695,
            "48": 0.10583613
        }
    },
    {
        "Id": 203151,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "23/03/2016",
        "OwnerUserId": 88794.0,
        "Score": 0,
        "ViewCount": 68.0,
        "Title": "doubt feature selection",
        "Body": "work text classification problem use python nltk get two frequency distribution one class basically binary classification doubt way apply feature selection since get two separate model classificator algorithm manually implement",
        "Tags": [
            "classification",
            "feature-selection",
            "text-mining",
            "natural-language"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "0": 0.21249357,
            "42": 0.2210032,
            "43": 0.38509807
        }
    },
    {
        "Id": 18760,
        "PostTypeId": 2,
        "ParentId": 18758.0,
        "AcceptedAnswerId": "",
        "CreationDate": "22/11/2011",
        "OwnerUserId": 4257.0,
        "Score": 4,
        "ViewCount": "",
        "Title": "",
        "Body": "assume king chose match ace hand hand card ace card left game amongst one ace interest opponent card total card one way handle real question especially unsure whether draw independent matter second card dealt one opponent come small stack look negation easy ass probability none f card opponent ace question consider probability number valid situation among possible situation possible situation possible draw card card c one valid exactly draw look come stack card hold ace draw make stack non-ace card similarly c since work probability finally probability happen exactly obtain assume draw independent demonstrate wo look problem another light card drawn hand face opponent make easy see independence draw card still face numbered card opponent dealt get number rest card get number look probability number valid situation among possible situation ace go one card look assign random number ace possible situation go give ace opponent namely random number obviously give love smell probability morning hop make homework",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "13": 0.26436433,
            "22": 0.22318447,
            "43": 0.43278047
        }
    },
    {
        "Id": 256636,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "17/01/2017",
        "OwnerUserId": 117306.0,
        "Score": 1,
        "ViewCount": 49.0,
        "Title": "bias simulation study",
        "Body": "do simulation study see model well prediction ann svm ann show good rmse high bias result acceptable purpose bias simulation study lot researcher use bias simulation",
        "Tags": [
            "time-series",
            "simulation",
            "bias"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "6": 0.1460737,
            "26": 0.11997944,
            "43": 0.34272355
        }
    },
    {
        "Id": 105770,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "04/07/2014",
        "OwnerUserId": 49498.0,
        "Score": 3,
        "ViewCount": 1094.0,
        "Title": "latent semantic classification",
        "Body": "create training data set document classification use lsa create term-to-document matrix class label also know whether add class label term-document matrix create anther matrix know exact step follow create training data",
        "Tags": [
            "machine-learning",
            "natural-language",
            "train",
            "lsa"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "0": 0.16200508,
            "42": 0.107820936,
            "43": 0.2966664
        }
    },
    {
        "Id": 431872,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "17/10/2019",
        "OwnerUserId": 262939.0,
        "Score": 0,
        "ViewCount": 64.0,
        "Title": "optimal lag-selection var-model r",
        "Body": "trouble lag specification var-model purpose model measure orthogonal impulse response function oil price shock macroeconomic variable gdp-growth unemployment rate inflation-rate interest-rate quarterly observation total observation variable put recursively set follow order oil price unemployment gdp inflation interest rate variable include log oil price checked stationarity use adf kp correct stationary problem come time select lag-length var-model r use command aic suggest lag-length suppose consistent result since us lot degree freedom estimate coefficient model guess lag would reasonable question therefore wrong something check specification variable work properly dataset suggestion guidance would highly appreciate thank cholesky decomposition",
        "Tags": [
            "model-selection",
            "aic",
            "var",
            "lags"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 2,
        "Topics": {
            "24": 0.19352314,
            "43": 0.27588987
        }
    },
    {
        "Id": 324136,
        "PostTypeId": 2,
        "ParentId": 313685.0,
        "AcceptedAnswerId": "",
        "CreationDate": "20/01/2018",
        "OwnerUserId": 36206.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "one way approach similar propose train som combine datasets node compute number representative r r compute proportion r r node datasets different size work quite well due representation one group relative need consider way normalize proportion visualize result number way make histogram node proportion u-shaped graph tell two datasets separate map whereas a-shaped graph tell mixed also plot node x-y plane assume two dimensional som color node proportion r r choose size map tricky small dimension map likely overlap unless datasets different large map likely lead separation datasets simply room spread example use map large number node start define definitive cluster distance cluster look distance node call u-matrix wikipedia page brief description along link in-depth detail",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "38": 0.19354516,
            "43": 0.28786218
        }
    },
    {
        "Id": 435044,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 435087.0,
        "CreationDate": "07/11/2019",
        "OwnerUserId": 196725.0,
        "Score": 2,
        "ViewCount": 104.0,
        "Title": "interpret intercept linear mixed model result python statsmodel package",
        "Body": "use python package statsmodel code link linear mixed model random variable x group one would run code produce one value intercept parameter table say different group case pig yet table show one intercept value e interpret table relationship happen hood word one value relate intercept",
        "Tags": [
            "mixed-model",
            "python",
            "statsmodels"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "3": 0.16368614,
            "5": 0.118147425,
            "33": 0.3027871,
            "43": 0.15032
        }
    },
    {
        "Id": 80064,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "18/12/2013",
        "OwnerUserId": 1005.0,
        "Score": 0,
        "ViewCount": 143.0,
        "Title": "cause sparseness data",
        "Body": "data sparseness appear due either high sample size high dimension different situation two case thanks",
        "Tags": [
            "dataset"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "14": 0.1409455,
            "15": 0.10235499,
            "43": 0.36696044
        }
    },
    {
        "Id": 306601,
        "PostTypeId": 2,
        "ParentId": 306595.0,
        "AcceptedAnswerId": "",
        "CreationDate": "06/10/2017",
        "OwnerUserId": 179633.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "input pass iterate instance training set one one till iterate instance training set many time number epoch iteration provide hyper parameter iterate adjust weight per instance multiple iteration four instance let say ran iteration yes instance sent network time together instance training sent one one time yes question first pas mean instance mean instance pas fifty time hope answer multiple question",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 3,
        "Topics": {
            "6": 0.25213102,
            "27": 0.12482857,
            "43": 0.36004063
        }
    },
    {
        "Id": 183477,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "25/11/2015",
        "OwnerUserId": 2728.0,
        "Score": 2,
        "ViewCount": 907.0,
        "Title": "standard library funk svd gradient descent svd eigenvalue",
        "Body": "want get first eigenvectors real symmetric matrix miss value since miss value able use common linear program technique stochastic gradient work funk svd use recommendation engine solves general problem low-rank svd approximation use gradient descent flavor funk svd able solve use r r cplusplus library play well rcpp standard implementation library use make sense roll standard algorithm",
        "Tags": [
            "svd",
            "gradient-descent",
            "recommender-system",
            "eigenvalues",
            "c++"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 4,
        "Topics": {
            "3": 0.20270137,
            "43": 0.37168133
        }
    },
    {
        "Id": 286793,
        "PostTypeId": 2,
        "ParentId": 286782.0,
        "AcceptedAnswerId": "",
        "CreationDate": "22/06/2017",
        "OwnerUserId": 165900.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "method problematic introduce human-spotted structural change increase complexity model complexity comparable imagine extreme situation every point spot changepoints arima point obviously parameterized lead likelihood value every part also two part model example arima structure change tendency discover pattern random observation mislead nature think arima enough capture variation increase model complexity increase lag order like arima use totally different model like hmm explicitly catch changepoints",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "24": 0.19024402,
            "41": 0.10174569,
            "43": 0.32929593
        }
    },
    {
        "Id": 251715,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "15/12/2016",
        "OwnerUserId": 35866.0,
        "Score": 2,
        "ViewCount": 82.0,
        "Title": "understand probability confidence interval bootstrapping",
        "Body": "look input whether use right tool job particular currently use bootstrapping begin suspect use something else background problem domain whenever web browser request item whether web page video image etc content provider youtube reddit etc may able send response via different internet route currently run experiment traffic spread randomly across route goal identify case significant difference route end user say significant refer difference metric user could perceive instance m difference latency amount time message flow client server analysis challenge one large problem face amount noise data instance microwave someone house cause wi-fi connection become degrade result increase latency measurement found make metric like average useless also reinforces fact need understand uncertainty measurement directly compare p average latency one route another use bootstrapping since data normally distribute plan use bootstrapping generate confidence interval give metric route compare performance two route would calculate difference confidence interval see great equal threshold set metric however immediately clear right thing particular two route confidence interval barely overlap seem like may still make sense assume one path well perhaps effectively adjust confidence interval size alternate seem bootstrapping difference metric bootstrapping difference median etc little bit closer give interval possible range one concern recall reading sample size must work true objective general one problem bootstrapping figure use result make decision ideally like able say x certain route well route b least m x threshold take action bootstrapping get confidence interval clear translate information comparison monte carlo simulation make sense simulate route performance record whether well b least m complete simulation check well b x time take action however knowledge use resampled data execute monte carlo simulation would great get insight different option look particular bootstrapping right tool happy switch away clear need way understand uncertainty measurement thanks",
        "Tags": [
            "probability",
            "confidence-interval",
            "bootstrap",
            "simulation",
            "uncertainty"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 0,
        "Topics": {
            "16": 0.10283277,
            "43": 0.42914113
        }
    },
    {
        "Id": 307422,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "11/10/2017",
        "OwnerUserId": 179984.0,
        "Score": 2,
        "ViewCount": 54.0,
        "Title": "implement robit model",
        "Body": "try implement robit model answer description currently function code write r follow procedure laid paper look like however think correct implementation page author say update nu use example half-interval method paper seem say intermediate step different nu use update base half-interval method clear",
        "Tags": [
            "expectation-maximization",
            "robust",
            "probit"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 2,
        "Topics": {
            "3": 0.32793835,
            "29": 0.118408,
            "43": 0.29287457
        }
    },
    {
        "Id": 60987,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 60989.0,
        "CreationDate": "05/06/2013",
        "OwnerUserId": 25761.0,
        "Score": 3,
        "ViewCount": 9857.0,
        "Title": "example label unlabelled data",
        "Body": "someone please provide example label unlabelled data reading definition semi supervise learn make clear two actually",
        "Tags": [
            "r",
            "data-mining",
            "semi-supervised"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 1,
        "Topics": {
            "0": 0.17370352,
            "15": 0.106463484,
            "43": 0.2628922
        }
    },
    {
        "Id": 299377,
        "PostTypeId": 2,
        "ParentId": 296680.0,
        "AcceptedAnswerId": "",
        "CreationDate": "23/08/2017",
        "OwnerUserId": 129145.0,
        "Score": 3,
        "ViewCount": "",
        "Title": "",
        "Body": "unlikely since batch normalization gamma beta variable top normalization process paper mention gamma beta use scale shift activation appropriate degree order correct represent data paper note simply normalize input layer may change layer represent instance normalize input sigmoid would constrain linear regime nonlinearity address make sure transformation insert network represent identity transform accomplish introduce activation x k pair parameter \u03b3 k \u03b2 k scale shift normalize value like paper say use sigmoid example normalization gradient likely always around linear regime sigmoid function e one high gradient may optimal model learn however without scale shift value normalization likely learn restrict area think way shift activation value properly like use bias way effective esp since direct manipulation bias see ineffective mention paper",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "6": 0.37633958,
            "43": 0.25719738
        }
    },
    {
        "Id": 175317,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "03/10/2015",
        "OwnerUserId": 2488.0,
        "Score": 2,
        "ViewCount": 61.0,
        "Title": "obtain uncertainty errors-in-variables machine learn algorithm",
        "Body": "field every value report come -sigma uncertainty value use random forest regressors estimate value input -sigma uncertainty information random forest regressor train error-free simulation want estimate value real data monte carlo fashion perturb input within -sigma error time feed random forest get estimate value question order obtain -sigma uncertainty predict value assume value get distribute normally ok take mean standard deviation value intuition need obtain prediction interval estimate somehow combine right",
        "Tags": [
            "estimation",
            "predictive-models",
            "standard-error",
            "prediction-interval",
            "uncertainty"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 1,
        "Topics": {
            "16": 0.13488802,
            "26": 0.14249083,
            "43": 0.16948435,
            "49": 0.11353127
        }
    },
    {
        "Id": 32311,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "15/07/2012",
        "OwnerUserId": 12624.0,
        "Score": 2,
        "ViewCount": 787.0,
        "Title": "control variable gee use spss",
        "Body": "dataset think would suit gee try ass relative contribution number simultaneous predictor interaction intensity hurt feeling ask people report one hurtful event relationship well fill number scale cod hurtful event number category measure person hurt participant level ordinal rm variable type hurtful event level nominal variable measure personality trait hurt-sensitivity self-esteem continuous variable also variable would like control time since hurtful event occur continuous variable please let know explain dataset clearly run gee use main predictor variable relationship person hurt participant hurtful event type hurt-sensitivity self-esteem spss look like make sense work ahurtat intensity hurt feeling arellvl relationship person hurt participant atoe rc type hurtful event ahps ctrd hurt-sensitivity arse ctrd self-esteem however sure control time since hurtful event occur include model know create type model genlin function work way specify want time since event occur first level model predictor second level help would greatly appreciate",
        "Tags": [
            "repeated-measures",
            "spss",
            "multivariate-analysis",
            "gee"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 0,
        "Topics": {
            "4": 0.106034406,
            "11": 0.22704779,
            "21": 0.16782215,
            "43": 0.31630602
        }
    },
    {
        "Id": 397402,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "13/03/2019",
        "OwnerUserId": 237309.0,
        "Score": 2,
        "ViewCount": 57.0,
        "Title": "plan contrast mixed model group level",
        "Body": "analysis base data modification pulse measurement make time point study participant randomize group b c repeat measure model heterogeneous autoregressive covariance structure get marginal mean follow wonder way compare three group time point base specify model without perform anova time point interested pairwise comparison provide method stage",
        "Tags": [
            "r",
            "lme4-nlme",
            "post-hoc",
            "contrasts"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 3,
        "Topics": {
            "21": 0.2410232,
            "33": 0.15783083,
            "39": 0.13526653,
            "43": 0.13540864
        }
    },
    {
        "Id": 323170,
        "PostTypeId": 2,
        "ParentId": 323158.0,
        "AcceptedAnswerId": "",
        "CreationDate": "15/01/2018",
        "OwnerUserId": 7224.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "first distribution unspecified belong extend exponential family e g location cauchy distribution order statistic indeed minimal sufficient statistic see answer earlier x validate question lehmann casella point answer earlier x ed question reason mle parameter unbiased except update op density symmetric reason bias constant across constant give further\u00b2 reason unmvue exist actually since order statistic minimal sufficient complete instance ancillary hence allow lehmann-scheff\u00e9 apply although also exist setting unmvue exists complete sufficient statistic example take lehmann p go follow take support probability find minimum variance unbiased estimator amount minimise return solution depends hence prohibits existence uniformly minimum variance unbiased estimator case location parameter lehmann scheff\u00e9 show uniform allow umvue apart constant function perfectly fit set question interestingly distribution unknown mean zero best unbiased estimator state lehmann answer specifically question possibly recentred mle compare pitman best equivariant estimator dominates original equivariant mle recentred also equivariant mle term mean square error later point op comment also found lehmann lemma pitman best equivariant estimator location parameter always unbiased hence dominates recentred mle however specific case cauchy location parameter exist umvue location parameter completes paper bondesson prof non-existence umv-estimator \u03b8 provide tail density tends zero rapidly enough",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 5,
        "Topics": {
            "20": 0.11010355,
            "26": 0.15369624,
            "43": 0.30652514,
            "47": 0.14131229
        }
    },
    {
        "Id": 54528,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "28/03/2013",
        "OwnerUserId": 22392.0,
        "Score": 1,
        "ViewCount": 172.0,
        "Title": "sequential monte carlo hierarchical model",
        "Body": "anybody know sequential monte carlobe apply multi-dimensionalproblems e simulate morethan distribution like inhierarchical model maybe know follow literature",
        "Tags": [
            "monte-carlo",
            "hierarchical-bayesian"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "22": 0.16893293,
            "43": 0.2748191
        }
    },
    {
        "Id": 100628,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 100639.0,
        "CreationDate": "30/05/2014",
        "OwnerUserId": 31420.0,
        "Score": 2,
        "ViewCount": 734.0,
        "Title": "mixed discrete-continuous distribution",
        "Body": "let mixed discrete-continuous pdf give could please help show nasty pdf integrates sum support initially try integrate complete gamma distribution parameter shape scale sum lead mess another way go thank",
        "Tags": [
            "probability",
            "distributions",
            "self-study",
            "continuous-data",
            "mixed-model"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 2,
        "Topics": {
            "20": 0.22983308,
            "43": 0.3866697,
            "45": 0.22644414
        }
    },
    {
        "Id": 414599,
        "PostTypeId": 2,
        "ParentId": 264852.0,
        "AcceptedAnswerId": "",
        "CreationDate": "25/06/2019",
        "OwnerUserId": 121522.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "r mean perfect fit data simple linear model really hard tell go without familiar data output look like try predict predict value understand correctly sound like model model model value long model use case would result perfect fit clear",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "10": 0.11553896,
            "43": 0.4139661
        }
    },
    {
        "Id": 396640,
        "PostTypeId": 2,
        "ParentId": 369585.0,
        "AcceptedAnswerId": "",
        "CreationDate": "10/03/2019",
        "OwnerUserId": 35989.0,
        "Score": 4,
        "ViewCount": "",
        "Title": "",
        "Body": "sparse data data many zero author seem call prior sparse favorite zero pretty self-explanatory look shape laplace aka double exponential distribution peaked around zero image source tibshirani effect true value distribution always peaked location parameter equal zero although small value parameter regularize effect reason laplace prior often use robust prior regularize effect say laplace prior popular choice want really sparse solution may well choice described van erp et al van erp oberski l mulder j shrinkage prior bayesian penalize regression journal mathematical psychology doi j jmp",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 1,
        "Topics": {
            "23": 0.10696952,
            "41": 0.18801747,
            "43": 0.1517849
        }
    },
    {
        "Id": 62543,
        "PostTypeId": 2,
        "ParentId": 61756.0,
        "AcceptedAnswerId": "",
        "CreationDate": "25/06/2013",
        "OwnerUserId": 5509.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "baker journal apply ecology use similar model ask reply us normal inspire use follow transformation actually use link article recursively substitute get simply take site-specific intercept easily solve classic glm trivial see transform equation equivalent original model trivially see whole fit proccess include poisson error also equivalent probably limitation brain actual problem transform model course easily fit use include overdispersion use quasipoisson family",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "1": 0.11679663,
            "43": 0.36542016
        }
    },
    {
        "Id": 409553,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 409644.0,
        "CreationDate": "22/05/2019",
        "OwnerUserId": 237422.0,
        "Score": 1,
        "ViewCount": 149.0,
        "Title": "latent class model choice data gmnl choose start value",
        "Body": "want run lcmnl use gmnl-package choice data read start value influence class allocation since lcmnl several optimum found follow example code would like adapt data determine start value seem quiet abitrary example code however change slightly influence model parameter quiet lot",
        "Tags": [
            "r",
            "model-selection",
            "latent-class",
            "choice",
            "mlogit"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 2,
        "Topics": {
            "3": 0.12706207,
            "43": 0.31673363
        }
    },
    {
        "Id": 233780,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "07/09/2016",
        "OwnerUserId": 36804.0,
        "Score": 4,
        "ViewCount": 886.0,
        "Title": "calculate accuracy monte-carlo type simulation",
        "Body": "get monte-carlo simulation programmed work well follow trend publish work field field engineering would like calculate accuracy propagation simulation due input data possibly fix amount mean input x -b b constant significantly small x simple calculation calculate different input uncertainty propagate however move monte-carlo side calculation lose underlie principal test single object locate one number position pick single position random many time equivalent distribution position precise location simulated value use calculate finite number possible position simple x -b accuracy represent accuracy calculation base randomly select location",
        "Tags": [
            "monte-carlo",
            "accuracy",
            "uncertainty"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "16": 0.100915395,
            "22": 0.117677964,
            "43": 0.16691498
        }
    },
    {
        "Id": 206032,
        "PostTypeId": 2,
        "ParentId": 206007.0,
        "AcceptedAnswerId": "",
        "CreationDate": "07/04/2016",
        "OwnerUserId": 4253.0,
        "Score": 3,
        "ViewCount": "",
        "Title": "",
        "Body": "observation independent impossible overdispersion bernoulli response sure make question arise regression model improperly specify different problem example logistic model strong omit covariate cause bias towards zero due non-collapsibility odds ratio",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "1": 0.18610904,
            "11": 0.22861467,
            "43": 0.20645446
        }
    },
    {
        "Id": 327032,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 327038.0,
        "CreationDate": "05/02/2018",
        "OwnerUserId": 153112.0,
        "Score": 1,
        "ViewCount": 4380.0,
        "Title": "fair unfair dice probability question",
        "Body": "box six fair dice single unfair die six one die chosen random roll twice roll six appear probability select die unfair one",
        "Tags": [
            "probability"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "13": 0.7566387
        }
    },
    {
        "Id": 54460,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 54493.0,
        "CreationDate": "27/03/2013",
        "OwnerUserId": 10340.0,
        "Score": 4,
        "ViewCount": 2266.0,
        "Title": "abbreviation p e mean",
        "Body": "come across paper us abbreviation p e khatri mardia von mises-fisher matrix distribution orientation statistic section page include short excerpt use density function give khatri find p e give abbreviation p e mean sentence",
        "Tags": [
            "terminology",
            "circular-statistics",
            "abbreviation"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "19": 0.102135,
            "29": 0.169408,
            "43": 0.22574627,
            "45": 0.10574887
        }
    },
    {
        "Id": 367939,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "21/09/2018",
        "OwnerUserId": 195845.0,
        "Score": 1,
        "ViewCount": 31.0,
        "Title": "infer incidence prevalence",
        "Body": "data set patient risk factor disease grade grade determine lab value e data look like first time measure individual second time lab measure individual struggle statistically describe change population average average discus average mean difference lab value ci x amount year ci determine significance change population would use pair t-test correct essentially study cohort determine incidence disease say incidence healthy grade x ci per year grade grade ci per year etc lastly access cdc data demonstrates forth include table grade grade cdc data actually prevalence since snap shot time infer incidence way assume subject many assumption would conduct hypothesis test demonstrate incidence determine question significantly high low cdc incidence suggests",
        "Tags": [
            "hypothesis-testing",
            "epidemiology",
            "medicine"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 2,
        "Topics": {
            "4": 0.19029926,
            "43": 0.3260735
        }
    },
    {
        "Id": 59200,
        "PostTypeId": 2,
        "ParentId": 59191.0,
        "AcceptedAnswerId": "",
        "CreationDate": "16/05/2013",
        "OwnerUserId": 159.0,
        "Score": 3,
        "ViewCount": "",
        "Title": "",
        "Body": "subtract mean observation proceed mean zero",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "24": 0.24882296,
            "25": 0.17634393,
            "43": 0.28249922,
            "49": 0.15213168
        }
    },
    {
        "Id": 387216,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "15/01/2019",
        "OwnerUserId": 234061.0,
        "Score": 1,
        "ViewCount": 20.0,
        "Title": "determine dichotomous variable randomly distribute predict instance",
        "Body": "product measurement x feature feature either pas fail statistical test use tell failure randomly distribute across feature example sample car tire car either flat tire flat randomly distribute across tire presence one flat predict presence another sample dataset num flat qty observe flat flat flat flat flat best guess estimate expect value use normal distribution base average standard deviation original dataset however run successful chi-square expect value zero know appropriate estimate normal distribution base original dataset calculate expect value",
        "Tags": [
            "distributions",
            "chi-squared",
            "discrete-data"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 4,
        "Topics": {
            "0": 0.10213191,
            "41": 0.12060731,
            "43": 0.19678691
        }
    },
    {
        "Id": 201084,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 201096.0,
        "CreationDate": "11/03/2016",
        "OwnerUserId": 98128.0,
        "Score": 2,
        "ViewCount": 509.0,
        "Title": "categorical variable regression",
        "Body": "building linear regression model categorical predictor create level dummy indicator variable categorical covariates let say variable temp hot moderate cold create dummy variable hot cold doubt model one indicator variable model none way include categorical variable use backward elimination method final model get dummy variable correspond hot use model ensure either dummy variable present none appreciate help",
        "Tags": [
            "categorical-data",
            "least-squares"
        ],
        "AnswerCount": 2.0,
        "CommentCount": 0,
        "Topics": {
            "11": 0.39249256,
            "43": 0.24675329
        }
    },
    {
        "Id": 162524,
        "PostTypeId": 2,
        "ParentId": 159756.0,
        "AcceptedAnswerId": "",
        "CreationDate": "21/07/2015",
        "OwnerUserId": 35398.0,
        "Score": 3,
        "ViewCount": "",
        "Title": "",
        "Body": "mentionned think urca var offer standard error could either use another software like gretl use bootstrap procedure package tsdyn bootstrap procedure us quantiles bootstrap distribution use fancy method like bootstrapping t-stat precisely unavailable give asymptotic refinement bootstrap shoul nevertheless give approximation confidence interval still compare gretl sure",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "3": 0.20249328,
            "16": 0.15922982,
            "26": 0.11968175,
            "43": 0.15785934
        }
    },
    {
        "Id": 176930,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "14/10/2015",
        "OwnerUserId": 91973.0,
        "Score": 2,
        "ViewCount": 4557.0,
        "Title": "obtain p-value chi-square test matlab",
        "Body": "currently fitting model dataset measure goodness-of-fit use chi-square test model fit data model fit data figure show datapoints black well fit model orange datapoints black occurences event disclosure security vulnerability along time axis time event total time event total thus orange curve obtain fitting alhazmi malaiya logistics model know model model vulnerability discovery process give equation parameter b c select fitting process model describes datapoints good possible therefore parameter combination result low select calculate use datapoints black value obtain solve equation time formula give case select get critical value accept state datapoints distribute accord model word model describes data reasonably well case exactly p-value -test accord pearson chi-square test p-value calculate would case result p-value would suggest significant fit model p-value good fit fit especialy make data close calculation correct unfortunately literature work properly explain use methodology seem reproduce result one paper use approach goodness-of-fit test described page last section",
        "Tags": [
            "chi-squared",
            "modeling",
            "p-value",
            "matlab"
        ],
        "AnswerCount": 2.0,
        "CommentCount": 12,
        "Topics": {
            "1": 0.118378334,
            "43": 0.33261713,
            "44": 0.113134935
        }
    },
    {
        "Id": 238399,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "04/10/2016",
        "OwnerUserId": 130324.0,
        "Score": 3,
        "ViewCount": 3522.0,
        "Title": "estimate standard deviation residual residual plot",
        "Body": "understand normal histogram scatter plot estimate standard deviation look portion contains data term residual plot regression however randomly size region around x estimate standard deviation residual",
        "Tags": [
            "regression",
            "data-visualization",
            "standard-deviation",
            "residuals",
            "descriptive-statistics"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 3,
        "Topics": {
            "36": 0.26956525,
            "43": 0.24191862
        }
    },
    {
        "Id": 415835,
        "PostTypeId": 2,
        "ParentId": 415793.0,
        "AcceptedAnswerId": "",
        "CreationDate": "03/07/2019",
        "OwnerUserId": 176202.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "model selection base -values bias coefficient final model towards significance form stepwise regression avoid goal confirmation -values actual chance false positive much high chosen level significance discuss several place cv notably therefore go model originally intend use inference assume full model interaction even especially expert knowledge suggest gender affect variable interest single model use interaction gender prefer interaction allows model gender-specific effect single model multiple advantage importantly observation note effect significant mean demonstrate might sample size limited compare number parameter effect size small variance effect large combination reason reason model selection base -values usually helpful",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 9,
        "Topics": {
            "1": 0.12489801,
            "43": 0.44087112
        }
    },
    {
        "Id": 266406,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 266409.0,
        "CreationDate": "09/03/2017",
        "OwnerUserId": 95227.0,
        "Score": 2,
        "ViewCount": 1023.0,
        "Title": "assumption linear regression gradient descent",
        "Body": "reading linear regression andrew ng lecture islr estimate coefficient use gradient descent understood gradient descent include dummy variable value one throughout sample point get intercept value assign weight variable randomly variable make prediction accord weight use cost function square error example compute loss value derivative cost function respect weight gradient adjust weight subtract gradient time learn rate weight iterate change weight insignificant number iteration do come assumption linear regression whole process assume anything like normality constant variance error auto-correlation independence feature agree response variable linearly related predictor variable model fit well assumption question assumption stem basis justification make assumption",
        "Tags": [
            "regression",
            "machine-learning",
            "multiple-regression",
            "generalized-linear-model",
            "gradient-descent"
        ],
        "AnswerCount": 2.0,
        "CommentCount": 0,
        "Topics": {
            "5": 0.16642386,
            "23": 0.14294809,
            "43": 0.2286959
        }
    },
    {
        "Id": 307616,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "12/10/2017",
        "OwnerUserId": "",
        "Score": 0,
        "ViewCount": 205.0,
        "Title": "time series deterministic trend",
        "Body": "set var model two variable one variable look like think still appropriate setup var time series drift think must drift trend data already log-transformed read paper impression every author handle differently trend graph show trend look graph seem clear",
        "Tags": [
            "r",
            "time-series",
            "trend"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 9,
        "Topics": {
            "24": 0.28798294,
            "43": 0.31720203
        }
    },
    {
        "Id": 411164,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": 411176.0,
        "CreationDate": "01/06/2019",
        "OwnerUserId": 249699.0,
        "Score": 2,
        "ViewCount": 39.0,
        "Title": "proof distribtion proportion binomial variable",
        "Body": "true proportion binomial variable follow normal distribtion n approach infinity true prove",
        "Tags": [
            "mathematical-statistics"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 1,
        "Topics": {
            "20": 0.30450395,
            "25": 0.13796471,
            "43": 0.15027964,
            "45": 0.12925299
        }
    },
    {
        "Id": 53107,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "23/03/2013",
        "OwnerUserId": 1005.0,
        "Score": 15,
        "ViewCount": 10362.0,
        "Title": "meaning completeness statistic",
        "Body": "wikipedia statistic say complete distribution every measurable function must independent parameter follow implication hold statistic say boundedly complete implication hold bound function read agree xi phaneron complete statistic mean one unbiased estimator base understand wikipedia say begin article essence completeness property statistic condition ensures parameter probability distribution represent model estimate basis statistic ensures distribution correspond different value parameter distinct sense completeness ensures distribution correspond different value parameter distinct distribution distribution complete statistic sense completeness ensures parameter probability distribution represent model estimate basis statistic optional bound completeness mean compare completeness",
        "Tags": [
            "mathematical-statistics",
            "intuition",
            "complete-statistics"
        ],
        "AnswerCount": 3.0,
        "CommentCount": 5,
        "Topics": {
            "43": 0.3092345,
            "47": 0.19847782,
            "49": 0.10434174
        }
    },
    {
        "Id": 448429,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "07/02/2020",
        "OwnerUserId": 248806.0,
        "Score": 0,
        "ViewCount": 10.0,
        "Title": "extract feature determine periodicity",
        "Body": "accelerometer time series data sample hz participant extract feature separate movement collection period per person use machine learn feature explore various aspect movement like rms standard deviation max dominant frequency kurtosis skewness etc however want add feature capture relationship current movement movement surround example plot show window accelerometer data one subject different portion record movement unique shape occur isolation related surround movement signal movement occur series vary shape timing signal occur regular pattern signal hop feature suggestion capture current movement part series characteristic series like frequency series movement type movement repeatedly v cycle movement like show signal consistent series calculate dominant frequency window seem capture frequency series movement ok relatively periodic often give somewhat similar value periodic fluctuation within movement way movement define really help much differentiate movement series autocorrelation could potentially helpful sure exactly feature extract acf max acf initial decline seem somewhat inconsistent would expect visually many paper read use lag- value seem related much would expect also see study fit arima model believe ar model would suffice give acf pcf plot extract coefficient feature may several feature preliminary model order sure consistent best model order different participant suggestion",
        "Tags": [
            "time-series",
            "correlation",
            "autocorrelation",
            "feature-construction",
            "feature-engineering"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 0,
        "Topics": {
            "24": 0.23745435,
            "43": 0.30279854
        }
    },
    {
        "Id": 267275,
        "PostTypeId": 2,
        "ParentId": 266784.0,
        "AcceptedAnswerId": "",
        "CreationDate": "13/03/2017",
        "OwnerUserId": 78229.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "think get many question however obtain answer important good recommendation one approach answer regression question would use lasso regularize method variable selection say every statistician sibling favorite variable selection method lasso advantage call larry wasserman defunct normal deviate blog one best contribution statistic last year lasso would reduce variable manageable few number plenty heuristic rank variable relative importance e identify driver bad choice avoid use beta regression coefficient since scale invariant well choice would rank absolute value t-statistics associate variable optimal choice relative variable importance would read ulrike groemping paper area statistical model implement approach call relaimpo",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 7,
        "Topics": {
            "11": 0.12897386,
            "43": 0.37237117
        }
    },
    {
        "Id": 439950,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "09/12/2019",
        "OwnerUserId": 255026.0,
        "Score": 0,
        "ViewCount": 9.0,
        "Title": "force hidden code autoencoder produce independent code unit",
        "Body": "try construct undercomplete autoencoder number hidden neuron middle layer bottleneck much low number input output neuron however would like create code consist independent code sorry formulation way find many neuron use",
        "Tags": [
            "independence",
            "autoencoders"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 0,
        "Topics": {
            "3": 0.17627798,
            "6": 0.27140877,
            "43": 0.21590045
        }
    },
    {
        "Id": 133892,
        "PostTypeId": 2,
        "ParentId": 133074.0,
        "AcceptedAnswerId": "",
        "CreationDate": "18/01/2015",
        "OwnerUserId": 64700.0,
        "Score": 11,
        "ViewCount": "",
        "Title": "",
        "Body": "assume say mean layer x filter number convolution simply discrete convolution per filter filter bank let say network go multi channel convolution depth respectively see depth convolution go change function depth input volume previous layer assume try figure compare single channel convolution well could multiply depth input volume number filter layer add together case tell many single channel convolution computationally intensive convolution computational intensity convolution depend variety parameter include far step individual filter calculation number pool layer etc",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "6": 0.31351325,
            "15": 0.23846401,
            "43": 0.21894425
        }
    },
    {
        "Id": 441761,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "20/12/2019",
        "OwnerUserId": 269145.0,
        "Score": 1,
        "ViewCount": 18.0,
        "Title": "uniqueness svm solution",
        "Body": "one show solution primal linear svm unique exists support vector whose correspond slack variable equal try show proof contradiction e suppose support vector correspond slack variable great would yield solution contradicts uniqueness solution show far assume slack variable correspond support vector mean support vector penalty slack variable find e sum label support vector number positive negative support vector equal true dataset constraint may hold yield desire contradiction",
        "Tags": [
            "svm"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 0,
        "Topics": {
            "0": 0.1427641,
            "17": 0.186835,
            "20": 0.24402837,
            "43": 0.11831828
        }
    },
    {
        "Id": 313092,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "10/11/2017",
        "OwnerUserId": 122087.0,
        "Score": 0,
        "ViewCount": 193.0,
        "Title": "calculate correlation categorical order categorical binary variable",
        "Body": "would calculate correlation categorical binary variabl\u00e7es categorical order variable use r",
        "Tags": [
            "r",
            "correlation",
            "categorical-data"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 4,
        "Topics": {
            "3": 0.17748475,
            "11": 0.41368452,
            "43": 0.11865052,
            "46": 0.11850617
        }
    },
    {
        "Id": 425561,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "09/09/2019",
        "OwnerUserId": 214651.0,
        "Score": 2,
        "ViewCount": 308.0,
        "Title": "calculate p-value z-score without use z-table",
        "Body": "find video documentation show calculate p-value z-score without use z-table possible hard know use scipy get information calculate without need",
        "Tags": [
            "normal-distribution",
            "python",
            "p-value",
            "z-score"
        ],
        "AnswerCount": 1.0,
        "CommentCount": 0,
        "Topics": {
            "3": 0.2574103,
            "43": 0.27254012
        }
    },
    {
        "Id": 181039,
        "PostTypeId": 2,
        "ParentId": 181033.0,
        "AcceptedAnswerId": "",
        "CreationDate": "10/11/2015",
        "OwnerUserId": 78229.0,
        "Score": 2,
        "ViewCount": "",
        "Title": "",
        "Body": "number predict transaction either estimate transaction probabilistic value function form model see comment clarification different model work develop prediction validation dealt paper btyd model quote p validate model work need divide data calibration period holdout period subsequent discussion walk issue quite thoroughly",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 9,
        "Topics": {
            "10": 0.10046463,
            "43": 0.34192404
        }
    },
    {
        "Id": 402167,
        "PostTypeId": 2,
        "ParentId": 402012.0,
        "AcceptedAnswerId": "",
        "CreationDate": "10/04/2019",
        "OwnerUserId": 163572.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "compare two algorithm data fold use pair t-test evaluate produce similar result also want show similar oppose one well two-sided test would make sense finally get large p-values test mean accept null hypothesis mean sufficient evidence reject make sure interpret result test correctly dont infer something actually true see e g statistician say non-significant result mean reject null oppose accept null hypothesis",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "43": 0.35105675,
            "44": 0.27256203
        }
    },
    {
        "Id": 273822,
        "PostTypeId": 1,
        "ParentId": "",
        "AcceptedAnswerId": "",
        "CreationDate": "15/04/2017",
        "OwnerUserId": 157351.0,
        "Score": 1,
        "ViewCount": 168.0,
        "Title": "determine mean sd parent normal distribution truncate data set",
        "Body": "seem like easy stuck try determine data set represent truncate normal distribution hypothesis test data see actually act truncate normal distribution rather poisson geometric distribution able calculate parent mean standard deviation truncate set could go extrapolate give data set would react without limitation truncation try avoid iterative process write code deal resource absolutely necessary determine parent mean standard deviation truncate data set please bear mind background business finance rather statistic apologize advance misuse term elementary basis understand high statistical concept",
        "Tags": [
            "normal-distribution",
            "truncated-normal",
            "extrapolation"
        ],
        "AnswerCount": 0.0,
        "CommentCount": 1,
        "Topics": {
            "14": 0.1580119,
            "25": 0.10373592,
            "43": 0.30798194,
            "45": 0.10149774,
            "47": 0.108658016
        }
    },
    {
        "Id": 389406,
        "PostTypeId": 2,
        "ParentId": 389399.0,
        "AcceptedAnswerId": "",
        "CreationDate": "27/01/2019",
        "OwnerUserId": 115634.0,
        "Score": 1,
        "ViewCount": "",
        "Title": "",
        "Body": "variable correspond data value collect single country form time series collection time series single country collect time series data variable multiple country would panel data collection time series multiple country",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 2,
        "Topics": {
            "8": 0.105902135,
            "14": 0.21826035,
            "24": 0.20462656,
            "43": 0.2789289
        }
    },
    {
        "Id": 371142,
        "PostTypeId": 2,
        "ParentId": 317929.0,
        "AcceptedAnswerId": "",
        "CreationDate": "10/10/2018",
        "OwnerUserId": 41467.0,
        "Score": 0,
        "ViewCount": "",
        "Title": "",
        "Body": "contrast orthogonal possible meaningfully say adjust x separately x see effect one predictor isolation type iii idea aim predictor orthogonal meaningful look data adjust x without also adjust x",
        "Tags": [],
        "AnswerCount": "",
        "CommentCount": 0,
        "Topics": {
            "5": 0.1181598,
            "21": 0.16881451,
            "43": 0.34015238
        }
    }
]